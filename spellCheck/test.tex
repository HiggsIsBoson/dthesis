%\documentclass {article}
\setlength{\topmargin}{-1.5cm}
\setlength{\oddsidemargin}{-0.3cm}
\setlength{\evensidemargin}{-0.3cm}
\setlength{\textwidth}{16.5cm}
\setlength{\textheight}{23cm}
%\usepackage{graphicx}
%\usepackage{float}
%\usepackage{url, braket, setspace}
%\usepackage{amsmath,amsthm,amssymb,bm}
%\usepackage{hyperref}


%%%%%%%%%%%%

%\begin{document}

\section*{Abstract} 
%Despite the enormous success of the Standard Model in particle physics, there are still quite a room for unrevealed mysteries of universe;
Despite the enormous success of the Standard Model in particle physics, there are still a number of problems left to be solved such as the problem of diverging higgs mass and the unaccounted presence of dark matter and so on.
%as well as for the persue toward the ``theoy of everything'' with grand unification and gravity.
It is then strongly motivated to extend the Standard Model, and the Minimal Supersymmetric Standard Model (MSSM) has been one of the most applealing candidates, introducing a boson-fermion symmetry (super-symmetry; SUSY). 
Experimental search of SUSY particles predicted by MSSM has been widely performed over the decade in collider experiments. Though no evidence has been claimed so far, searches in the Large Hadron Collider (LHC) are anticipated since it allows to probe heavier regions with the unprecedented high center-of-mass energy with increased data statistics.
The motivation of gaugino search is increasingly emerging in light of the discovery of higgs boson in its mass of $125\gev$, and gluino search is particularly interesting due to the large production in LHC.   \\

This thesis presents the search for gluinos via proton-proton collisions with the center-of-mass energy of $\sqrt{s}=13\tev$ at LHC, by focusing on the final state with exactly one leptons. 
Using the improved analysis technique and increased data with 36.1 fb$^{\-1}$ of integrated luminosity collected in the ATLAS detector, the sensitivity to heavier gluino is drastically gained.  \\

In this analysis, the main improvement with respect to past searches are two-fold: 
while only a few typical scenario of gluino decays have been studied in the past, 
the new analysis covers all the possible gluino decay chains that can be targeted in 1-lepton final state are explored, 
and the exclusion limits are explicitly provided for the first time; 
A dedicated data-driven strategy of background estimation is introduced, enabling robust estimation in regions where conventional simulation-based method estimation is not very reliable. \\

No significant excess is found in the unblinded dataset, and exclusion limits are set on wide range of gluino decay scenarios. As a general conclusion,
it is confirmed that up to $1.7\tev - 2.0 \tev$ in gluino mass and up to $\sim 1\tev$ in the lightest neutralino mass is excluded for typical mass spectra, 
while the limit extends up to $1.5\tev - 1.9 \tev$ in gluino mass in case of compressed EW gaugino masses ($\Delta M \sim 20-30\gev$) that is motivated by dark matter relic observations.



\clearpage

%{\it Maybe knowledge is as fundamental, or even more fundamental than reality.  \;\;\; }
%\begin{flushright}
%Anton Zeillinger
%\end{flushright}

%{\it The eye sees only what the mind is prepared to comprehend. \;\;\; }
%\begin{flushright}
%Henri Bergson
%\end{flushright}



%\end{document}
\section{Introduction}
This section will provide the theoretical background necessary to motivate and understand the context of rest of the thesis.
It starts with an brief overview of successful works done by the Standard Model (SM) in the particle physics, and review the remained homework (widely referred from \cite{Peskin} and \cite{QandL}).
The concept of supers-symmetry is then introduced as potential yet strong candidate of the solution by extending the SM.
A particular emphasis is put on the Minimal Super-Symmtric Standard Model (MSSM), with closely outlining the phenomenology and present experimental constraints.
Finally, the experimental signature targeted in the thesis is explained as well as discussing the searching strategy. \\


\subsection{The Standard Model of Elementary Particles}
The particle content of the SM is shown in Tab. \ref{tab::Introduction::SMFermions} and Tab. \ref{tab::Introduction::SMBosons}. There are three types of particles: fermions with the spin of 1/2 that consists matters: gauge bosons with the spin of 1 mediating the interaction acting between particles: and the spin-0 Higgs boson feeding their masses through the the Brout-Englert-Higgs (or BEH) mechanism. \\

\tab{c | c c c | c c c c c}
{
\hline
        &  \multicolumn{3}{c|}{Generation} &   $Q$ &  $T$  & $T^3$ &  $Y$ & $N_C$\\
\cline{2-4} 
        &  1st   &  2nd    &  3rd         &     &     &    &   \\
\hline
\hline
Quarks  &  $\colv{u \\ d}_{\mL}$   &  $\colv{c \\ s}_{\mL}$    &  $\colv{t \\ b}_{\mL}$ &   $\colv{2/3 \\ -1/3}$ &  1/2  & $\colv{1/2 \\ -1/2}$ &  $1/3$ & 3\\
        &  $u_{\mR}$   &  $c_{\mR}$    &  $t_{\mR}$ &   $2/3$  &  0  & 0 &  $4/3$  & 3\\
        &  $d_{\mR}$   &  $s_{\mR}$    &  $b_{\mR}$ &   $-1/3$ &  0  & 0 &  $-2/3$ & 3\\
\hline
Leptons  &  $\colv{\nu_e \\ e^-}_{\mL}$   &  $\colv{\nu_\mu \\ \mu^-}_{\mL}$    &  $\colv{\nu_\tau \\ \tau^-}_{\mL}$ &   $\colv{0 \\ -1}$ &  1/2  & $\colv{1/2 \\ -1/2}$ &  $-1$ & 0 \\
        &  $e_{\mR}$   &  $\mu_{\mR}$    &  $\tau_{\mR}$ &   $-1$  &  0  & 0 &  $-2$ & 0 \\
\hline
}
{Fermion contents in the SM. The quantum numbers $Q$, $T$, $T^3$ and $Y$ are respectively electroc charge, weak isospin number, the third component of weak isospin and weak hyper charge. $N_C$ represents the number of color states. The subscripts L, R indicate the chirality (left- or right-handed respectively), and the pharenthses denote the $SU(2)_L$ doublet.
}
{tab::Introduction::SMFermions}


\tab{c c | c c c c c}
{
\hline
             &          &  $Q$ &  $T$  & $T^3$ &  $Y$ & $N_C$ \\
\hline
\hline
gluon        & $g$      &   0  &  0    &  0    &  0   &  8    \\
\hline
weak bosons  & $W^\pm$  &$\pm1$&  1    &$\pm1$ &  0   &  0    \\
             & $Z$      &   0  &  0    &  0    &  0   &  0    \\
\hline
photon       & $\gamma$ &   0  &  0    &  0    &  0   &  0    \\
\hline
\hline
higgs       & $h$       &   0  &  1/2  &  -1/2 &  1   &  0    \\
\hline
}
{Gauge bosons and higgs in the SM. The notation for the quantum numbers are the same with \ref{tab::Introduction::SMFermions}.}
{tab::Introduction::SMBosons}


The three types of gauge bosons; gluon ($g$); weak bosons ($W^{\pm},Z$) and photon ($\gamma$) respectively characterize strong interaction, weak interaction and electromagnetic interaction. 
%\footnote{Gravity has yet been accommodated by the SM and any successful extension of it.}
Fermions are further two-hold: quarks which senses all the three interactions: lepton which couple only via weak and eletromagnetic interaction. Both family have up- and down-type, together with two more duplications of them (``2nd / 3rd generation'') with exactly the same properties except the masses. Each fermions furthermore have the charge conjugated partner called anti-fermions. \\


\subsubsection{The Gauge Principle and Particle Interaction} \label{sec::Introduction::gaugePrinciple}
A successful theory for elementary particles must be quantum and relativistic.
The theory of SM is constructed in a relativistic framework of field theory, fully exploiting the virtue that time ($t$) and position ($\bm{x}$) are treated equivalently; both are parameters of coordinates rather than observables. It is characterized by a Lorentz-invariant Lagrangian in which particles are described by a function in terms of $x^\mu$ (``fields'') following the Lorentz transformation law of corresponding spin expression. The free Lagrangian for fermions are given by:
\begin{align}
\mathcal{L} = i \bar{\psi} \gamma_\mu \partial_\mu \psi - m \bar{\psi}\psi + \mathrm{h.c.}
\label{eq::SMfreeLag}
\end{align}
where $\psi$ is spinor field for a fermion with the mass of $m$, and $\gamma_\mu$ is $4\times4$ gamma matrices. \\
The first term corresponds to the kinetic terms and the second is to the mass term.
%\begin{align}
%\colv{\bm{1} & \\ \bm{1} & }
%\mathrm{(In Weyl's representation)}
%\end{align}
%Note that $\sigma_i$ are Pauli's matrices

Interaction between particles are ruled by a local symmetries called ``gauge symmetry''. 
The interaction terms are obtained by requiring the free Larganian for invariance against gauge transformation which is for instance a $U(1)$ transformation in case of electromagnetic interaction:
\begin{align}
\psi \ra e^{i\theta(x) Q} \psi = e^{i\theta(x) q} \psi
\end{align}
where $Q$ is the generator of the $U(1)$ transformation, $q$ is charge that the fermion $f$ has, and $\theta(x)$ is an arbitrary time-space dependent phase.
The free Lagrangian in Eq. (\ref{eq::SMfreeLag}) is not invariant under this transformation, however can be fix by employing a small hack in the differential ($\partial_\mu$) terms in the free Lagrangian:
\begin{align}
\partial_\mu \ra D_\mu := \partial_\mu - ieA_\mu (x)
\end{align}
where $A(x)$ is a vector field transformed by the gauge transformation by:
\begin{align}
A_\mu \ra A_\mu + \frac{1}{e} \partial_\mu \theta(x).
\end{align}
%The replaced $D_\mu$ is referred to ``covariant differential''.

The interaction term emerges as the extra terms in the Lagrangian:
\begin{align}
\mathcal{L}_{\mathrm{int.}} = e\bar{\psi} \gamma^\mu \psi A_\mu.
\label{eq::SMfreeLag}
\end{align}
From the consistency with classical Maxwell equation, this represents the interaction between fermion $f$ and electromagnetic
and $A_\mu$ is identified as the electromagnetic potential in the classical electromanetism or the particle field for photon. \\



\subsubsection{Perturbation and Renormalization}
The effect of interaction is often characterized via transition amplitude from an inital state ($i$) to a final state ($f$):
\begin{align}
\bra{f} e^{-i\mathcal{H}_{\mathrm{int.}}t} \ket{i},
\end{align}
which is a basic quantity for phenomenological prediction on interaction cross-section or decay branch. 
This is however in most of the cases not analytically calculable therefore done through a perturbation expansion based on the coupling constant of the interaction, for which $\alpha := e^2/4\pi$ is conventionally used for electromagnetic interaction. 


While the small coupling constant of electromagnetic interaction $\sim 1/137$ is supposed to guarantee a good convergence behavior of the expansion, as well as to provide validity of calculation with truncated orders in the series, it is however found that the higher order terms immediately lead to divergence quite everywhere in cross-section calculation (infrared / ultraviolet divergences). This problem was solved by a procedure called ``renormalization'' 
%\cite{Tomonaga}\cite{Feynman}, 
where physical parameters (i.e. the masses and coupling constants) are redefined to absorb the infinities, resulting in a finite cross section calculation. Historically, it is firstly formulated successfully in QED, and then understood by that the gauge symmetry played an important role in calceling the divergence \cite{Ward1950}\cite{Takahashi1957}. From this moment, gauge symmetry started establishing the status as a guidance principle in constructing theory, beyond merely a prescription. It is also shown with considerable generality that well-behaving theory (``renormalizable theory'') must respect gauge symmetry \cite{tHooft1972}. \\

Also the concept of renormalization provided a critical insight that the magnitude of physics parameters in a theory could effectively vary depending on the energy scale with which the interaction happen. The evolution is characterized by the renormalization group equation (RGE), for example, as for the coupling constant ($\alpha$): 
\begin{align}
\dfrac{1}{\alpha(Q)^2}-\dfrac{1}{\alpha(Q_0)^2} = -\dfrac{\beta(\alpha)}{2\pi} \log{\left(\dfrac{Q}{Q_0}\right)},
\end{align}
where $Q$ is the scale defined by the typically momentum transfer of interaction processes, and $\beta(\alpha)$ is the beta function, proportional to $\alpha^2$ at 1-loop level. This evolution is known as the ``running'' effect, which is an useful proxy for exploring the behavior of theory over the scale. \\


\subsubsection{QED, QCD, and the Electroweak Theory}
The Lagrangian for Quantum Electromagnetic Theory (QED) is given by adding the kinetic terms for photon $-\frac{1}{4}F_{\mu\nu} F^{\mu\nu}$ to one obtained in Sec. \ref{sec::Introduction::gaugePrinciple}:
\begin{align}
\mathcal{L} = -\frac{1}{4}F_{\mu\nu} F^{\mu\nu} + i \bar{\psi} \gamma_\mu \partial_\mu \psi - m \bar{\psi}\psi \, + \mathrm{h.c.}
\end{align}
with $F_{\mu\nu}$ being the field strength:
\begin{align}
F_{\mu\nu} := \partial_\mu A_\nu - \partial_\nu A_\mu.
\end{align}


Similar to what is done in QED with the gauge group of $U(1)$, the Lagrangian for strong and weak interaction can be generated by considering gauge groups of $SU(2)_L$ and $SU(3)$:
$$
\psi \ra e^{i \theta_a(x) \lambda^a}, \psi \,\,\,\,\,\,\,\, a=1,2,\dots (N^2-1) \,\,\,\,\,\, \mbox{\phantom{MM}  (for $SU(N)$)} 
$$
where $\lambda^a$ are the generators of the gauge group. \\
The choice of gauge groups are respectively motivated by observation of approximately valid iso-spin symmetry in the theory of nucleus decay, and an enhanced number of degree of freedoms by factor about 3 for quarks implied by the measurement in $ee\ra qq$. \\

The Lagrangian for strong interaction is:
\begin{align}
\mathcal{L}_{\mathrm{QCD}} & = -\frac{1}{4} \bm{\hat{G}}_{\mu\nu} \bm{\hat{G}}^{\mu\nu} + \,\, \bar{q}(i \gamma_\mu D_\mu - m) q + \mathrm{h.c.}, \nn \\
D_\mu & := \partial_\mu + ig_s \sum_{a=1}^8 G^a_{\mu}\frac{\lambda_a}{2} \nn \\
\bm{\hat{G}}_{\mu\nu} & := \partial_\mu \bm{G}_\nu - \partial_\nu \bm{G}_\mu - g_s \, \bm{G}_\mu \times \bm{G}_\nu, \nn \\
\bm{G}_\mu & := \{G^a_\mu; a=1,2,\dots,8\}
\label{eq::QCDLag}
\end{align}
where $G^a_\mu$ and $q$ represent the fields for gluons and quarks respectively.
$g_s$ the related to the coupling constant $\alpha_s$ by $\alpha_s = g_s^2/4\pi$. 
The charge of strong interaction is called ``color'' (red, blue, green), and the theoretical framework is referred to Quantum Chromo Dynamics (QCD). 
Quarks are in the triplet and gluons are in the octet expression with 3 and 8 degenerated states respectively.
In addition, due to the non-abelian nature of $SU(3)$, gluon has self-interaction with coupling to itself. 
One distinct characteristic for the strong interaction is the negatively slope of running coupling:
\begin{align}
\alpha_s(Q) = \frac{4\pi\alpha_s(\mu_R)}{4\pi + \beta_0 \alpha_s(\mu_R)\log{(Q^2/\Lambda^2_{\mathrm{QCD}})} }
\end{align}
where $\beta = 11 -2n_f/3$ ($n_f$ is number of quarks with the mass above $Q$), $\mu_R$ the renormalization scale (a reference scale of renormalization, different from the physical energy scale $Q$), and $\Lambda_{\mathrm{QCD}}$ the QCD cut-off scale at $\sim 200\mev$. The indication of $\beta<0$ is decreasing coupling constant with increased energy scale $Q$.
Despite of the generally larger coupling than that of electromagnetic interaction, in the energy scale interested in LHC ($Q>100^gev$), $\alpha_s$ typically about 0.1, which is small enough to recover the perturbative picture (``asymptotic freedom''). 
On the other hand, the coupling becomes increasing strong as approaching to $\Lambda_{\mathrm{QCD}}$ leading to immediate catastrophe of perturbation, forcing colored particles to unite each to form color singlet state (``confinement''). \\


Weak interaction is described by a larger gauge group $SU(2)_L \times U(1)_Y$, in a manner residing the electromagnetic interaction altogether. The basic idea for this is to consider that they share the common origin at high energy scale and branch into separate interactions at some point through a spontaneous symmetry breaking (SSB) $SU(2)_L \times U(1)_Y \ra U(1)_Q$.
The regime of unified interaction is commonly referred as electroweak interaction (EW). \\

The gauge transformation distinguishes chirality of fermions, in that $SU(2)_L$ electively acts on left-handed component, expressing the parity violaing nature of weak interaction:
\begin{align}
& \psi_{\mL} \ra e^{i\theta T+i\Theta Y} \psi_{\mL}   \\
& \psi_{\mR} \ra e^{i\Theta Y} \psi_{\mR}.
\end{align}
The Lagrangiaon arrives at:
\begin{align}
\mathcal{L}_{\mathrm{EW}} & = -\frac{1}{4} \bm{\hat{W}}_{\mu\nu} \bm{\hat{W}}^{\mu\nu}-\frac{1}{4}B_{\mu\nu} B^{\mu\nu} + \,\, \bar{\psi}(i \gamma_\mu D_\mu - m) \psi + \mathrm{h.c.}, \nn \\
%
D_\mu & := \partial_\mu + ig \sum_{a=1}^3 W^a_{\mu} \tau_a  + ig^{'} \frac{Y}{2} B_\mu \nn \\
\bm{\hat{W}}_{\mu\nu} & = \partial_\mu \bm{W}_\nu - \partial_\nu \bm{W}_\mu - g \bm{W}_\mu \times \bm{W}_\nu \nn \\
\bm{W}_\mu & := \{W^a_\mu; a=1,2,3\} \nn \\
B_{\mu\nu} & = \partial_\mu B_\nu - \partial_\nu B_\mu
\label{eq::EWLag1}
\end{align}
where $W^a_\mu$ and $B\_mu$ are the fields of EW gauge bosons, and $g, g^{'}$ are the coupling respectively for $SU(2)_L$ and $U(1)_Y$. $\bm{\tau} (= \bm{\sigma}/2)$ are generators of $SU(2)$. \\

The Lagrangian can be also re-written by introducing currents:
\begin{align}
\mathcal{L}_{\mathrm{EW}} & = -\frac{1}{4} \, \sum_{a-1}^3 W^a_{\mu\nu} W^{a\mu\nu}-\frac{1}{4}B_{\mu\nu} B^{\mu\nu} \nn \\
& -\frac{g}{2} (J_\mu^+ W^{-\mu} + J_\mu^- W^{+\mu}) -g J_\mu^3 W^{3\mu} - \frac{g^{'}}{2}J_\mu^YB^\mu  + \mathrm{h.c.}  \nn \\
 J^{\pm}_\mu & := \frac{1}{\sqrt{2}} (W_\mu^1\mp iW_\mu^2) \nn \\
 J^a_\mu & := \bar{\psi}_L \gamma^\mu \tau_q W_\mu^a \psi_L \nn \\
 J^Y_\mu & := Y \bar{\psi}_L \gamma^\mu  \psi_L.
\label{eq::EWLag2}
\end{align}
$J_\mu^\pm$ represent currents changing $T_3$, while $J_\mu^0$ and $J_\mu^Y$ neutral current conserving either $T_3$ and $Y$. \\

The consequence of EW symmetry breaking is implemented by mixing $(W_\mu^3, B_\mu)$ into $(Z_\mu, A_\mu)$:
\begin{align}
\colv{Z_\mu \\ A_\mu} := 
     \left(
   \begin{array}{cc}
     \cos{\theta_W} &  -\sin{\theta_W} \\ 
     \sin{\theta_W} &  \cos{\theta_W} 
   \end{array}
     \right)
 \colv{W_\mu^3 \\ B_\mu}
\end{align}
with a mixing angle (Weinberg angle $\theta_W$) of:
\begin{align}
\tan{\theta_W} := \frac{g^{'}}{g}.
\end{align}
%
The current terms in the Lagrangian Eq. (\ref{eq::EWLag2}) then becomes:
\begin{align}
 & -\frac{g}{2} (J_\mu^+ W^{-\mu} + J_\mu^- W^{+\mu})  \nn \\
 & + \frac{g}{\cos{\theta_W}} \left( -\cos^2{\theta_W} J_\mu^3 + \frac{\sin^2{\theta_W}}{2} J_\mu^Y  \right) \, Z^{\mu}  \nn \\
 & - g\sin{\theta_W} \left( J_\mu^3 + \frac{1}{2} J_\mu^Y \right) A^\mu  \nn \\
\end{align}
By choosing $Y := 2(Q-T^3)$, $A_\mu$ is associated with the gauge field of electromagnetic interaction, 
and the electric charge is found to be related to the weak coupling constant by the Weinberg angle: $e=g\sin{\theta_W}$.


\subsubsection{Electroweak Symmetry Breaking and the Higgs boson}
One outstanding problem in the EW Lagrangian is the prohibition of mass terms, for both gauge bosons and fermions, since they explicitly violates the gauge invariance. 
%since left-handed and right-handed fields obey different $SU(2)_L$ gauge transformation:
%\begin{align}
%  m\bar{\psi}\psi = m \bar{\psi}_{\mL} \psi_{\mR}.
%\end{align}
The BEH mechanism is then employed to solve the problem, where assuming a $SU(2)$ doublet $\phi$ ($Y=-1, T=1/2$) with scalar fields $\phi=(\phi_1,\phi_2)=(\phi^+,\phi^0)$,  and a potential $V(\phi)$ added in the Lagrangian:
\begin{align}
\mathcal{L}_{\mathrm{Higgs}} & := \left(D_\mu \phi \right)^\dg \left(D^\mu \phi \right) - V(\phi) \nn \\
                     V(\phi) & := \mu^2 \phi^\dg \phi + \lambda (\phi^\dg \phi)^2.
\label{eq::HiggsPotential}
\end{align}
While the minimum of the potential is always found in $\phi=(0,0)$ in the $\phi_1-\phi_2$ plane when $\mu^2>0$, 
negative $\mu^2$ leads to non-trivial minima in $v := |\phi|^2 = -\mu^2/2\lambda$ causing the shift of the vacuum expectation value : $<\phi>=0 \ra v$ (spontaneous symmetry breaking). \\

Retaining the origin of $\phi$ as:
\begin{align}
\phi = \colv{0 \\ v+h(x)}
\label{eq::SSB}
\end{align}
and applying the $\partial\mu \ra D\mu$ prescription to Eq. (\ref{eq::HiggsPotential}),
one finds the mass terms for $W,Z$ as:
\begin{align}
m_W & = gv/2 \nn \\
m_Z & = \sqrt{g^2+g^{' 2}} v/2, \nn
\end{align}
where the mass for $W$ and $Z$ is successfully provided.\\

The mass for scalar field $h$ is also found to be:
\begin{align}
m_h & = \sqrt{-2\mu^2}.  \nn
\end{align}
thus $h$ can be also regarded is physical mode, referred as higgs particle. \\

The fermion masses are fed by adding extra terms into Lagrangian:
\begin{align}
\mathcal{L}_\mathrm{Yukawa} := -\bar{\psi}_{i,L} y^{a}_{ij} \phi_a \psi_{j,R} -\bar{\psi}_{i,R} y^{a}_{ij} \phi^\dg_a \psi_{j,L} \nn 
\end{align}
where $i, j=1, 2, 3$ index the generations of fermions, and $a$ denote type of fermions (``up-type quarks'',``down-type quarks'',``leptons'') and $\phi_a$ corresponds to $\phi, \phi^{c}, \phi$ respectively. $y^a_{ij}$ are Yukawa matrices, $3\times 3$ matrices spanned over the family space.
The off-diagonal components in $y^a_{ij}$ are responsible for mixing between generation, which are all zero for leptons, while they are non-zero and the Yukawa matrices are diagonalized by CKM matrix in case of quarks. \\

Inserting Eq. (\ref{eq::SSB}), $\mathcal{L}_{\mathrm{Yukawa}}$ is finally reduced to:
\begin{align}
\mathcal{L}_{\mathrm{Yukawa}} & = \sum_f y_f v \bar{\psi} \psi + y_f \bar{\psi}{\psi} h \nn \\
& = \sum_f m_f \bar{\psi} \psi + y_f \bar{\psi}{\psi} h,
\end{align}
where $f$ is the index of fermions and $y_f$ is the eigenvalues of corresponding Yukawa matrix. \\


Higgs boson (Breit-Englere-Higgs boson) was discovered in LHC in 2012, bringing the last piece of the Standard Model in human knowledge. Measurements on its properties including the mass, spin and coulplings follow, and they show all consistent to the SM at the moment. Precision measurement on its couplings is planning in the later stages in LHC.
%\begin{align}
%\end{align}


%\subsubsection{Standard model processes in LHC}

%\subsubsection{Summary of the SM Lagrangian}
%The complete list of the SM Lagrangian is given by:
%\begin{align}
%\mathcal{L}_{\mathrm{SM}}
% & = -\frac{1}{4} F_{\mu\nu} F^{\mu\nu} \\
% & = i \bar{\psi}\gamma_{\mu}D^\mu \psi + \mathrm{h.c.} \\
% & = \psi_i y_{ij} \psi_j + \mathrm{h.c.} \\
% & = |D_\mu \phi|^2 - V(\phi).
%\end{align}




%
%
\clearpage
\subsection{Remained problems for the Standard Model}
There a couple homework from the SM, from critical ones to potential issues.
A overview is provided in this sub-section, with a focus on problems in which SUSY is particularly motivated as the solution.


\subsubsection{Quadratic Divergence of Higgs Mass}
While the divergences that appears in higher order calculation in SM are universaly cured by renomrlization, the only exception is the mass of higgs boson \ref{fineTune_Barbieri1987}. Since the SM higgs has no parter that is in the same multiplet represented by a certain symmetry, there is no counter terms possible to cancel the leading quadratic divergence happening in the self-energy correction, forcing the higgs mass to explicitly contain the dependence on the cut-off scale $\Lambda$ upto which the loop momentum is integrated.
For instance, the loop correction given by a top-quarks loop is:
\begin{align}
\Delta m_h ^2 = - \frac{3|\lambda|^2}{8\pi^2} \Lambda^2 + O\,(\log\Lambda). 
\label{eq::naturalness1}
\end{align}
%
\begin{align}
m^2_{h,\mathrm{obs.}} = m^2_{h,\mathrm{bare}} + \Delta m_h ^2
\label{eq::naturalnessBare}
\end{align}
%The magnitude can be order of $10^{38} \gev^2$ when assuming SM is valid upto Planck scale: $\Lambda \sim 10^19$, or $10^{8} \gev^2$ with the more pessimistic scnario where SM is only valid upto around scale that has been experimentally explored: $\Lambda \sim 10\tev$. Given the experimental mass is $125\gev$, a naive conclusion is that the tree-level higgs mass $m_h^{\mathrm{bare}}$ and total radiative correction $\Delta_h$ has to cancel in a precision of $10^{-38 \sim -8}$.
The magnitude can be order of $10^{38}$ when assuming SM is valid upto Planck scale: $\Lambda \sim 10^{19}$. 
Given the experimental mass is $125\gev$, a naive conclusion is that the tree-level higgs mass $m_h^{\mathrm{bare} \, 2}$ and total radiative correction $\Delta_h$ has to cancel in a precision of $10^{-34}$ (``fine tuning problem'' or ``naturalness problem'').
It is highly unnatural for a theory to have a structure requiring such level of fine tuning in it, therefore it is preferred to conceive the underlying mechanism behind it.  \\
%\footnote{Such as the fine tuned mass splitting of proton between neutron, which is naturally driven by the approximate symmetry in iso-spin.}
%\footnote{Fish does try to invent a Schrodinger eq. that naturally predicts a world with only water, because they know it!}
The simplest solution is provided by the Pauli-Villars prescription where a set of partners giving the negative contribution are newly introduced and stop the divergence by the destructive interference. SUSY is the typical example. By introducing a bosonic partner of top-quark (scalar-top; ``stop'') with the mass of $m_S$ and the same coupling to higgs, the quadratic terms cancel out:
%SUSYはまさにこのタイプにあたり、higgsと同じcouplingでint.するboson版のpartnerを回すことによってloop correctionはこうなる。
%for the particles in the loop of higgs self-energy, with the negative yet identical magnitude 
\begin{align}
\tilde{\Delta} m_h ^2 & =  2 \times \frac{3|\lambda|^2}{16\pi^2} \Lambda^2 + O\,(\log\Lambda) \nn \\
\Delta m^2_{h, \mathrm{PV}} 
&  = \Delta m_h ^2 + \tilde{\Delta} m_h ^2  = O\,(\log\Lambda)
%&  = - \frac{\lambda}{8\pi^2} m_S^2 \log{\left(\frac{\Lambda}{m_S}\right)},
\label{eq::naturalness2}
\end{align}
restoring the safe description of perturbation. \\
%他にもhiggsにさらなるinternal structureがある考えてloopの積分を途中でtrancateするというやり方である。(composite higgs) technicolorなどのmodelがよくstudyされている



\subsubsection{Dark Matter}
%A number of observatory evidences has been offered impliing that the universe is filled with masses more than expected by the visible matter abandunce. 
Historically, the argument of dark matter (DM) originated from an observation implying a bulk of mass in a gallaxy center that can not accounted by visible masses measured based on eletromagnetic interaction. The DM hypothesis has been then verified by the gravitational lensing effect etc., and the density abundance is dedicatedly measured via cosmic micro background (CMB) by WMAP \cite{WMAP2013} and Planck \cite{Planck2015}:
\begin{align}
\Omega_h h^2 = 0.119 \pm 0.006 
\end{align}

DM is usually considered in the framework in Weakly Interacting Massive Particles (WIMPs) 
%\footnote{Less common though, frameworks involving warm DM \cite{} or Strongly Interacting Massive Particle (SIMP) \cite{SIMPMurayama} have been also proposed.}
that satisfying following conditions: 
\begin{itemize}
\item it senses very weak interaction except gravitation. 
\footnote{This almost requires electrically neutral, but completely forbidden \cite{chargedDM}.}
\item it is in non-relativistic motion, given that DM is relatively spatially localized in galaxy center etc.
\end{itemize}
While the SM has no candidates for DM, SUSY prodives several attractive candidates assuming R-parity conservation, as seen in later sub-sections. 
%A number of new physics models are motivated by DM 他にもDM candidatesを含むNPはいっぱいある。Axionとか
There are also a number of experiments for direct detection, in which the SUSY DM scenario can be tested. \\



\subsubsection{Grand Unification}
It is the ultimate desire for phycists to explain all phenomena on the earth with a single principle. 
While in the SM, the EW symmetry breaking $SU(2) \times U(1) \ra SU(2)_L \times U(1)_Q$ implies a common origin of electromagnetic and weak interaction, this encourages such physists to conceive another unification together with strong interaction at a higher scale (Grand Unification; GUT). \\

Running coupling constants are useful proxies to analyze the possibility of unification.
The evolution of coupling constants along scale is given by the RGE:
\begin{align}
\dfrac{1}{\alpha_i(Q)^2}-\dfrac{1}{\alpha_i(Q_0)^2} = -\dfrac{\beta_i}{2\pi} \log{\left(\dfrac{Q}{Q_0}\right)},
\end{align}
with the indices $i=1,2,3$ denote strong, weak and electro-magnetic interaction respectively.
$\beta_i$ are the beta functions. In the SM at 1-loop level, these are:
\begin{align}
\colv{b_1 \\ b_2 \\ b_3} = \colv{ 1/10 \\ -43/6 \\ -11} + n_{\mathrm{gen}} \colv{4/3 \\ 4/3 \\ 4/3},
\end{align}
where $n_{\mathrm{gen}}$ is the number of generation of fermions, which is equal to $3$ for $Q>m_t$.
One naively expects an convergence of the three couplings at a certain scale ($\mu_{\mGUT}$) in case of grand unification.
Unfortunately, this does not happen in the SM, as demonstrated in Fig. \ref{fig::Introduction::GUT} (a).
However, it can be relatively easily realized in the SUSY regime, where contains more fermion particles which change the slope of the running. The beta function for MSSM is:
\begin{align}
\colv{b_1 \\ b_2 \\ b_3} = \colv{3/5 \\ 1 \\ -3},
\end{align}
and the coupling unification is achieved at $\mu_{\mGUT} \sim 10^{16}\gev$, as shown in Fig. \ref{fig::Introduction::GUT} (b).


\fig[100]{Introduction/GUT.pdf}
{Two-loop renormalization group evolution of the inversed gauge coupling $1/\alpha_i$ in case of SM (dashed lines), and a scenario in MSSM (solid lines) where the masses of SUSY partners are set between $500\gev$ and $1.5\tev$ \cite{SUSYPrimer}.}
{fig::Introduction::GUT}
%\subsubsection{Others}
%\paragraph{Strong-CP problem}
%かなりの精度で現在CPは保たれているのにSMではStrong CPVを禁止するschemeがなく、しかもunstableである。
%なのでSMにそういうschemeを入れるというideaが色々あり、その帰結としてaxionが出る。


%\paragraph{Bariogenethesis and CP violation}
%CKMの定式化とCroninの実験を通じて今やCPが破れているのはこの世の常識となりつつあるが
%matter-antimatter asymmetryを説明するほどのmagnitudeはない。よってextra sourceが必要である。
%SM-likeのframeworkの範疇ではhiggs sectorやneutrino sectorでのCPVが期待できる。特にneutrinoは破れていることが割ともう決定されそうである
%new physicsに頼る場合はSUSYとかaxionかなあ

%もっとどうにもならなそうな難しい問題としては
% parameter大杉, quark mass hieararchyなどがある
%\footnote{これは理論parameterにhieararchyがあるだけで、理論の中にfine tuneの構造があるわけではないという点でhiggs massよりは深刻ではない}
% 
% Gravity, 
% Dark Energy
% 

%\paragraph{}
%Essentially no clue.
%修正重力？
%一応そういうものがあった場合のtestは多少proposeされていて、LHCでttbarつかって見えるものもある
%, ???SUSY????????#parameters????, gravity, EW hierarchy problem, ??CP??}


%%%%%%%%%%%%%%%%%%
\subsection{The Super-Symmetry and MSSM}
%The key concept of the super-symmetry (SUSY) is to assign an invariace on theories against boson-fermion transformation:
%\begin{align}
%& \hat{Q} \, \ket{ \mathrm{Boson}}   = \xi \ket{\mathrm{Fermion}}  \nn \\ 
%& \hat{Q} \, \ket{ \mathrm{Fermion}} = \lambda \ket{\mathrm{Boson}}
%\end{align}
%このthesisではMSSMだけを基本的には扱う
Minimal Super-Symmetric Standard Model (MSSM) is a SUSY framework where decent minimum matter contents and degrees of freedom are newly introduced, so as:
\begin{itemize}
\item only one set of SUSY partners is employed ($\mathcal{N}_{\mathrm{SUSY}}=1$),
\item SUSY parters of SM fermions have the spin of 0, while the partners for boson in SM (gauge boson and higgs) are spin-1/2,
\item and use only two higgs doublets to denoting the higgs sector.
\footnote{This expansion is caused due to the fact that  SUSY Lagrangian can not be easily constructed in presence of the charged conjugated higgs doublet $\phi^c$ which plays the role feeding the mass to down-type fermions in SM.}
%superpotentialでSM Yukawaを表現するというSUSY lag. を作る際に発生する問題に起因する. homomorphyic?? ここでは深くは立ち入らない
\end{itemize}

Note though it is called ``minimal'', MSSM is a framework general enough to expressing the natures typical to SUSY in phenomenology level, and this thesis will only deal with MSSM accordingly.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Particle Contents in MSSM}
The particle contents are summarized in Tab. \ref{tab::Introduction::particleContMSSM}

\tab{ c c | c c c c c }
{
\hline
\multicolumn{2}{c|}{Super-multiplet}   & SM sect.              & SUSY partner                          & $n[SU(3)_C]$ &  $n[SU(2)_L]$ & $Y$ \\
\hline
\hline
gluon/gluino       & g             & g                     &  $\tg$                                &  8  &  1  &  0  \\ 
\hline
EW gauge boson /   & W             & $W^{\pm},W^0$         &  $\tW^{\pm},\tW^0$                    &  1  &  3  &  0  \\
EW gaugino         & B             & $B^{\pm},B^0$         &  $\tB^{\pm},\tB^0$                    &  1  &  1  &  0  \\
\hline
lepton / slepton   & L             & $(\nu_e,e)_\mL$       &  $(\tilde{\nu}_e,\tilde{e})_L$        &  1  &  2  &  -1  \\ 
                   & e             & $\te_\mR$             &  $e_\mR$                              &  1  &  1  &  -2  \\ 
\hline
quark / sqaurk     & Q             & $(u_\mL,d_\mL)$       &  $\left(\tu_\mL, \td_\mL \right)$     &  3  &  2  &  1/3  \\
                   & u             & $u_\mR$               &  $\tu_\mR$                            &  3  &  1  &  4/3 \\
                   & d             & $d_\mR$               &  $\td_\mR$                            &  3  &  1  & -2/3  \\
\hline
Higgs boson /      & $H_\mup$      & $(H_\mup^+,H_\mup^0)$ & $(\tH_\mup^+,\tH_\mup^0)$             &  1  &  2  &  1  \\
higgsino           & $H_\mdn$      & $(H_\mdn^0,H_\mdn^-)$ & $(\tH_\mdn^0,\tH_\mdn^-)$             &  1  &  2  & -1  \\
\hline
}
{
Matter content of MSSM. The left column defines the naming convention for SUSY particles. $n[SU(3)_C]$($n[SU(2)_L]$) represents the degree of freedom of the $SU(3)_C$($SU(2)_L$) multiplet that the field(s) belongs to. All of them belongs to the single of $U(1)_Y$, thus the $U(1)$ charge $Y$ is shown instead. There are also two set of replications for the 2nd and 3rd generation of (s)quarks/(s)leptons, which are not shown here.
}
{tab::Introduction::particleContMSSM}

Note that sfermions has two modes indexed by $L,R$ indicating they reside in super-multiplet with left-handed or right-handed SM fermions. On the other hand, gauginos are all Majorana, in order to match the degree of freedom with either the patner SM gauge bosons and higgs.

MSSM higgs sector has two higgs doublets ($\bm{H}_\mup := (H_\mup^+,H_\mup^0)$, $\bm{H}_\mdn := (H_\mdn^-,H_\mdn^0)$) with their own vacuum expectation values (VEV):
\begin{align}
 v_\mup :=  \left< H^0_\mup \right>, \,\,\,\,  v_\mdn :=  \left< H^0_\mdn \right>, \nn
\end{align}
where each provides the masses for up- or down-type fermions respectively. The splitting of the them is commonly parametrized using a mixing angle $\beta$:
\begin{align}
\tan\beta := v_\mup/v_\mdn.
\end{align}
The consistency with SM EW symmetry breaking is ensured by relating the VEVs as 
\begin{align}
v^2_{\mathrm{SM}} = v^2_{\mathrm{u}} = v^2_{\mathrm{d}}.
\end{align}

Note that if gravity is quantized in the picture of QFT, there should be also the corresponding gauge boson "graviton" and along a natural extension, its SUSY partner "Gravitino". Depending on SUSY breaking scenario, gravitino can act a important role such as GMSB where gravitino is LSP. Note that gravitino problem in cosmology.


%%%%%%%%%%%
\subsubsection{The MSSM Lagrangian}
Construction of a super-symmetric Lagrangian is not as a simple extension from SM Lagrangian as just adding extra terms accounting for the particle contents. It is commonly done using the method of super-potential or super-space. The MSSM Lagrangian is then found to be:
\begin{align}
\mathcal{L}^{\mathrm{MSSM}}
& = \mathcal{L}^{\mathrm{MSSM}}_{\mathrm{SUSY}} + \mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}},
& = \frac{1}{4} F^a_{\mu\nu} F^{a\mu\nu} \\
& + \psi^\dg \bar{\sigma}^\mu D_\mu \psi - i \lambda^{\dg a} \bar{\sigma} D_\mu \lambda^a \\
& + D^\mu\phi^* D_\mu \phi - V(\phi,\phi^{*}) \\
&  -\frac{1}{2} W^{ij} \psi_i \psi_j + h.c. \\
&  - \sqrt{2} g (\phi^{*}T^a\psi) \lambda^a + h.c. \\ 
& + \mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}},
\label{eq::MSSMLag}
\end{align}
including the kinetic terms for gauge bosons, SM fermions and sfermions (first two lines), 
the SM higgs potential (third line), 
the Yukawa terms for SM fermions (fourth line),
interaction terms of gauginos (fifth line),
and the soft breaking terms accommodating (sixth line, explained below). \\

The derivation is not shown in this thesis, however there are a couple of caveat remarks:
\begin{itemize}
\item SUSY is broken, and it is via ``soft breaking'' \\
While an exact super-symmetry requires the SUSY partners being in the identical masses with respect to the SM particles, which is at least not the case in the energy scale in our universe, since no SUSY particles have been observed so far. Therefore, a realistic model must include the SUSY breaking at EW scale as an effective theory, with SUSY breaking term in the Lagrangian ($\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}}$ in Eq. \ref{eq::MSSMLag}). On the other hand, we don't want to ruin the desired features in SUSY, particularly as the prescription of the higgs mass divergence. It is then common to restrict the SUSY breaking in a form of soft breaking where higgs quadratic divergence is cured, with particular choice of terms in building $\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}}$.
%This is a powerful requirement that only allows terms with 
%\begin{align}
%M_a\lambda^a \lambda^a, a^{ijk} \phi_i \phi_j \phi_k,  b^{ij}\phi_i \phi_j, t^i\phi
%\end{align}
%in the ragragian.
%
Some models also offer the mechanism of the SUSY breaking from the most minimal ones such as GMSB, AMSB, SUGRA, to next-to-minimal ones including their generalization: NUMH, GGM. Their SUSY breaking terms also satisfy the requirement of soft breaking.


\item R-parity  \\
A quantum number $R$ associated with the number of ``SUSY partner'' can be defined by the spin, baryon number and lepton number as:
\begin{align}
R := (-1)^{3(B-L)+2S}.
\end{align}
The corresponding symmetry is referred to R-parity, which conservation law will prohibit a SUSY particle from decaying only into SM particles, as well as SM particles annihilating into a resonance of SUSY particle. 
This ends up in a set of spectacularly important consequences: 
\begin{itemize}
\item the lightest SUSY particles (LSP) become DM candidates if they are electric neutral, particularly the lightest neutralino,
\item proton decays via diagrams in Fig. \ref{fig::Introduction::protonDecay} are naturally suppressed, coping with stringent constraints by experiments.
\end{itemize}
Therefore phenomenologically R-parity is highly motivated.
In the framework of MSSM, the R-parity conservation (RPC) is explicitly assumed, which is equivalent to discard following terms in the most general soft breaking Lagrangian:
\begin{align}
W_{\Delta L=1} & = \frac{1}{2} \lambda^{ijk} L_i L_j \bar{e}_\mathrm{k} + \lambda^{'\, ijk} L_i Q_j \bar{d}_k + \mu^{'i} L_i H_\mup \nn \\
W_{\Delta B=1} & = \frac{1}{2} \lambda^{'\, ijk} \bar{u}_i  \bar{d}_j  \bar{d}_k.
\label{eq::RPVterms}
\end{align}

%%%
%\fig[100]{Introduction/protonDecay}
%{}
%{fig::Introduction::protonDecay}
%%%

%It is however also true that RPC is not a kind of requirement that 
%R-parity violating scenarioも実験では探索されている. 
\end{itemize}

The soft breaking terms ($\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}}$) contains the most phenomenologically critical information in MSSM. The most general form in MSSM is given as \cite{SUSYPrimer}:
%
\begin{align}
\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}} 
& = \frac{1}{2} \left( M_3 \, \tg\tg + M_2 \tW\tW + M_1 \tB\tB + \mathrm{c.c.}  \right)  \mbox{\phantom{MMMMMMM} (gaugino mass terms)}  \label{eq::MSSM_gauge} \\
%
& - \tQ^{\dg} \, \bm{m}_{\tQ}^2    \, \tQ 
  - \tL^{\dg} \, \bm{m}_{\tL}^2    \, \tL 
  - \tubar    \, \bm{m}_{\tubar}^2 \, \tubar^{\dg} 
  - \tdbar    \, \bm{m}_{\tdbar}^2 \, \tdbar^{\dg} 
  - \tebar    \, \bm{m}_{\tebar}^2 \, \tebar^{\dg}  \mbox{\phantom{M}(sfermion mass terms)  }  \label{eq::MSSM_sfermions} \\
%
& -  \left( 
  \tubar \bm{a}_\mup \tQ H_\mup 
- \tdbar \bm{a}_\mdn \tQ H_\mdn 
- \tebar \bm{a}_e    \tL H_\mdn 
+ \mathrm{c.c.}  \right)  \mbox{\phantom{MMMMMM} (trilinear coupling)} \label{eq::MSSM_trilinear} \\
%
& - m^2_{H_\mup} H_{\mup}^{\dg} H_\mup  
  - m^2_{H_\mdn} H_{\mdn}^{\dg} H_\mdn  
  - (b H_\mup H_\mdn + \mathrm{c.c.})   \mbox{\phantom{MMMMMM} (Higgs potential)} \label{eq::MSSM_higgs}
%
%\label{eq::MSSMLag}
\end{align}
The notation of the particle fields follow the definition in Tab. \ref{}.
The first line Eq. (\label{eq::MSSM_gauge}) show the mass terms for gauginos, with $M_1$, $M_2$ and $M_3$ are respectively bino, wino and gluino mass. Eq. (\label{eq::MSSM_trilinear}) and (\label{eq::MSSM_sfermions}) are the Yukawa terms for SUSY particles where the former are the standard sfermion mass terms, and latter the trilinear terms describing the Yukawa interaction coupling left-handed and right-handed sfermions, emerged as the cross terms of super-multiplet. The mass matrices ($\bm{m}_{\tQ}$, $\bm{m}_{\tL}$, $\bm{m}_{\tubar}$, $\bm{m}_{\tdbar}$, $\bm{m}_{\tebar}$), and the A terms ($\bm{a}_\mup$, $\bm{a}_\mdn$ and $\bm{a}_e$) are $3\times3$ matices spanned in family space, equivalent to the CKM matrix in the SM sector multiplied by sparticles masses. The last terms \label{eq::MSSM_higgs} are the MSSM higgs potential, controlling the EW symmetry breaking. 
% trilinear = helicity flipping term by higgs interaction in SM. In MSSM, left/right is differentiated so this can not be understand as mass terms of generic interaction term


\subsubsection{Mass spectra in MSSM}
The masses of SUSY particles are derived by specifying the coefficient associated with mass terms (e.g. $m$ in $m\phi\phi$), after a full expansion of the Lagrangian Eq. (\ref{eq::MSSMLag}). This is effectively done by extracting relevant terms and performing the diagonalization of the mass matrices, accounting for the mixing between eigenstates of interactions. \\

\paragraph{Squarks and sleptons}
Sfermion masses are fed solely from the soft Lagrangian. Generally, they are allowed to mix between different generations via the off-diagonal components either in the mass matrices or the A terms. These are however known to lead to a significant rate of flavor changing natural current and CP violation, therefore they are experimentally highly constrained and usually set to zero:
\begin{align}
&  \bm{m}^2_{\tQ}    = m^2_{\tQ} \,    \bm{1}, \,\,\,\,\,\,
  \bm{m}_{\tL}^2    = m_{\tL}^2 \,    \bm{1}, \,\,\,\,\,\,
  \bm{m}_{\tubar}^2 = m_{\tubar}^2 \, \bm{1}, \,\,\,\,\,\,
  \bm{m}_{\tdbar}^2 = m_{\tdbar}^2 \, \bm{1}, \,\,\,\,\,\,
  \bm{m}_{\tebar}^2 = m_{\tebar}^2 \, \bm{1}, \,\,\,\,\,\, \nn \\
%
& \bm{a}_\mup = A_\mup \, \bm{1}, \,\,\,\,\,\,
  \bm{a}_\mdn = A_\mdn \, \bm{1}, \,\,\,\,\,\,
  \bm{a}_e    = A_e \, \bm{1}
\end{align}
In addition, it is also allowed to mix left-handed sfermion and right-handed sfermion since they share the same gauge quantum number. Ignoring the off-diagonal components of the Yukawa matrix, the mass matrix for sfermion $\tilde{f}$ is given by:
\begin{align}
& \left(  
  \begin{array}{cc}
    m^2_{\tilde{f}_{\mL}} + m_Z^2 \,(T_{3,f}-Q_f\sin{\theta_W}^2) \cos{2\beta} + m^2_f    &  v_f ( A_f - \mu y_f)         \\
    v_f ( A_f - \mu y_f)                &     m^2_{\tilde{f}_{\mR}} + m_Z^2 \, Q_f\sin{\theta_W}^2\cos{2\beta} + m^2_f    
  \end{array} 
\right), \nn  \\
%
& v_f = \begin{cases}
  v_{\mup} \mbox{\phantom{MMM}} (\tilde{f}=\tilde{u}, \tilde{c}, \tilde{t}) \\
  v_{\mdn} \mbox{\phantom{MMM}} (\tilde{f}=\tilde{d}, \tilde{s}, \tilde{b}) 
     \end{cases}
%
\end{align}
where $T_{3,f}$ and $Q_f$ are the iso-spin and electric charge of $\tilde{f}$. As the magnitude off-diagonal component scales with the Yukawa coupling, the effect of the mixing is only sizable for the case of third generation sfermions (stop, sbottom and stau).
%The splitted two mass eigenstates are often referred to $f_1$ and $f_2$ with $m_2>m_1$.
This is why the third generation sfermions are particularly phenomenologically important,   
since the masses of lighter eigenstates can be significantly lowered by the splitting, enhancing the change for being within experimental reach.  \\



\paragraph{Gauginos}
The mass terms for Higgsinos are given from $\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{SUSY}}$, while the masses of other gauginos are defined in $\mathcal{L}^{\mathrm{MSSM}}_{\mathrm{soft}}$. The eigenstate of charged EW gauginos (charginos; $\tW^\pm, \tH^+_\mup, \tH^-_\mdn$) in the same signs will mix each other.
The mass matrices are common and described as:
\begin{align}
\left(  
  \begin{array}{cc}
    M_2                    &  \sqrt{2} m_W \sin{\beta} \\
    \sqrt{2} m_W \sin{\beta} &  \mu                    \\
  \end{array}
\right). \nn \\ \nn
\end{align}

The diagonalized mass engenstates are then:
\begin{align}
m_{\tilde{\chi}_{1,2}^{\pm}}^2 = \dfrac{1}{2} \left[ (M_2^2+\mu^2+2m_W^2) \mp \sqrt{(M_2^2+\mu^2+2m_W^2)^2 - 4(\mu M_2-m_W^2\sin2\beta)^2} \right].
\end{align}

The mass matrix for neutral EW gauginos (neutralinos; $\tB, \tW^0, \tH^0_\mup, \tH^0_\mdn$) are given as:
\begin{align}
\left(  
  \begin{array}{cccc}
    M_1                       &  0                         & -\cos\beta\sin\theta_W m_Z &  \sin\beta\sin\theta_W m_Z   \\
    0                         &  M_2                       &  \cos\beta\cos\theta_W m_Z & -\sin\beta\cos\theta_W m_Z   \\
   -\cos\beta\sin\theta_W m_Z &  \cos\beta\cos\theta_W m_Z &  0                         & -\mu                         \\
    \sin\beta\sin\theta_W m_Z & -\sin\beta\cos\theta_W m_Z & -\mu                       &  0                           \\
  \end{array}
\right). \nn \\ \nn
\end{align}
The eigenfunction is quartic and the solutions are :
\begin{align}
m_1 & = M_1 + \frac{m_Z^2\sin^2{\theta_W}}{M_1^2-\mu^2} (M_1 + \mu+\sin2\beta)   \nn \\
m_2 & = M_2 + \frac{m_Z^2\cos^2{\theta_W}}{M_2^2-\mu^2} (M_2 + \mu+\sin2\beta)   \nn \\
m_3 & = \mu + \frac{m_Z^2(1+\sin{2\beta})}{2(\mu-M_1)(\mu-M_2)} (\mu-\cos{\theta_W}M_1-\sin{\theta_W}M_2)  \nn \\
m_4 & = \mu + \frac{m_Z^2(1-\sin{2\beta})}{2(\mu+M_1)(\mu+M_2)} (\mu+\cos{\theta_W}M_1+\sin{\theta_W}M_2)  \nn \\
\end{align}
A common notation of neutralino mass is with
$m_{\tilde{\chi}_{1-4}^0}$ where $m_{\tilde{\chi}_{1}^0}<m_{\tilde{\chi}_{2}^0}<m_{\tilde{\chi}_{3}^0}<m_{\tilde{\chi}_{4}^0}$

Finally, gluinos are color-octet fermions and do not mixed to any other sfermions. \\



\paragraph{Higgs sector of MSSM}
% higgs sector
Due to the two higgs doublets with 4 real and 4 imaginary parts, 
there are in total five degree of freedoms as particles after the gauge fixing.
The MSSM higgs potential is given by:
\begin{align}
V(H,H) 
& = \left( |\mu|^2 + m^2_{H_\mup} \right)  \left( |H^0_\mup|^2+|H^+_\mup|^2 \right) \nn \\
& + \left( |\mu|^2 + m^2_{H_\mdn} \right)  \left( |H^0_\mdn|^2+|H^-_\mdn|^2 \right) \nn \\
& + \left[ b (H^+_\mup H^-_\mdn-H^0_\mup H^0_\mdn) + \mathrm{c.c.} \right] \nn \\
& + \frac{1}{8} \left(g^2+g^{'2} \right) \left(   |H^0_\mup|^2+|H^+_\mup|^2-|H^0_\mdn|^2-|H^-_\mdn|^2   \right) ^2  \nn \\
& + \frac{1}{2} |H^+_\mup H^{0*}_\mup + H^+_\mdn H^{-*}_\mdn|. 
\label{eq::MSSMhiggsP}
\end{align}
Similarly to the case in SM, implementing the spontaneous symmetry breaking with $H_{\mup,\mdn} \ra v_{\mup,\mdn} + \eta_{\mup,\mdn}$and requiring $dV/dv_\mup = dV/dv_\mdn = 0$, one arrives:
\begin{align}
\sin{2\beta} & = \dfrac{2b}{m^2_{H_\mup} + m^2_{H_\mdn} + 2|\mu|^2 } \label{eq::MSSMSSB1} \\ 
\frac{1}{2} m_Z^2 & = -|\mu|^2 + \dfrac{m^2_{H_\mup}-m^2_{H_\mdn}\tan^2{\beta}}{\tan^2{\beta}-1} \label{eq::MSSMSSB2}
\end{align}
The higgs masses are found by the masses terms with inserting Eq. (\ref{{eq::MSSMSSB1}})-(\ref{{eq::MSSMSSB2}}) back to Eq. (\ref{{eq::MSSMhiggsP}}):
\begin{align}
m^2_{A} & = 2 |\mu|^2 + m^2_{H_\mup} + m^2_{H_\mdn}, \nn \\
m^2_{H^{\pm}} & = m^2_{A^0} + m^2_W \nn \\
m^2_{h,H} & = \dfrac{1}{2} \left( m^2_{A^0} + m_Z^2 \mp \sqrt{(m^2_{A^0} + m^2_Z)^2-4m_Z^2 m^2_{A^0}\cos^2{2\beta}}   \right),
\label{eq::MSSMhiggsMass}
\end{align}
where $H^{\pm}$ is the charged, $A$ the CP-odd higgs respectively. 
$H$ and $h$ are the mass eigenstates of CP-even neutral higgs, where the lighter one $h$ is often associated with the SM higgs. Given that no observation of $H$ has been claimed upto $400\gev-1\tev$, it is generally preferred to have large mass splitting between $h$ and $H$, which implies a large $\tan{\beta}$. \\





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Running masses and GUT}
Though the SUSY masses are mostly free parameters in MSSM, 
an useful insight can be obtained from an quick analysis under the GUT regime in which the coupling constants unify at the GUT scale: $\mu_{\mathrm{GUT}}\sim 10^{16-17}\gev$. \\

In the SUSY context, the mass unification is often in addition considered, typically under the regime where:
\begin{itemize}
\item all sfermions masses converge to $m_1/2$
\item all gaugino masses converge to $m_0$
\item all higgs boson ($H_u$, $H_d$) masses converge to $(\mu^2+m_0^2)^{1/2}$.
\end{itemize}
This configuration is particular advantageous in naturally causing EW symmetry breaking at the EW scale, and adopted in many minimal models including SUGRA, NUMH and cMSSM. \\

Starting with gaugino masses, using the general condition satisfied in 1-loop renormalization:
$$
\frac{d(M_i/\alpha_i)}{d\mu} = 0, \,\,\,\, (i=1,2,3),
$$
it turns that $(M_i/\alpha_i)$ is constant in arbitrary scale. Therefore, one obtains
\begin{align}
\frac{M_i}{\alpha_i}|_{\mu=\mu_{\mathrm{EW}}}  = \frac{M_i}{\alpha_i}|_{\mu=\mu_{\mathrm{GUT}}} = \frac{m_{1/2}}{\alpha_{\mathrm{GUT}}}
\end{align}
which results an univeral ratio in gaugino masses holding at any scale:
\begin{align}
M_1 : M_2 : M_3 \sim 6:2:1.
\end{align}
This is the reason the mass hieararchy between gluino, wino and bino are especially motivated and commonly assumed in SUSY phenomenology, though it is true that the assumption of mass unification may too strong. \\

As for sfermions, the running masses also provide some idea about the mass spectra at the EW scale
The running masses are calculated unambiguously using the renormalization group equations as:
\begin{align}
m_{\tilde{d}_{\mathrm{L}}}^2   & = m_0^2 + K_3                    + K_2                    + \frac{1}{36} K_1 + \Delta_{\tilde{d}_{\mathrm{L}}}   \nn  \\ 
m_{\tilde{u}_{\mathrm{L}}}^2   & = m_0^2 + K_3                    + K_2                    + \frac{1}{36} K_1 + \Delta_{\tilde{u}_{\mathrm{L}}}   \nn  \\ 
m_{\tilde{d}_{\mathrm{R}}}^2   & = m_0^2 + K_3                    + \mbox{\phantom{$K_2$ +}} \frac{1}{9} \,\,\,  K_1 +  \Delta_{\tilde{d}_{\mathrm{R}}}   \nn  \\ 
m_{\tilde{u}_{\mathrm{R}}}^2   & = m_0^2 + K_3                    + \mbox{\phantom{$K_2$ +}} \frac{4}{9} \,\,\,  K_1 +  \Delta_{\tilde{u}_{\mathrm{R}}}   \nn  \\ 
m_{\tilde{e}_{\mathrm{L}}}^2   & = m_0^2 + \mbox{\phantom{$K_3$ +}} K_2                    + \frac{1}{4} \,\,\,  K_1 +  \Delta_{\tilde{e}_{\mathrm{L}}}   \nn  \\ 
m_{\tilde{\nu}_{\mathrm{L}}}^2 & = m_0^2 + \mbox{\phantom{$K_3$ +}} K_2                    + \frac{1}{4} \,\,\,  K_1 +  \Delta_{\tilde{\nu}_{\mathrm{L}}} \nn  \\ 
m_{\tilde{e}_{\mathrm{R}}}^2   & = m_0^2 + \mbox{\phantom{$K_3$ +}} \mbox{\phantom{$K_2$ +}} \mbox{\phantom{kk}}\, K_1 + \Delta_{\tilde{e}_{\mathrm{R}}}   \nn  \\ 
\end{align}
where $K_1$, $K_2$ and $K_3$ respectively denotes the contribution from the interaction of $U(1)_Y$, $SU(2)_L$ and $SU(3)_C$, which are approximately:
\begin{align}
K_1 \sim 0.15 \, m_{1/2}^2, \,\,\, K_1 \sim 0.5 \, m_{1/2}^2, \,\,\, K_3 \sim 6 \, m_{1/2}^2,
\end{align}
and the correction factors $\Delta_{\tilde{f}}$ are given by:
\begin{align}
\Delta_{\tilde{f}_\mathrm{L}} & = (T_3-Q\sin^2{\theta_W}) \, m_Z^2 \cos{2\beta} + m_f^2   \nn \\
\Delta_{\tilde{f}_\mathrm{R}} & = Q\sin^2{\theta_W} \, m_Z^2 \cos{2\beta} + m_f^2.   \nn \\
\end{align}
Since the running effect on masses are always larger for squarks than sleptons due to the $SU(3)_C$ interaction, 
it generally implies lighter masses for sleptons.
%Also, it is shown that there is no strong flavor dependence in the running, given that $m_0 \sim O(TeV)$.
The typical running mass spectra is shown in Fig. \ref{fig::Introduction::runningMass}.

%%%%%
\fig[100]{Introduction/runningMass.pdf}
{Evolution of scalar and gaugino mass parameters in the MSSM with mSUGRA boundary conditions [nob30]. The parameters are $m_0=200 \gev$, $m_{1/2}=600 \gev$, $A^0=-600\gev$, $\tan{\beta}=10$ and sign($\mu$)$>0$.}
{fig::Introduction::runningMass}
%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Constraints on SUSY so far} 
\subsubsection{Constraints from Observed Standard Model Higgs Mass}
It is a string fact that in MSSM the mass of 125 GeV higgs ($h$) is bounded by:
\begin{align}
m_{h} < m_Z \cos{2\beta} < m_Z = 91.2 \gev,
\end{align}
according to Eq. (\ref{{eq::MSSMhiggsMass}}).
Therefore, a sizable radiation correction is needed to achieve the 125 GeV.
The 1-loop correction is dominantly given by the remnant of cancellation of top and stop loop in Eq. (\ref{eq::naturalness2}):
\begin{align}
\Delta m_h^2 := \frac{3}{4} \frac{m_t^4}{v_{\mSM}^2} \left[ \log{\frac{m_{\ttop}^2}{m_t^2}} + \frac{X_t^2}{m_{\ttop}^2} \left(1-\frac{X_t^2}{12m_{\ttop}^2} \right)  \right],
\end{align}
which has to match with 
\begin{align}
\sqrt{(125\gev)^2 - m_Z^2} \sim 85\gev.
\end{align}
This is a tremendously powerful constraint that forces either of following two ambivalent choices:
\begin{enumerate}
\item without assuming anything on stop mixing (e.g. $X_t$ is free) and $O(10\tev)$ of stop mass, with relatively serious fine tuning.
\item maximal stop mixing ($X_t \sim \sqrt{6}m_{\ttop}$), and $500\gev-1\tev$ of stop mass, with mild fine tuning.
\end{enumerate}
The consequence implication of 1. is that all the squrks and sleptons are heavy, and only gauginos could be explorable in LHC,
while 2. leads to a firm conclusion that only stop (or sbottom) is accessible by the LHC energy.

The higgs mass fine tuning argument in MSSM is rather subtle, since the observed $m_{h}$ is no longer 
as straightforwardly associated with its own mass parameter $H_\mup$ as in the case in SM, but also involved by the other MSSM parameters as seen in Eq. (\ref{{eq::MSSMhiggsMass}}).
The magnitude of fine tuning is usually quoted by the linear response of all MSSM parameters:
\begin{align}
\Delta_{m_{h}} := \max_i \left|  \frac{\partial \log [m^2_h (\mbox{1-loop})] }{\partial \log{p_i}} \right|.
\end{align}
In scenario 1. above, the resultant fine tuning is typically $\Delta_{m_{h}} \sim O(10^{-5})$, while $\sim 10\%$ is achievable in the scnerio 2. with $\sim 600\gev$ stop.  \\
%
As a level of $\sim O(10^{-5})$ of the fine tuning is not as fatal as that in the SM ($10^{38}$),
in the thesis, we pursue the scenario 1. by probing gluino in the experiment, with assuming all the sfermions are decoupled.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection{Constraint from Dark Mater Relic density and Detection experiments.}
\subsubsection{Constraint from Dark Mater Relic Density} \label{sec::Introduction::DMconstraint}
%\paragraph{Relic density}
The main stream of current DM theory is based on the ``cold matter'' regime in which DM used to be in a thermal equilibrium at the begining of the universe, and cooled down according to the cosmic expansion later on, and being decoupled at a certain scale, keeing the abandunce upto now. The relics is strongly related by the annihilation cross-setion, which can be calculated within the MSSM framework. \\

There are couples of drastically different DM scenarios depending on the component of LSP.
In a limit where all the squarks are decoupled, the case of pure bino-LSP almost immediately exluded, since it has to rely on the annihilation with slepton, 
%\footnote{A small exception is when the LSP is in the mass of $m_Z/2, m_h/2$ decaying into $Z$ or $h$ with large cross-section due to the threshold enhancement, or closeto the mass of stau where co-annihilation cross-section is enhance by the similar mechanism.}
and needs $m_{\tlep}<110\gev$ to achieve the observed relic abundance $\Omega_h h^2= 0.12$ which is already excluded by LEP2. \\

On the other hand, 
the annihilation cross-section tends to be too large in case of pure-wino or pure-higgsino LSP, where $\sim 3\tev$ of wino mass or $\sim 1\tev$ of higgsino mass is needed to be consistent with the observed relic $\Omega_h h^2$, which is unfortunately beyond the LHC reach. \\

What if the mixed case? It is particlualr intresing to consider doping a bit of wino or higgsino component into bino-dominated LSP, where moderated annihilation cross-setion and experimental accessible LSP mass can be achieved simultaneously. This type of LSP is called ``well-tempered'' neutralino LSP, typically predicting a moerately small mass splitting between the next-to-the-lightest SUSY particle (NLSP) and the LSP with $20-50\gev$ \cite{WTN_calcdM}. \\

Note that a number of caveats are to be added on the discussion:
\begin{itemize}
\item DM is not necessarily ``cold'', and it is generally know to increase the relics with warm DM. \\
\item The DM annihilation cross-section calculation so far is dominantly done at the lowest-order (LO) in the perturbation. 
The presense of higher order terms will generally increase the expected relics. 
%The pure-bino with decoupled sfermions is supposed to particularly affected by the higher order terms since the it has effectively no viable annihilation channel at LO.
\item Is is awkward though, it is possible for other new phyics to supply the DM relics.
\end{itemize}
Therefore, it is sensible to be generous to scenario with too small relics.

			
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Constraint from Direct Search at Collider Experiments} 
The direct search of SUSY had been widely performed in collider experiments including LEP, Tevatron and LHC, with comprehensive coverage over signatures and scenarios. Unfortuanately no evidence has been claimed, it is interpretated into constraints on various minimal complete models (mainly SUGRA-type models, GMSB and cMSSM), or either particular production and decay chains (known as the simplefied model, as discussed in below).
%simplified modelによるdecay chainごとへのupper limit(後述), またpMSSMといたfull model orientedなmodelに対してinterpretationがなされている。
This sub-section will overview status of constraints provided by those experiments,
with particular focus on the limits for the simplified models 
that are used in interpretation in this thesis. \\

\paragraph{Gluinos}
The best job is done by hadron collider due to its outstandingly high production cross-section.
It is particularly the case in LHC Run2, dominating the sensitivity in most of the scenarios in terms of the mass spectra and gluino decay. \\

The exclusion limits on the most typical gluino decays set by ATLAS and CMS are shown in Fig. \ref{fig::Introduction::LHCLimitGG}, namely the direct decay where gluino directly fall into LSP with emitting two quarks, and a 1-step decay via NLSP chargino. Upto $\sim 2\tev$ in gluino mass is excluded for case with large mass splitting between gluino and LSP, and $\sim \tev$ for the most pessimistic case where gluino and LSP are highly compressed.\\

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/LHC_SUSY_GG_direct_36.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/LHC_SUSY_GG_1step_36.pdf}}
    \caption{Up-to-date constraints set by ATLAS and CMS on (a) direct gluino decay: $\tg \ra q \bar{q} \tLSP$, and (b) 1-step chargino-mediated gluino decay: $\tg \ra q\bar{q} \tchic$ with the mass being in the middel between gluino and the LSP.}
    \label{fig::Introduction::LHCLimitGG}
\end{figure}


Gluino decaying with top quarks addresses particular importance since it can be enhanced by the light stop which is motivated by naturalness. They are exclusively searched with dedicated signal regions, and the resultant limit in given in Fig. \ref{fig::Introduction::LHCLimitGtt}.

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/ATLAS_SUSY_Gtt_36.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/CMS_gg_tt_36.pdf}}
    \caption{Up-to-date constraints set by (a) ATLAS and (b) CMS on direct gluino decay with associated top quarks: $\tg \ra t\bar{t} \tLSP$.}
    \label{fig::Introduction::LHCLimitGtt}
\end{figure}


\paragraph{Squarks}
Sine stop claims particular motivation for naturalness among all squarks, 
the analyzes are dedicatedly designed to address to wide range of decays of mass configurations.
The strongest limits are provided by LHC, and Fig. \ref{fig::Introduction::LHCLimitStop} presents the exlusion limits obtained by ATLAS and CMS on the most typical decay scenario $\ttop \ra t \tilde{\chi}_1^0$.
Upto about $400\gev \sim 1\tev$ of stop mass is exluded for depending on the mass splitting, which is similar to the other decay models as well including $\ttop \ra b \tchic$.

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/ATLAS_SUSY_Stop_tLSP.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/CMS_stop_36.pdf}}
    \caption{Up-to-date constraints set by (a) ATLAS and (b) CMS on stop pair production with direct decay $\ttop \ra t \tilde{\chi}_1^0$.}
    \label{fig::Introduction::LHCLimitStop}
\end{figure}



\paragraph{Electroweak Gauginos}
A number of searches have been performed in LEP, Tevatron and LHC, and the LHC result set the majority of the current storngest limits. The target signatures are mainly pair produced NLSPs ($\tchic$ or $\tchin$) directly decaying to LSP, where wino donimated NLSP, bino-dominated LSP, and decoupled squarks are commonly assumed.
\footnote{Under the decoupled squark scenario, bino production is strongly surpressed.}
The signal regions typically require multiple leptons and large missing ET in the final states.
The exclusion limits set by ATLAS and CMS is shonw in Fig. \ref{fig::Introduction::LHCLimitEWKino}.
About upto $500\gev$ of NLSP mass is excluded for cases with large NLSP-LSP mass splitting, and $200-250\gev$ for small mass splitting.  \\


\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/ATLAS_EWgaugino_WZh_36.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/CMS_EWgaugino_WZh_36.pdf}}
    \caption{Up-to-date constraints set by (a) ATLAS and (b) CMS on direct EW gaugino production with decays via $W/Z/h$.}
    \label{fig::Introduction::LHCLimitEWKino}
\end{figure}


The scenario with wino LSP is explored using a strikingly different approach. Since the mass splitting between NLSP wino-chargino and the wino-LSP is extremely compressed ($150 \sim 160\mev$), wino-chargino retains $O(\mathrm{ns})$ of moderately long lifetime, resulting the characterstic disappearing track signature where a visible charged track disappearing halfway in the tracker due to the decay. The result from ATLAS and CMS (Run1) is given in Fig. \ref{fig::Introduction::LHCLimitWino}.
The exlusion runs upto $300-500\gev$ of wino mass for the lifetime (or the NLSP-LSP mass splitting) predicted by MSSM. \\

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/ATLAS_SUSY_LLDT_36.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Introduction/CMS_SUSY_LLDT_Run1.pdf}}
    \caption{Constraints set on wino-LSP scenario provided by (a) ATLAS and  (b) CMS.}
    \label{fig::Introduction::LHCLimitWino}
\end{figure}

No intrepretation has been made for higgsino production and wide higgsino LSP scenario so far by LHC, 
due to the marginal production cross-section ($\sim 1/4$ of that of wino production) and small NLSP-LSP splitting that is generally predicted in case of higgsino LSP.
%\footnote{This does not mean LHC gives no constraints for the higgsino LSP scenario, since it is possible to have NNLSP production decaying into LSP, which should be constrained by the }
While the presense of light higgsinos are highly motivated in light of naturalness, 
the strongest limit on direct higgsino production is still held by LEP2.
The limit is shown in Fig. \ref{fig::Introduction::LEP2_chargino_comb}, where upto $\sim 90\gev$ of LSP mass is excluded.

\fig[60]{Introduction/LEP2_chargino_comb.pdf}
{Exclusion limit on direct production of higgsino pairs set by LEP2 in which results from all four experiments are combined.}
{fig::Introduction::LEP2_chargino_comb}


%\subsubsection{Constraint from Indirect Search Experiments}
%\paragraph{Flavor}
% ud occilation?suppression???light squark??mixing?? or light squark?????????

%\paragraph{Proton Decay}
% R-parity conservation?imply

%\paragraph{Limit on long-lived gluino from cosmology}
% Super Long-lived gluino?????			




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Targeted SUSY Scenario and the Search Strategy in this work} 
\subsubsection{Targeted SUSY Scenario }
To summarize the assumptions and scnearios discussed above, in thesis, we will focus on the MSSM scenarios where:
\begin{itemize}
 \item Squarks are all heavy $(>3\tev)$.
 \item Allow the higgs mass fine tuining at order of $10^{-5}$.
 \item LSP is neutralino.
 \item Respect observed DM relic at least as the upperlimit of abandunce.
\end{itemize}

The targeted experimental signature is the pair production of gluinos (Fig. \ref{fig::Introduction::gluinoPairProd}) with the mass ranging from $\sim 800\gev-2\tev$, followed by various decays. \\
No particular assumption is going to be made for the mass spectra, however as motivated by the well-tempered neutralino DM scenario, an additional focus is put on the case of $\dmcn = 20\gev \sim 30\gev$ with dedicated signal region setting and interpretation.
%\footnote{Bino DMのco-annihilation scenarioとしてgluinoとbino LSPのcompressed modelを考えるものもあるが、この場合はgluinoがlong-livedになるためdisplaced vertex signatureで見ることになり, このthesisの範疇にはない.}


\subsubsection{The Strategy of Decay Chain Based Search}
Though general and minimal scnearios will be pursued as the leading principle, 
given that the most streightforward scenarios (e.g. the most minimal models such as mSUGRA) has been largely excluded by LHC so far, we would extend the scope of the search to a more general direction.
%依然として最もgeneralでminimalなscenarioを探す.
%ただしRun1で最も"都合のいい''SUSY model/scenarioが死んだことを踏まえて, less model orientedなgeneral searchをやるのがいいだろう
Ideally, we prefer to consider as general as possible e.g. MSSM, but constraining the full parameter spaces is not realistic (e.g. $>100$ parametersfor the most general MSSM).
However, it is also true that most of the MSSM parameters only affect the spins or decay branchings of SUSY particles, rather than kinematics i.e. they do not change the signal acceptance. On the other hands, kinematics of SUSY signatures are dominantly determined by SUSY mass spectra. Therefore, we only have to care about the care about the mass dependence, once a full decay chain is specified, In other words, setting the cross-section upper limit on each decay chain and mass spectra is no less general than constraining the full parameter space of a model.  \\
\footnote{This is the same to admit our search has no sensitivity in determining the model parameters other than masses. }

Placing upper limits on particular decay chain $A\ra B$ is essntially equilavent to setting exclusion limit on following model called ``simplified model'':
\begin{itemize}
\item Assuming $Br(A \ra B)$ is 100$\%$.
\item Parameters other than SUSY masses are set to arbitrary number. E.g. in LHC analysis, the EW gaugino mixing is usually set so that NLSP and LSP become wino- and bino-dominant. 
\end{itemize}
Though this simplified model based interpretation has been widely employed in LHC searches, the problem is that the coverage of decay chains and mass spectra is far from complete, for instance, in case of gluino, only a few decays are considered. In this thesis, all the possible gluino decay chains, and setting the limit on each of them with full coverage of mass assumption on gluino and EW gauginos. In the following sub-section, the target decay chains are closely specified.

% よって、終状態ごとにupperlimitをつけるのは以下で定義するsimplified modelに対してlimitをつけるのとほぼ同義になる:
% 一応squark/EW gaugino mixingはhelicity angleを通じてkinamaticsに影響を与えるが, gluino decayの場合かなり限定的。
% (footnote) stop/EW gauginoでは大きく影響する場合がある. なので適当なstop mixing, wino NLSP - bino LSPにfixして話を進めるのはダメなときがある
%MSSMの100 dimension parameter space を2-3D times 110にreduce.



\subsubsection{Targeted Gluino Decay Chains} \label{sec::Introduction::targetModels}
%Gluinos are generated in pair under the assumption of R-parity conservation.
The decay is always 3-body through heavy virtual squarks assumed to be heavier than the gluino, ending up in 2 SM quarks and a EW gaugino:
\begin{align}
\tg \ra 
  \begin{cases}
    (u\bar{d}, c\bar{s}, t\bar{b}) \times (\tilde{\chi}_{1,2}^{-}) \nn \\
    (d\bar{u}, s\bar{c}, b\bar{t}) \times (\tilde{\chi}_{1,2}^{+})  \nn \\
    (u\bar{u},d\bar{d},s\bar{s},c\bar{c},b\bar{b},t\bar{t}) \times (\tilde{\chi}_{1-4}^{0}). \nn 
  \end{cases}
\label{eq::gluinoDecays}
\end{align}
Including the subsequent EW gaugino decays, this will lead to an enormous number of final states,
however kinematically some the them are approximately equivalent which can be merged or trimmed. 
For instance, the mass spiliting between higgsino-dominated states or winos-dominated states are compressed, leading to the effectively the same kinematic patterns. 
\footnote{The splitting will be rarely greater than $50\gev$ even when all $M_1$, $M_2$ and $\mu$ are at the same mass leading to the maximum mixing.} 
This eventually reduces all gluino decays into either direct decay in which gluino directly de-excites into LSP, ``1-step'' decay with one intermediate EW gaugino state, and ``2-step'' decay in which gluino decays via two resolved intermediate EW gauginos mass states, with possible three types of scenarios in terms of the  mass spectra, as schematized in Fig. \ref{fig::Introduction::signal_massConfig}.


%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.21\textwidth]{figures/Introduction/massConfig_direct.pdf}}
    \subfigure[]{\includegraphics[width=0.38\textwidth]{figures/Introduction/massConfig_no_higgsino.pdf}}
    \subfigure[]{\includegraphics[width=0.38\textwidth]{figures/Introduction/massConfig_with_higgsino.pdf}}
    \caption{ 
Possible gluino decay paths are illustrated for mass spectra where
(a) all the EW gauginos are heavier than gluino ($100\%$ direct decay of gluino),
(b) higgsinos are heavier than gluino while the other EW gauginos are lighter (mixture of direct decay and 1-step decays),
(c) all the EW gauginos are below gluino mass (mixture of direct decay, and numerous 1-step and 2-step decays),
are illustrated.
The hieararchy between wino and bino in (b), or between all the EW gauginos in (c) are also allowed.
\label{fig::Introduction::signal_massConfig} }
\end{figure}
%%%%%%%%%%

As for the scenario in Fig. \ref{fig::Introduction::signal_massConfig}, a numerious MSSM parameters scans demonstrate that the probability of 2-stes decays are generally much lower than that of direct or 1-step decays, except for some of the cases where each of the intermideate masses are aligned with relatively equal distance 
%(See Appendix)
. Therefore, in the analysis, we confine our scope within direct and 1-step decay of gluino. \\

The acceptance dependence between ligh quark flavors ($u,d,s,c$) are also small, therefore can be merged i.e. a simplified model where gluino decays with equal rate between $u,d,s,c$ is considered. \\

For seb-sequent gaugino decays, charginos are assumed always decays into on-shell or off-shell $W$-boson, while two options for neutralinos decays via $Z/h$ are considred. The decays into slepton is ignored here, majorly for convenience sake of restricting the number of final states, however with a few justifications; under a general unification regime, slepton masses are order of squark masses which are considered to be $>3\tev$; when respecting the observed DM relic abundance, the mass splitting between NLSP and LSP becomes naturally small (typically $<50\gev$). Decays via sleptons requires sleptons masses in between NLSP and LSP, which is unlikely to happen. \\

With all the scrutinization, the targeted gluino decay chains are reduced into Tab. \ref{tab::Introduction::gluinoDecay2}.
Corresponding Feymann diagrams are shown in Fig. \ref{fig::Introduction::gluinoDecay_summary}. 
\tab{c|l}{
\hline
Direct decay (3) & $\tg \ra (q\bar{q},q\bar{q},q\bar{q}) \tilde{\chi}_{1}^{0}$  \\
\hline
\hline
1-step decay (8) & $\tg \ra (q\bar{q}', t\bar{b}(b\bar{t})) \,\,\, \tilde{\chi}_{1}^\mp, \,\,\,\,\,\,\, \tilde{\chi}_{1}^- \ra W^\mp \tilde{\chi}_{1}^{0} $ \\
                 & $\tg \ra (q\bar{q}, b\bar{b},t\bar{t}) \,\,\,  \tilde{\chi}_{2}^{0}, \,\,\,\,\, \tilde{\chi}_{2}^{0} \ra Z\tilde{\chi}_{1}^{0})$ \\
                 & $\tg \ra (q\bar{q}, b\bar{b},t\bar{t})  \,\,\, \tilde{\chi}_{2}^{0}, \,\,\,\,\, \tilde{\chi}_{2}^{0} \ra h\tilde{\chi}_{1}^{0})$ \\ 
\hline
}
{Summary of targeted gluino decay chains. The number in the pharenthese indicates the numbers of chains in the categoty.}
{tab::Introduction::gluinoDecay2}


\fig[150]{Samples/signalModels/gluinoDecay_summary.pdf}
{Target gluino decay chains.}
{fig::Introduction::gluinoDecay_summary}


The full decay chains of pair produced gluinos become increasingly complicated: 11 symmetric decays (two gluinos experience the same decay chains), 55 symmetric decays (two gluinos experience different decay chains).
In total, 66 decay chains are identified as the targets. \\


\subsubsection{Target Signal Models for 1-lepton Final State}
In LHC, analyzes are conventionally divided based on number of hard leptons in the final state, since eiter signal kinematics and the backgroud strategy are drastically different. In gluino decays, ignoring the decays into sleptons, leptons are always generated via decays of W/Z/H boson. Therefore, giving their small leptonical branching ratio, 0-lepton or 1-lepton final state are the most promissing channel for inclusive search, while 2/3-leptons final states are more specialized in specific types of scenarios such as long-chain multi-step gluino decays where more W/Z/H bosons are involved. \\

This thesis focus on the final state with exactly one lepton. 
Therefore decay chains with marginal branching ratio into final state with exactly 1-lepton will be excluded from the targets. 
The full list of the target decay chains in the thesis (referred as ``benchmark model'') are shonw in Tab. \ref{tab::Introduction::modelsBV} - Tab. \ref{tab::Introduction::models3B}, with the naming convention for each decay chain defined as Eq. (\ref{eq::gridNameConvention}). They are further categorized based on the number of expected b-quarks in the final state, as the signal regions will be segmented based on the numberof b-tagged jets. The reference models for each b-categories are respectively chosen as \textbf{QQC1QQC1},\textbf{QQC1BTC1} and \textbf{TTN1TTN1} for BV, BT and 3B (Fig. \ref{fig:Introduction::refModels}), which will be used as the reference in designing signal regions and other various studies. The Feynman deagrams for the reference models are illustrated in Fig. \ref{fig:Introduction::refModels}.

%Although the 0-lepton final state often is more advantageous in terms of coverage of models and signal acceptance, 1-lepton addresses some unique merits over the 0-lepton channel as following:
%- 0Lよりclean. QCDとかnon-collisionみたいな変なBGの心配をしなくていい
%- 2Lよりはsignalが多い. 2LをCRにできる.  (0Lで1LをCRとすると結構contamiが無視できない?)  あとCRがpure
%- 0Lとのcincidenceを取ることでSUSYとのconsistencyについて議論できる

% compressedとかだとそこに書いてあるよりmultiplicity減るっていうことにも一応言及

\begin{align}
  \mbox{Model name} & := aaXXbbYY  \nn \\
  aa,bb & = \mbox{``QQ'',``BB'',``TT'',``BT''}  \nn \\
  XX,YY & = \mbox{``N1'',``C1'',``N2Z'',``N2H''} 
%  \label{eq::gridNameConvention}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%
\input{tex/Introduction/GG_database/genGG_summary.tex}
%%%%%%%%%%%%%%%%%%%%


\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.32\textwidth]{figures/diagrams/GG_QQC1QQC1.pdf}}
    \subfigure[]{\includegraphics[width=0.32\textwidth]{figures/diagrams/GG_QQC1BTC1.pdf}}
    \subfigure[]{\includegraphics[width=0.32\textwidth]{figures/diagrams/GG_TTN1TTN1.pdf}}
    \caption{ Feymann diagrams for the Benchmark models (a) QQC1QQC1 (b) QQC1BTC1 (c) TTN1TTN1. }
    \label{fig:Introduction::refModels}
\end{figure}



\subsection{Structure of the thesis}
The rest of the sections will proceed as follow: 
The LHC and the ATLAS detector is firstly overviewed in Sec. \ref{sec::Detector}, 
to have idea of typical environment that LHC provides and 
the precision the ATLAS detectors system offers ; 
Sec. \ref{sec::objDef} follows with describing the off-line algorithms of reconstructions and identifications for particles or jets; 
The detail of simulation used in analysis is given in Sec. \ref{sec::Samples}; 
The description of the analysis starts with Sec. \ref{sec::SRdefinition} in which the preselection and signal regions are designed. Sec. \ref{sec::BGestimaiton} is then involve comprehensive discussion on the background estimation, the most important part in the analysis;
Uncertainties associated with background estimation and signal modeling is overviewed in Sec. \ref{sec::Uncertainties}
; finally the unblinded result and resultant limits are shown in Sec. \ref{sec::Result}; 
with brief conclusion remarks in Sec. \ref{sec::Conclusion}.


\section{Experiment Apparatus:  The ATLAS Detector at the LHC}

\subsection{The Large Hadron Collider} 
The Large Hadron Collider (LHC) \cite{LHC} is a 27 km long circular proton accelerator embedded underground of the Geneva area.
%and Frech villages overviewing the Mont Blanc \cite{MontBlanc}.
It is designed to collide protons at a center-of-mass energy of $\sqrt{s} = 14 \tev$, at the four detector cites (ATLAS\cite{ATLAS_exp}, CMS\cite{CMS}, ALICE\cite{ALICE} and LHCb\cite{LHCb}) built on the accelerator ring. ATLAS and CMS are general purpose detectors designed to study a vast range of physics programs, while LHCb and ALICE are specialized in studying b-hadrons and heavy-ion collisions respectively. \\

The operation started in 2010, offering proton-proton (pp) collisions at a center-of-mass energies of $7\tev$ and $8\tev$ with $4.7\ifb$ and $20.3\ifb$ of integrated luminosity until 2012 (Run1). The center-of-mass energies has been almost doubled to $13\tev$
in the runs starting from 2015 (Run2). The LHC has also delivered lead-ion (Pb-Pb) collisions with a center-of-mass energy of $\sqrt{s_{NN}} = 2.76\tev$ and proton-lead (p-Pb) collisions with $\sqrt{s_{NN}} = 5.02\tev$.

% In a chamber, 
% 

% The protons are then accelerated (段階的に) sequently through the booster acceleratos (linac, PS, SPS) and finally injected into LHC. 
% In LHC

% number of pileup
The acceleration of protons stages into various steps: 
Protons are firstly seeded from hydrogen gas, by blowing off the electrons from the hydrogen atoms using high electric field.
They are injected in the linear accelerator LINAC2 with accelerated upto 50 MeV, and sent into the Proton Synchrotron Booster (PSB) being accelerated up to an energy of 1.4 GeV. 
The subsequent accelerator is the Proton Synchrotron (PS) elevating the energy of the protons to $25\gev$, and injecting them into the Super Proton Synchrotron (SPS). After being accelerated to $450\gev$ in SPS, the protons finally enter the two LHC beam pipes running the beam oppositely each other. The acceleration chain is illustrated in Fig. \ref{fig::Detector::LHC}.

\fig[170]{Detector/LHC_schematic.pdf}
{LHC. \cite{LHCPhoto}}
{fig::Detector::LHC}


The LHC accelrator consists of octant-shaped 2.45 km arcs with 1232 superconducting magnets located at the curves, providing 8.33T of magnetic field to bend the proton trajectory.
%They are operated at 1.9K cooled by liquid-helium, 
In total, 39 bunch-trains can be filled into the LHC simultaneously in a design conditions, with 2808 bunches per beam are brought to collision in the LHC. Each bunch contains about $10^{11}$ protons. 
The beam bunches are collided with a beam crossing angle of 285 mrad. 
The peak luminosity amounts upto $L = 0.7-1.4 \times 10^{34} \mathrm{cm}^{2} \mathrm{s}^{-1}$ in the 2015-2016 runs, as shown in Fig. \ref{fig::Detector::DAQ} (a).

Due to the high frequency of collisions and the dense proton bunches, multiple proton collisions can take place simultaneously within the same bunch crossing, referred as ``pile-up''. 
% to maximizing the chance of interesting reaction 
The average pile-up $\mu$, defined as the mean number of interactions per bunch crossing,  
has been evolved according to the peak luminosity increase, amounting to $20-40$ in recent runs in 2016. The $\mu$ profile is shown in Fig. \ref{fig::Detector::DAQ} (b).

%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Detector/peakLumi2016.png}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Detector/mu_2015_2016.png}}
    \caption{  (a) Peak luminosity evolution in 2016 runs \cite{DAQ2016}, and (b) the pile-up profile obtained in 2015-2016 runs  \cite{lumiPubResult}.
      \label{fig::Detector::DAQ}
    }
\end{figure}
%%%%%%%%%%



%and are categorized as in-time or out-of-time pile-up. In-time pile-up events are caused by additional interactions of protons in the same bunch collision. The out-of-time pile-up occurs when traces from an event in a different bunch-crossing are recorded. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Particle Measurement and The ATLAS Detector}
ATLAS (A Toroidal LHC ApparatuS) is a general purpose detector, aiming to probe a wide range of physics programs from precision measurements of EW physics to the energy frontier experiments, through a dedicated measurement of particles produced in $pp$ collisions. The detector spans over 44m in width and stand 25m in height, covering the interaction point (IP) by a sylindrical barrel and two endcaps, achieving a nearly full solid angle coverage. The total weight amounts 7000 tons including stopping materials to fully accomodate the produced particles, enabling complete measurement of particle energy. The cut-away image is shown in Fig. \ref{fig::Detector::ATLAS_whole}. \\

The purposes of the detector are mainly two-fold:
\begin{itemize}
\item identification of particle species,
\item determination of particle's energy and momentum,
\end{itemize}
with two complemental concepts of measurment:
\begin{itemize}
\item fast measurement for providing trigger
\item precision measurement of particle properties
\end{itemize}
 
To satisfy these functionalities at the same time, following sub-detectors are arranged in a designed order from the innermost toward outside with respect to the IP.

\begin{itemize}
\item Inner detector (and magnets) to identify and measure electrically charged particles, as well as define the primary vertices. \\
Charged particle can easily interact with materials by ionizing the molecules inside. The path of flight can be ``imaged'' as a track, by recording the position of ionization. 
In ATLAS, a complex of descrete layers of silicon sensors and a continuously volumed gas chambers are placed in the innermost. 
The momentum can be measured in addition by applying magnetic field, and quantifying the cavature of the bent trajectory. 

\item Calorimeters to measure the energy of electron, photon and hadrons. \\
Electrons and photons traveling inside materials above certain energy 
\footnote{Referred to the critical energy. $\sim 800 \mev$ for typical material.}
lose their energy through electromagnetical showering; photons create $e^+e^-$ pairs and electrons spew bremsstrahlung photon; 
the daughter electrons and photons are multiplicated by this recursive repeatition; ending up in a particle shower. 
Most of the energy are absorbed after traversing about 20 radiation lengths ($X_0$) of material. 
Hadrons (mostly pions) also cause similar cascade reactions. 
The shower branch evolves by interacting with neucleus in the material via strong interaction, meanwhile produced $\pi_0$s promptly decay into two photons which shower electromagnetically. 
The resultant shower is combination of a long hadronic shower and small local EM clusers in it. 
Electromagnetic and hadronic calorimeters are set as the outer layers of the trackers, to measure such showers by absorbing them in it.\\

\item Muon spectrometer (and the magnet) to measure the muons penetrating the detector. \\
Among all the particles that interact with material, muons are only exception who do not seriously deposit the energy in calorimeter. 
This is due to the fact that muons happen to have the mass realizing the minimum EM interaction with material (Minimum Ionizing Particle; MIP), 
and the corresponding critical energy for EM showering is usually at several TeV level.
%, surpassing the typical energy range of particles generated in LHC. 
This is actually a lovely coicident for human being (or poor particle physicists), 
since they can be easily identified i.e. particles puching through the calorimeter are automatically muons. The muon spectrometer located outermost serves for identifying such muons as well as measuring the tracks together with the inner tracker described above.

\item Given the total momentum conservation in transverse direction in each collision, 
the presence of non-interacting particles such as neutrinos and hypothetical new particles can be indirectly detected through the imbalance; 
This is referred to missing $\ET$ ($\met$), 
\footnote{The ``$\ET$'' in the name is due to a historical reason; it used to be calulated only using calorimeter deposits, which is now actually outdated. 
Also, momentum and energy effectively give no differences for most of detected particles in LHC, since their energy typically overwhelms the masses.}
defined by the negative of the vectoral sum of transverse momentum of all detected particles.
\end{itemize}


In the following subsections, each of the sub-detector system will be overviewed, comprehensively based on references \cite{ATLAS_exp} and \cite{ATLAS_TDR}.

\fig[110]{Detector/figures_AtlasDetectorLabelled.png}
{Full-body view of the ATLAS detector \cite{ATLAScosmicPerf}. The geometry is completely forward-back symmeric.}
{fig::Detector::ATLAS_whole}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Coordinate System}
For refercing the position of the detector as well as the orientation of particles, a right-handed Cartesian coordinate system is defined with the interation point being the origin and the x-axis pointing to the center of the LHC ring. The y,z-axes are accordingly the direction of sky or the beam diretion respectively. Polar angle $\theta$ and azimuthal angle $\phi$ are defined by the sylindrical representation $(\theta,\phi,z)$: $\theta$ ranges from 0 to $2\pi$ with respect to the z-axis, and $\phi$ runs from $-\pi$ to $\pi$ from the x-axis. The two endcaps in the ATLAS detector are referred as ``A-side'' and ``C-side'', corresponing to the position of positive and negative coordinate in the z-axis. \\

It is the unfortunate fate for hadron colliders that particles generated by collisions are usually highly boosted along z-axis, since the energy of the inital interacting partons inside the hardons are asymmetric. From this point of view, a set of variables with Lorentz-invariant nature are introduced for describng momentum or position. In particular, it is useful to define the transverse component of variables, such as transverse momemtum $\pt := p\sin{\theta}$ or transverse energy $E := E\sin{\theta}$. The advanrage over the use of $p$ or $E$ is obvious that they do express the intrinsic hardness of the particles in the center-of-mass frame of the reaction, and also that the vectoral sum of all particles conserves before and after the collision. \\

Similarly, pseudo-radidity $\eta$ defined below commonly serves as the coordinate of polar angle:
\begin{align}
\eta := \ln \left( \log{\frac{\theta}{2}} \right).
\end{align}
It has two practical advantages over $\theta$ the difference in pseudo-radidity between particles $\Delta\eta$ are invariant against the boost towards z-direction. 
\footnote{
This is true when the particles are massless, which is approximately valid given that the boos along z-axis is sourced by the momentum of order of the beam energy.}
; $\eta$ has an effectively finer measure at very forward direction where $\theta$ suffers from the degeneracy i.e. $\cos{\theta}\sim 1$, thus more convenient in expressing the orientation of forward particles.

Angular distance between two particles are commonly expressed by $R$ that is defined as: 
\begin{align}
\Delta R := \sqrt{(\Delta\eta)^2+(\Delta\phi)^2}.
\end{align}

%\fig[110]{Detector/ACside.pdf}{.}{fig::Detector::ACside}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Inner Detectors}
The inner detector (ID) is placed the inner-most of the ATLAS detector, designd to measure the tracks of charged particles, 
as well as precisely determining the position of vertices of the hardest scattering in interest.

It consists of a scilicon tracker (the pixel detector and the semiconductor tracker ;SCT) at the inner radii,
and the Transition Radiation Tracker (TRT) for continuous tracking at the outer radii. 
The detector arrangement is illustrated in Fig. \ref{fig::Detector::innerDetector_xsec} and Fig. \ref{fig::Detector::innerDetector}.
The outer raius is surrounded by the central solenoid, providing a magnetic field of 2T along the $z$-axis,
to bend the tracks traveling inside the ID volume.
%The general requirement for tracking is the fine granulaliry of detector segmentation, to achieve high resolution of track momentum as well as quality measurement. 
% as a result, number of channel skyrocket.
%high position resolution 
% trackのresolutionは内側の方が1点あたりのresolution impactが大きいので, 内側ほどgranularityの高いsub-detectorを置いている

% \footnote{Ideally pixelで全部被覆できれば最高であるが、costがr^2でscaleしてとんでもないことになるのでcostとbest compromizeした結果今のようになった。それでもchannel数, 被覆面積は...であり、crazyの極致である。}

% 基本的にチャネルが多くて読み出しに時間がかかるのでR&D purposeを除いてtriggerは発行していない
%
%%%
% 後段のcaloのmeasurementを邪魔しないために、materialはminimizeされてる
As a general requirement, ID has to contain material as less as possible, to avoid disturbing the measurement downstream by the energy loss. 
Fig. \ref{fig::Detector::IDmaterial} shows the total material profile of the ID over $|\eta|$. 
The material volume is supressed below $2.5$ radiation length and 1 neuleus interaction length, which is low enough compared with energy dropped in the calorimeter.


%%%%%%%%%
\fig[110]{Detector/innerDetector_xsec.pdf}
{Cross-section of the ATLAS inner detectors \cite{ATLAS_exp}.}
{fig::Detector::innerDetector_xsec}


\fig[110]{Detector/ATLAS_innerDetector.jpg}
{Cut-away view of the ATLAS inner-detector \cite{ATLAS_exp}.}
{fig::Detector::innerDetector}

%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Detector/radLength_ID.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/Detector/intLength_ID.pdf}}
    \caption{ Simulated material profile of whole ID in unit of (a) electro-magnetic radiation length and (b) neucleus interaction length \cite{ATLAS_exp}.
      Peak $|\eta|\sim 1.5$ corresponds to the barrel-endcap trasition area through which service cables travel.
      \label{fig::Detector::IDmaterial} }
\end{figure}
%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{The silicon trackers: Pixel and SCT} 
The detection principle of silicon detector is based on the electron-hole pair creation induced by a traverse of a charged particle.
%Thanks to the narrow band gap ($1.7 \mathrm{eV}$). 
Those electron-hole pairs are inhaled by the bias voltage applied on the sensor, and transfered into an electric signal. 
Silicon is partilularly advantageous in 
The choice of simicon is largely for its radiation hardness durable the enormously high radiation around the IP. 
On the other hand, the performance (e.g. noise level, gain) is relatively sensitive to temparature, therefor they are kept in low temparature ($-5 \sim 0\deg$) during the operation.  \\

The pixel detector is the unit of layers of pixelated scillicon sensors located closest to the IP of all the detector component. 
%The two-dimensional segmentation of the sensors gives space points without any ambiguities. 
Oxygen enriched n-in-n silicon semiconductor is used for the sensors.
Four sylindrical layers are placed in the barrel at the radial distance of 31 mm - 122.5 mm with respect to the IP, 
and 3 disk layers cover each side of the endcap, providing an acceptance with $|\eta|<2.5$. 
The innermost layer in the barrel is referred as the ``insertable b-layer'' (IBL) installed during the long shutdown between Run1 and Run2, providing the highest precision and playing a prominent role in identifying the secondary vertex of late decaying particles ($\tau$, $b$-hadrons etc.). 
The pixels are in the 50 $\times$ 250 $\um$ granularity in the IBL, and 50 $\times$ 400 $\um$ in the other layers. 
The resolution is purely determined by the pixel size. A spacial resolution of $4\um$ and $115\um$ is achieved along the radial and beam z-direction respectively, 
by combining the hit information from the four layers. \\

The SCT is located outside of the pixel detector. The sensors are made by single-sided p-on-n silicon semiconductors with 150 V of bias voltage applied. %要確認
The strips of barrel SCT aligning along the z-axis with $80\um$ pitch, giving a precision position in the $r-\phi$ plane. 
A slight angle stereo (40 mrad) alternated by layers is add to the arrangement, providing decent $z$-position determination in addition. 
The intrinsic resolution is $17\um (580\um)$ in $r-\phi (z)$ direction respectively.
%Each sensor has 768 strips readout indivisually.
The strips in the endcap SCT are aligned in a mesh in terms of $x-y$, capable of 3D position determination together with the $z$-coordiname of the disks.

%The total area coverage of silicon amounts up to $61m^2$, with 6.2 million readout channels.
% track 200um離れてれば分離可能
% thickness 285um

%%%%%%%%%%%%%


%基本的な流れ
%-前後とのつながり
%-一番小さいunitと検出動作原理, 読み出し, operation volrageとtemp, 素材への要求とか
%-それらがどう配置されてるか, geometryの特徴, hitの数とか
%-performance


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The pixel detector [1-9] is designed to provide a very high-granularity, high-precision set of measurements as close to the interaction point as possible. The system provides three precision measurements over the full acceptance, and mostly determines the impact parameter resolution and the ability of the Inner Detector to find short-lived particles such as B hadrons and τ lep- tons. The two-dimensional segmentation of the sensors gives space points without any of the ambiguities associated with crossed strip geometries, but requires the use of advanced electron- ic techniques and interconnections for the readout. The readout chips are of large area, with in- dividual circuits for each pixel element, including buffering to store the data while awaiting the level-1 trigger decision. Each chip must be bump-bonded to the detector substrate in order to achieve the required density of connections. In addition, the chips must be radiation hardened to withstand over 300 kGy of ionising radiation and over 5×1014 neutrons per cm2 over ten years of operation. The system contains a total of 140 million detector elements, each 50 μm in the Rφ direction and 300 μm in z, which are invaluable for the task of pattern recognition in the crowded environment of the LHC.
%The system consists of three barrels at average radii of ~4 cm, 10 cm, and 13 cm, and five disks on each side, between radii of 11 and 20 cm, which complete the angular coverage. The system is designed to be highly modular, containing approximately 1 500 barrel modules and 700 disk modules, and uses only one type of support structure in the barrel and two types in the disks.
%The pixel modules are designed to be identical in the barrel and the disks. Each module is 62.4 mm long and 21.4 mm wide, with 61 440 pixel elements read out by 16 chips, each serving an array of 24 by 160 pixels. The output signals are routed on the sensor surface to a hybrid on top of the chips, and from there to a separate clock-and-control integrated circuit. The modules are overlapped on the support structure in order to give hermetic coverage. The thickness of each layer is expected to be about 1.7% of a radiation length at normal incidence.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%
%Each module consists of four . On each side of the module, two detectors are wire-bonded together to form 12.8 cm long strips. Two such detector pairs are then glued together back-to-back at a 40 mrad angle, separated by a heat transport plate, and the electronics is mounted above the detectors on a hybrid. The readout chain consists of a front-end amplifier and discriminator, followed by a binary pipeline which stores the hits above threshold until the level-1 trigger decision. The end-cap modules are very similar in construction but use tapered strips, with one set aligned radially. To obtain optimal η-coverage across all end-cap wheels, end-cap modules consist of strips of either ~12 cm length (at the outer radii) or 6-7 cm length (at the innermost radi- us).
%%%
%endcap diskは3種類 (inner/midle/outer)のdimension
%pn type  bias voltage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Trasition radiation tracker}
% gas mixture
% TRTの原理もうちょい
% radiation at the boundary between two di-electric material s
TRT is a gaseous detector designed for tracking particles as well as identifying the species using the characteristic transtion radiation.
The detector is filled with 4mm-diameter straw tubes in which xenon-based active gas is confined.
Ionized secondary electrons are collected by the $30\um$-diameter gold-plated tungsten-Rhenium anode wire in the center of each straws.
73 layers of aligned straw tubes are arraged in the barrel, and 160 layers in the endcap sectors. 
The tube length is 144 cm (37 cm) in the barrel (endcap) region. 
The barrel tubes are arranged in parallel along the beam pipe, with 7 mm of interval between layers.
The intrinsic position resolution per straw is about $130\mu m$.
A traverse of charged particle fires 36 straws on average, 

Transition material is inserted between the straws.
$19\um$-diameter polypropylene fibers are used in barrel, and $15\um$-thick polypropylene radiator foils isolated by a polypropylene net are set for the endcaps.
Transition radiation can address unique sensitivity in particle identification, particularly to  $e/\pi$ separation, 
since the intensity is sensitive to incident particle's velocity (proportional to $\gamma=E/m$) rather than the energy or momentum. 
Given that the signal of transition radiation typically yield more amplitude, in the TRT two differenct thresholds are set; 
the lower threshold to collect the signal of normal ionization and the high threhold for transition radiation. 
The high threshold is carefully designed so that only electrons can trigger in typical range of energy ($0.5\gev - 150 \gev$) while pions are inert to it. \\

Fig. \ref{fig::Detector::TRTPerf} shows the $\gamma$-dependence of high threshold rate, demonstrating good separation of particles with electron-like momentum and pion-like momentum.

%%%%%%%%%%%%%%
\fig[170]{Detector/TRTPerf.pdf}
{TRT high threshold rate as funtion of Lorentz factor ($\gamma=E/m$) of incident particles \cite{TRTPub}.
The $\gamma$ scale of typical pions and electrons are shown aside. Left/right plot corresponds to barrel/endcaps.}
{fig::Detector::TRTPerf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Combined Tracking Performance}
The combined tracking performance has been validated via measurement of the cosmic muons \cite{ATLAScosmicPerf}. 
The resolution for a single muon track is obtained as function of muon transverse monmentum: 
% Typically, three pixel lay- ers and eight strip layers (four space points) are crossed by each track. A large number of track- ing points (typically 36 per track) is provided by the straw tube tracker (TRT) [1-8],
%The combination of two complental technologies provides very robust pattern recognition and high precision in
\begin{align}
\frac{\sigma_{\pt}}{\pt} = 1.6\% \oplus \frac{0.053\%}{\gev}\times \pt.  \label{eq::Detector::perf_ID}
\end{align}
%ATLAS_ID_performance_cosmic



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Calorimetery}
ATLAS includes two calorimeter systems composed of the electromagnetic calorimeter (EM calorimeter) and the hadronic calorimeter (HC), located outside the ID. 
The whole view is given by Fig. \ref{fig::Detector::calo}.
%The idea is ... to absorb the incident particles and  inside and calculate the energy deposit . intenseな物質とのhard interationでcascade状シャワー作って
EM calorimeter, the inner part of the calorimetry, is aimed to measure energy of photons and electrons by causing them to EM shower. 
This is done by the Liquid-Argon sampling calorimeter (LAr), alternately sandwiching the lead absorber layers and the sensor layer filled with liquid-argon.
Most of the hadrons do not create EM showers and penetrate the EM calorimeter without major energy loss, which are then captured by the HC is placed the downstream. THe HC exploits two detection technologies: the barrel HC, covering pseudo-rapidity range of $|\eta|<1.7$, is referred to the ``Tile calorimter'' consisting of the sensor layers with scintilator tiles and steel absorbers; The endcap HC ($1.5<|\eta|<3.2$) employ the technology of LAr calorimter similar to the EM calorimeter, however in a differnt geometry and coarser cell granularity. In addition to the EM calorimeter and HC, the forward calorimeter located in the very foward region ($3.2<|\eta|<4.9$) serve a suplemental function capturing the defracted particles from jets. The detector technology used and the spatial segmentation are summarized in Tab. \ref{fig::Detector::caloSpec}.
Thanks to the fast response of the readout, calorimeter also provide the function of trigger, based on the fast processing of particle identification and the energy measurement using the information of indivisual showers, as detailed in Sec. \ref{sed::Detector::TDAQ}

\clearpage
\fig[140]{Detector/caloSpec.pdf}
{Summary of geometries for calorimeters \cite{ATLAS_TDR}.}
{fig::Detector::caloSpec}
\clearpage


\fig[110]{Detector/ATLAS_calorimeter.jpg}
{Cut-away view of the ATLAS calorimetery \cite{ATLAS_exp}.}
{fig::Detector::calo}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Electromagnetic calorimeter}
%構造
%
% Basicなrequirementとしては, 
% Desnse material is preferred for absorber in general 
%e-piのPIDの観点からEGはECAL内で終わってほしいし、hadronはHCALでなるべくシャワーをスタートさせてほしい。そういうわけでradiation lengthの差は大きければ大きいほどよい。
The basic unit of LAr calorimeter consists of a gap filled with liquid argon (1.1-2.2mm) generating the ionizined electrons, a copper-kapton electrodes to collect the ionized charge, 
and a steel-claded lead absorber layer to develop the EM shower (1.13-1.53mm). Bias voltage of 2000V between the electrodes and the absorbers is applied, achieving the drift time of 450ns. The detector is maintained at a constaint temparature of $88K$ by the cryostats surounding the barrel EM calorimeter. \\
%The readout current is tranfered into pulse signal ... 読み出しの話

%%%%%%%%%%
\fig[100]{Detector/LAr_cell.pdf}
{Geometry of barrel LAr sampleing layers. 
Position resolution is addressed by the innermost sampling layer by the highest $\eta-\phi$ granularity of $0.0031\times0.098$,
and the energy measurement is mainly provided by the second layer with the largest volume.
The third layer standing behind in the plot is the tail catcher collecting the information of shower profile.
    \cite{ATLAS_TDR}.}
{fig::Detector::LArcell}
%%%%%%%%%%

The geometry and cell segmentation varies between barrel and endcap depending on the required function.
Fig. \ref{fig::Detector::LArcell} (b) illustrate the segmentation in the barrel ECM. 3 sampleing blocks are placed along shower with different $\eta-\phi$ segmentation.
The first sampleing layer has the finest $\eta-\phi$ granularity ($0.0031\times0.098$), identifying the precise angular position of the incident particle. The second sampling addresses the largest volume ($16X_0$) layer contains the most of shower in which the energy is mainly measured. The third sampleing layer is intended to measure the very tail of EM showers, providing the information about longitudial shower profile together with the other layers. \\
The layer units are arranged in an accordion geometry, which is the characteristic to the barrel ECM, designed to be fully hermitic in terms of angular acceptance. \\
%endcap EM calorimeterの特徴
%Only the latter two sampling layers are 

In order to compensate for upstream energy loss, a presampling layer is additionally located in front of the EM calorimeter for both barrel and the endcaps.
%with the thickness of 11 mm (5 mm) for barrel (endcap).

The total thichness amounts to $>22X_0$ in the barrel and  $>24X_0$ in the endcap, which can fully accommodate the EM showers of photons or electrons in an energy of a few TeV.
The transition region between the barrel and endcaps ($1.37 < |\eta| <1.52$) is dedicated to detector services and is thus not fully instrumented.

%needed for EG reconstruction -> fine segmentation , METの計算
%ATLASは特にpi0->gamgam veto(?)とconverted photonのIDに力入れてるので特にangular resolution に優れたECALになってる。
%3D showers are reconstructed by units of topo clustering

%sampled -> measured bipolar pulse shape transfer?
%pileup contribution should be subtracted all the time->negative biasing

The designed resolution is given in Eq. \ref{eq::Detector::perf_calo} \cite{ATLAS_LAr_TDR}:
\begin{align}
\frac{\sigma_E}{E} = \frac{10\%}{\sqrt{E}} \oplus \frac{17\%}{E} \oplus 0.7\%.
\label{eq::Detector::perf_calo}
\end{align}

The energy resolution for the off-line objects can be further improved through the dedicated calibration exploiting the full detail of the shower and the information from the other detector. 
This high poiting resolution is the characteristic advantage of the ATLAS EM calorimeter, which vastly benefits the particle identification (photon, high energy tau etc.) as well as MET reconstruction.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Hadronic Calorimeter}
The ATLAS hadronic calorimeter consists of the barrel Tile HC ($|\eta|<1.7$) and endcap LAr HC.
The Tile HC is the sampling calorimeter composed by a periodic units of plastic scintillators tiles and steel absorber.
Fig. \ref{fig::Detector::caloCell} (a) schematizes one module in the Tile HC. Generated scintillation photons are read out by the photo-multiplier tubes equipped at the ends of the module via wavelength shifting fibers. 

Barrel Tile HC is segmented into three sections, the central barrel section ($|\eta|<1.0$) and the two extended barrel sections ($1.0<|\eta|<1.7$), using different channel dimensions. There are three sampling layers along the shower development with the thickness of 1.5$\lambda$, 4.1$\lambda$ and 1.8$\lambda$ for barrel, and 1.5$\lambda$, 2.6$\lambda$ and 3.3$\lambda$ for extended barrel respectively. \\

The endcap HC is the sampling calorimeter with liquid-argon sensor layers and copper absorber. 
The choice of material is dominantly based on the durablity against the extremely high radiation flux in the forward region.


The intrinsic resolution of barrel Tile HC and endcap LAr HC for an indivisual hadron jet is given as Eq. \ref{eq::Detector::perf_calo} \cite{ATLAS_Tile_TDR}: \\
% simulated ? measured?
%The gap between the sections is 4cm, in which the service cables for ID travel through.
\begin{align}
& \frac{\sigma_E}{E} = \frac{50\%}{\sqrt{E}} \oplus 3\%, \,\,\,\,\,\,\,\,\,\,\,\,  \mbox{(Tile HC)} \\
& \frac{\sigma_E}{E} = \frac{100\%}{\sqrt{E}} \oplus 10\%,  \,\,\,\,\, \mbox{(Endcap LAr HC)}
\label{eq::Detector::perf_calo}
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Forward Calorimeter}
A set of LAr calorimeter layers are arranged in a very foward region close to the beam axis covering $3.1<|\eta|<4.9$, 
designed to capture the full content from jets or particles from hard scattering particles from extremely boosted center-of-mass. The location with respect to the adjacent calorimeter systems are illustrated as Fig. \ref{fig::Detector::caloCell} (b).
Foward calorimeter is made by three sampling layers in which both functions of EM calorimeter and hadronic calorimeter are integrated; The first layer is with copper absorber working as EM calorimeter, and the later two layers are with tungsten functioning as EM calorimeter. An overlap with endcap HC is deriberated to realize smooth transtion.

%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.415\textwidth]{figures/Detector/tile_cell.pdf}}
    \subfigure[]{\includegraphics[width=0.575\textwidth]{figures/Detector/caloForward.pdf}}
    \caption{ (a) Illustration of a Tile HC module. (b)  Alignment of each detectors in an endcap; endcap LAr EM calorimeter (EMEC); endcap LAr Hadronic calorimeter (HEC); and the Forward calorimeter (FCal)   ) \cite{ATLAS_exp}.
      \label{fig::Detector::caloCell} }
\end{figure}
%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Muon Spectrometer}
Muon spectrometers are located outermost in the ATLAS, consisting of four sub-detectors; Monitored Drift Tube (MDT); Cathode Strip Chamber (CSC); Restric Plate Chamber (RPC); and the Thin-Gap Chamber (TGC). 
The former two are detecated to precision measurement of muon tracks and the latter two are to triggering. 
The spectrometer covers the pseudo-rapidity range $ |\eta|< 2.7$ and allows identification of muons with momenta above 3 GeV and precise determination of $\pt$ up to about 1 TeV with $10\%$ momentum resolution.

The magnetic field for tracking is sourced by the three pieces of toroidal superconducting magnets; two end-cap toroids at the ends of the detector, and a barrel toroid embedded in the space inside the muon spectrometers. $3.9T$ and $4.1T$ B-field is provided in the barrel and endcap region respectively. The internal volume of toroldal coils are vacant (``air-core''), in order to reduce the material with which muons experience the multiple scattering, which is one of the limiting factors of the momentum measurement. The integrated B-filed profile at the position of MDT is shown in Fig. \ref{fig::Detector::BField_MDT}, while the global schematic of the magnet system is given in Fig. \ref{fig::Detector::magnet}.

\fig[110]{Detector/magnet_captioned.pdf}
{Schematic of the ATLAS magnet system with one central solenoid and 3 toroidals (barrel+2 endcaps). \cite{ATLAS_exp}.}
{fig::Detector::magnet}

%\fig[110]{Detector/magnet_photo.pdf}
%{Phono of ATLAS cross-seciotn.}
%{fig::Detector::magnet_photo}

\fig[110]{Detector/Bfield_MDT.pdf}
{ Simulated magnetic field integral provided by a single troid octant, from the innermost MDT layer to the outermost. \cite{ATLAS_exp}.}
{fig::Detector::BField_MDT}




%%%%
%momentum measurement from 3GeV to 1TeV (?), with 10% resolution at 1TeV
% |eta|<1.7 for barrel and 1.6<|eta1<2.7 for endcap
%toroidal, 8 coil per toroid
%∫B_T d_T: 1.5Tm-5.5Tm for barrel, 1Tm - 7.5Tm for endcap
%%%%

\paragraph{MDT}
MDT is a gaseous drift chamber filled with the basic detection elements of 30 mm-diameter aluminium tubes that are covered by a $400\um$ thickness wall. 
Drifting electrons are absorbed in the center of a tube by a $50\um$-diameter tungsten-Rhenium wire with a bias voltage of 3080V is applied, and read out by a low-impedance current sensitive preamplifier.
The gas mixture is tuned at Ar : $\mathrm{CO_2} ()$ = $93\%:7\%$, maintaining the maximum drift time of 700ns. The single wire resolution is $\sim 80\um$.
%In order to boost the number of hits per particle passage, the tubes are assembled in a layer, and the layers are stacked along the path of parcitles flight, forming a muti-layer, as schematized in Fig. \ref{}. 
There are three layers of MDT chambers located both in barrel and endcap, covering pseudo-rapidity range of $|\eta|<2.0$.
The limitation in the $\eta$-coverage is determined by its maximum durable rate ($150 cm^{-1}s^{-1}$), instead CSC takes over the role in the further forward region.
%% performance


\paragraph{Cathode Strip Cahmber}
%%読み出し
The CSCs are multi-wire proportional chambers covering the foward region ($|\eta|>2.0$) in the endcaps, providing 2D position of incident particles.
It is operated with a gas mixure of Ar $(80\%)$ and CO2 ($20\%$) and with a bias voltage of 1900V.
The cells are symmetric in terms of the pitch of readout cathodes and the anode-cathode spacing, which is equally set to 2.54 mm.
As the spatial resolution of the CSCs is sensitive to the inclination of tracks and the Lorentz angle, To minimise degradations of the resolution due to these effects, they will be the chamber is fixed at tilted posture so that tracks originating from the IP become orthogonal to the chamber surface.
%% performance


\paragraph{Restric Plate Chamber}
The RPCs are digital gaseous detectors specialized in fast timing response for triggering.
They are mechanically mounted on the surface in the barrel MDT, covering the pseudo-rapidity range of $|\eta|>1.05$.
The elementary detection unit is a gas gap filled with non-flamable gas mixture (94.7$\%$ $\mathrm{C_2 H_2 F_4}$, 5$\%$ $\mathrm{Iso-C_4 H_10}$, 0.3$\%$ $\mathrm{SF_6}$). An uniform high electric field ($\sim$ 4900 V/mm) is applied so that the ionized electrons amplitude by themselves by the avalanches. Signal is read out by a metal strip attached on both ends of the gaps, arranged with a pitch of 30 mm - 39.5 mm.
The typical spatial and timing resolution achieves 1 cm and 2 ns respectively.

%2 plates for measuring eta, phi position


\paragraph{Thin-Gap Chamber}
The TGCs are a special type of multi-wire proportional chambers characterized by the notably small distance between the anode wires and the read out cathode strips (1.4mm).
Together with the highly quenching gas mixure with $\mathrm{CO_{2}}$ ($55\%$) and n-pentan ($45\%$), a quick drain of secondary electrons is achive with the timing response of 5 ns. TGCs also contribute to the momentum determination by supplementing the measurement in $\phi$ by MDT.
%A high voltage of 2900V is applied during the operation. 
Three modules are placed per endcap, covering $1.05<|\eta|<2.7$ by the innermost one and $1.05<|\eta|<2.4$ by the two behind. Trigger is generated using tracks in $1.05<|\eta|<2.4$, while all tracks are utilized for momentum measurement.


\fig[110]{Detector/ATLAS_muonDetector.pdf}
{Global view of the ATLAS muon spectrometers \cite{ATLAS_exp}.}
{fig::Detector::muon}


\fig[110]{Detector/muon_xsec2.pdf}
{Cross-section of the ATLAS Muon spectrometer \cite{ATLAScosmicPerf}.}
{fig::Detector::muon_xesc2}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Luminosity Detectors}
Luminosity determination is particular important since it provides the reference in normalizing simulated dataset and enables the sensible comparison to data. 
The instantaneous luminosity is calculated the formula below:
\begin{align}
\mathcal{L} = \frac{\mu n_b f_b}{\sigma},
\end{align}
where $n_b$ is the number of colliding bunches and the $f_b$ the frequency of the beam circulation.
$\sigma$ is total fiducial cross-section of $pp$ interaction including both elastic and inelastic scattering, and $\mu$ is the average number of such interaction per bunch crossing. While $\sigma$ is provided by a dedicated calibration measuring the lateral beam profile using overlaping two beams referred as the van der meer scan \cite{VdMScan}, $\mu$ is obtained directly by exploiting the rate information from luminosity detectors located in the very forward region neaby the beam pipe. Dedicated calibration and luminosity determination algorithm studied in \cite{LumiMeasurement}.

%Three luminosity detectors contribute to the luminosity measurement:
Two luminosity detectors mainly contribute to the luminosity measurement:
\begin{description}
%\item[BCM] (Beam Conditions Monitor) \\

\item[LUCID] (LUminosity measurements using Cherenkov Integrating Dector) \\
LUCID are located at the both ends of the ATLAS detector at a distance of 17m from the IP, covering the pseudo-rapidity range of $5.6<|\eta|<6.0$.
The LUCID detector consists of 16 aluminum tubes filled with $\mathrm{C}_4 \mathrm{F}_{10}$ gas, 
and designed to count the Cherenkov photons kicked out by charged particles flying along the beam axis which are mainly generated by proton-proton inelastic scattering in the IP.
The measurement is translated into luminosity, which is used for online monitoring of the instantaneous luminosity as well as the beam condition. 
%collcted by the photomultiplier set in the back-end

%array of 20 Cerenkov tubes (?) 
%pp非弾性散乱から来る破片のcharged particleの数を数える


\item[ALFA] (Absolute Luminosity For ATLAS) \\
ALFA is located beyond the ATLAS envelope at $z=\pm 240$ m, sandwiching the beam pipe from top and bottom.
The detectors are composed of 8 scintillating fibers, designed to mainly measure the elastic scattering component of the $pp$ interaction.

%小角のpp弾性散乱で飛んで来たpを測る. optical theoremでtotal cross-sectionわかるので、
\end{description}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Trigger and Data Acquisition System}
While ATLAS enjoys incrediblely high collision rate of about 100 MHz (40 MHz beam bunch crossing together with pile-up),
these data cannot entirely read out due to the limitation from data trasmission as well as the computation resource.
Luckily or unluckily, most of them are junk QCD reactions resulting in cheap low $\pt$ jets, 
the rate can be drastically surpressed by requiring hard jets, leptons or $\met$ in the events.

A highly sophisticated data acquisition system have been developed for ATLAS, referred as ATLAS Trigger and Data Acquisition System (TDAQ) \cite{trigger2015}, handling the trigger and readout. The schematic of the readout stream is shown in Fig. \ref{fig::Detector::triggerFlow}.
It consists of a two-staged trigger pipeline seved by the hardwere-based Level-1 Trigger (L1) and the software-based High-Level Trigger (HLT).
The idea is to reject the major trivial QCD events in L1, based on a fast particle reconstruction with a coarse resolution, and perform further filtering in HLT based on using more sophisticated reconstruction and energy measurent, enjoying the timing latency that L1 earns. The benchmark of rate suppression is 100 kHz at the end of L1 and down to 1 kHz after the HLT on average. \\
The L1 consists of two independent sub-trigger systems: L1Calo, identifies the EM or hadronic clusters in calorimer and reconstruct primitive jets, electrons, photons and taus (L1 objects) with calibrated energy in EM scale.; L1Muon, identifies and measure the tracks in the muon pectrometer, designed to accept events with muons. 
The object reconstruction is based on the coarsely segmented blocks of combined detector channel called ``trigger tower'' with $\eta\times\phi$ granularity of $0.1 \times 0.1$. 
%RoIの話も?
$\met$ is also calculated at the L1 stage by the vectorial sum of the calorimeter deposits, referred as L1XE. Trigger accept is issued by the Central Trigger Processors (CTP) when the L1 objects meet certain criteria in terms of $\pt$ thresold and number of objects.  \\

In HLT, an offline-like algorithm is employed to refine the energy of L1 objects, or recover the mis-identified objects (typically low $\pt$ muons) by scanning over whole detector. This is performed by a set of custom farmwares with a processing time of 0.2 s on an average. The event that is triggered by the HLT is subsequently sent to event storage infrastructures outside the ATLAS. 
Fig. \ref{fig::Detector::trigger_ratePhys} illustrates the acception rate of HLT in 2016 operation. \\

The performance of triggers relevant to the analysis is dedicatedly overviewed in Sec. \ref{sec::Samples::trigger}.

%Relevant trigger chains to the thesis is shown as Tab. \ref{}.
%The $\met$ trigger is used as the main trigger.
%The single-lepton triggers supplement the 

% HLTはofflineにかなり近いparticle reconstruction/identification/calibrationを使う
%
\fig[110]{Detector/triggerFlow.pdf}
{The logic of ATLAS trigger system \cite{trigger2015}. Trigger detectors have seprated readout line for trigger, sending input information for trigger decision to CTP. 
CTP reconstruct L1 objects and issue a global accept signal relieving the buffered data, once the trigger criteria are satisfied. 
The $(\eta,\phi)$ position of identified trigger object is sent to downstream HLT, in which off-line like software based trigger runs to filter events further.
L1 topological trigger (L1 Toplo) and Fast Tracker (FTK) have been in comissioning since 2015. 
%L1Topo is designed to offer triggers exploiting the event topoloy information (e.g. azimuthal angle difference between two particles) using the combined output from L1Calo and L1Muon. FTK is the hardware-based fast tracking module working in the L1 time desgined to feeding the reconstucted tracks HLT.
}
{fig::Detector::triggerFlow}

\fig[110]{Detector/trigger2016_physicsRate.png}
{Rate of HLT stream for physics analysis during the 2016 data-taking \cite{trigPubResult}. Horizontal axis is in unit of lumi-clock, the smallest unit of data-taking in the same configuration.}
{fig::Detector::trigger_ratePhys}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Recorded Data by ATLAS}
The $pp$ collision data analysed in the thesis were collected by ATLAS during 2015 and 2016 at a centre-of-mass energy of 13 TeV. 
The evolution of the peak luminosity and  corresponding average pileup increase are shown in Fig. \ref{fig::Detector::DAQ}.

Data-quality requirements are applied based on each lumi-block, the smallest unit of data-taking defined as the period in the run configuration which varies conditions of beam and detector. 
Rejected data is typically at the period with more than a certain of fraction of modules in the sub-detectors being disabled or in a wrong operation voltage.
Integrated luminosity delivered by LHC and recorded in ATLAS during the 2016 operation is shown in Fig. \ref{fig::Detector::dataInt}. After the quality requirement the luminosity of data used for analysis is to $32.9\ifb$, to which $3.2\ifb$ from 2015 data taking is added, amounting to $36.1\ifb$ with the measurement erorr of $\pm3.2\%$. \\

\fig[110]{Detector/intLumi2016.png}
{Evolution of integrated luminosity delivered by LHC (green) and recorded in ATLAS (yellow) in 2016 oepration \cite{DAQ2016}.}
{fig::Detector::dataInt}

\section{Object Reconstruction and Identification}  \label{sec::objDef}
The raw detector-level information of particles is translated into phyics quantities through the sequence of particle reconstroction, identification and calibration.
Though this is partially done at the trigger level, the recorded events are further elaborated by the sophisticated off-line algorithms, enjoying the detail of full event information and absence of limitation in computation resource and time.
These particles reconstructed off-line refer to ``object''. In this section, the object consturction methods are extensively overviewed, paticularly with focus on electrons, muons and jets that are used in the gluino search 1-lepton analysis.
%algorithmもそうだが、そもそも使える素材が多い。例えばtrackが使える. standardなtrackingはtime-consumingなので（特にpileupに対してnon-linearに計算量が増えるので）triggerには今のところ使われていない

%reco/id/calibのterminologyは以下の通り
%- reco: combination of particle finding, loose particle identification and 4-momentum detemination
%- id:   dedicated separation algorithm for rejecting fake objects
%- calib.: including efficiency measurement and correction on simulation


\subsection{Tracks} \label{sec::objDef::tracks}
Charged tracks are the fundamental units seeding almost in all the off-line particle reconstruction. 
Standard tracks used in ATLAS refers to ID tracks, reconstructed by the hits create in the inner detector (ID).
The MS tracks for muon identification are separatedly reconstructed, which is described in Sec. \ref{sec::objDef::muons::reco}.
The reconstruction algorithm mainly consists of 5 steps concised as following. 
More detail can be found in \cite{130_trackingRun2}.


\begin{itemize}
\item Based on the 3-dimensional position information and the readout charge associated to each hit, 
spatial charge profile is constructed event-by-event. 
Hits from the same partcle traverse are merged, using a combination of a pattern recognition technique called connected component analysis (CCA) \cite{CCApatterRecog}, and a neural network classifier \cite{NNClustering}.
Seed tracks are then reconsructed from three aligned clusters.

\item The seed tracks are extrapolated outward,
and the association with the TRT hits are tested using the Kalman fitter charactrized by five tracking parameters,
with a pion track hypothesis assuming the MIP energy loss in the ID material.

\item If the first pattern recognition fit fails, a second fit is attempted with a modified algorithm that allows for energy loss at each hit surface. This procedure recovers electrons with significant energy loss due to bremsstrahlung.

\item Successful tracks from the Kalman Filter are rerun using the ATLAS Global $\chi^2$ Track Fitter \cite{157_ATLASGlobTrackFitter}.
A pion or an electron hypothesis is used, depending on which was used successfully in the previous step.

\end{itemize}
A refined algorithm (Tracking In Dense Environment; TIDE) is used from Run2 \cite{130_trackingRun2}, 
to cope with denser particle environment due to the increased pile-up and collision energy.
The performance is shown red lines in Fig. \ref{fig::objDef::trackEff1}. Typically over $95\%$ of efficiency is maintained. \\ 

%Standard trackingはjet中のpionやelectronを想定してこのように内側のdetectorで作ったseedを外側に外挿してfindされるが, 
%外側(TRT)でseedを作って内側に外挿してくalgorithmも存在する。これはconverted photonをIDするためのalgorithmであり, photon IDに使われる.
%%%%%%%%%%
\fig[110]{ObjectDef/trackEff1.pdf}
{Reconstruction efficiency of tracks in jets as function of anglular distance with respect to barycenter of the jet \cite{130_trackingRun2}}. Red points corresponds to the tracking algorithm used from Run2.
{fig::objDef::trackEff1}
%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Primary vertices} \label{sec::objDef::PV}
The positions of $pp$-collisions are identified using the reconstructed ID tracks. These vertices refers to ``primary vertices'' (PV) 
\footnote{The ``primary'' is meant to distinguish with vertices genearted by the late decaying particles known as ``secondary-vertices''.}
and are important for providing reference origin point of retracking and objects calibrations.

PVs are reconstructed using the Iterative Vertex Finding algorithm \cite{134_vertexing_Run1}\cite{135_vertexing_Run1_2012}, indentifying the peak in the $z$ distribution of extrapolated tracks. The positon of identified PVs are further elaborated using the adaptive vertex fitting algorithm \cite{136_adaVertexFit}.

The ID tracks are then re-fit taking advantage of these reconstucted PVs. The retracking procedure in principle lasts until all the tracks are associated to either of the PVs. PVs with less than two associated tracks are discard. 
%Fig. \ref{fig::objDef::vertexEff} shows the simulated reconstrution efficiency for PVs under the Run2 condition.

Though $10-30$ PVs are reconstructed per buch corssing, usually there is only one PV causing meaningful scattering reaction that fires the trigger. This PV is referred as the ``hard-scatter'' (HS) vertex  identified as the PV with highest sum of associated track $\pt$ ($\sum\pt$), and the position is used as the origin for object calibration. \\

%\fig[160]{ObjectDef/vertexEff_136.pdf}
%{Efficiency of primary vertex detection.}
%{fig::objDef::vertexEff}





%%%%% 4,2,0 is Run1 parameters
\subsection{Topo Cluster} \label{sec::objDef::TopoCluster}
Topo-cluster (or TC) is the basic unit of energy measurement in calorimeter and used as the input for jet clustering  (Sec. \ref{sec::objDef::jets::clustering}) as well as in computing the isolation variables (Sec. \ref{sec::objDef::fakeAndIsolation}). 
It is formed by three-dimentionally grouping the cells with signifincant energy deposit.
The clustering algorithm proceed as follow \cite{138_topoClustering_Run1}:
\begin{itemize}
\item Find cells with energy deposit exceeding $4\sigma$ from the expected noise level. These cells are identified as seed cells.
\item Neighbouring cells touching the boundary of seed cells with energy deposit exceeding $2\sigma$ from the expected noise level are added to the cluster and become the seed cells for the next iteration.
\item Iterate the previous step until the cluster stops growing.
\item Split the cluster if there are two or more local maxima with $E_{\mathrm{cell}}>500\mev$.
%how?
\end{itemize}
EM-scaled energy is assigned for TCs. \\

%Fig. \ref{fig::objDef::TC} shows the 
%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/TCnoiseLv_mu30.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/nTC.pdf}}
%    \caption{ (a) Pile-up dependence of noise level in calorimeter. (b) Number of reconstructed topo-cluster. 
%      \label{fig::objDef::TC} }
%\end{figure}
%%%%%%%%%%



\subsection{Electron} \label{sec::objDef::electrons}
%Fig. \ref{} schematizes the electron interaction with detectors and the involved detector system electron in the reconstruction and identification.

\subsubsection{Reconstruction} \label{sec::objDef::electrons::reco}
The electron reconstruction algorithm proceeds as following, widely referred from \cite{156_ElectronEffMeas_2015data}: 

\begin{itemize}
\item Reconstruction of a EM cluster from energy deposit in the EM calorimeter.
This is done by the sliding window algorithsm. Cells in the all four layers in the EM calorimeter are grouped into $\eta\times\phi$ towers of $0.025\times0.025$, and a window defined by the $3\times5$ units of towers are slided over the detector. A local maximum in the window energy above $2.5\gev$ is identified as the cluster. About $95\%$ ($99\%$) of clustering effciency are maintained with electrons in $\ET=7 \gev$ ($>15 \gev$).
%larger window is used to accommodate the electrons with energy loss due to the brems

\item Track-Cluster matching \\
The EM cluster is matched in the angular space with $|\Delta \eta|<0.05$ and $-0.2<\Delta \phi<0.05$ 
where positive $\Delta \phi$ corresponds to case in which the fitted track is bending away from the cluster barycente.
Closest track in $\Delta R$ with respect to the EW cluster is chosen if multiple tracks satisfy the matching criteria.
%if failed, rescale the track energy and test if $|\eta|<0.05$ and $-0.1<\phi<0.05$  (trajectoryはどうやって変える？)

\item Track refitting \\
The matched track enjoys further correction by a re-tracking using the Gaussian Sum Fitter (GSF) \cite{158_GSF} algorithm in which Bremstralung is dedicated modeled.

\item energy determinaton
The information from track momentum and calibrated EM cluster energy are combined using a multivariate algorithm \cite{160_ElecCombMVA}, 
achieving the best available energy resolution.
%低い方だとtrackの方が強い.. とかも
\end{itemize}

The reconstruction efficiency is measured by $Z\ra ee$ events. Fig. \ref{fig::ObjDef::electronRecoEff} presents the result together with the prediction by MC. Over $96\%-98\%$ of efficiency is achived for $\ET>2-\gev$.

\fig[160]{ObjectDef/electronRecoEff_156.pdf}
{ Reconstruction efficiency simulated (grey) or measured (blue) using $Z\ra ee$ events \cite{156_ElectronEffMeas_2015data} as function of (a) $\ET$, and (b) pseudo-rapidity of reconstructed EM clusters.}
{fig::ObjDef::electronRecoEff}


%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/electronRecoeff_pt_156.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/electronRecoeff_eta_156.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%



\subsubsection{Identification} \label{sec::objDef::electrons::id}
Reconstructed electrons are dominated by fakes from pions from jets, parcilularly when they are low-$\ET$. Therefore, a powerful idetification algorithm is employed in the subsequnt identification, using a multi-dimensional likelihood exploiting all the relevant detector information. The input variables amounts up to 17, including the longitudinal and transcer EM shower profile and the number of high-threshold hits in TRT etc. The full list of input variables are found in \cite{156_ElectronEffMeas_2015data}.
The discriminant is given by a form of ratio, which is known to generally provide the best separation \cite{NPLemma}.
The signal and background PDF is modeled using the simulated events of Zee and di-jet respectively.
%great advantage over the cut-based id since each variable 
%こいつはHLT electron triggerにも入っててon-lineのperformanceも上がったし, 
%off-lineとのsynergyによってturn-onもバリ改善した
Fig. \ref{fig::ObjDef::electronIDEff_156} shows the effifiency ....
Multiple working points are available with different cut value in the likelihood ratio. In the anaylsis two working points``Loose'' and ``Tight'' are used, which correspondto about $90\%$ $70\%$ of efficiencies at $\ET = 30\gev$.

\fig[160]{ObjectDef/electronIDEff_156.pdf}
{Electron identification efficiency as function of (a) $\ET$, or (b) pseudo-rapidity of reconstructed electron candidates \cite{156_ElectronEffMeas_2015data}. $Z\ra ee$ events are used for both MC and data.}
{fig::ObjDef::electronIDEff_156}


%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%




\subsubsection{Calibration} \label{sec::objDef::electrons::calib}
The electron calibration consists of several different procedures, differently applied to simulation and data.
The flow of steps is illustrated in Fig. \ref{fig::objDef::elecCalibFlow}.

\begin{description}
\item \textbf{A MC-based calibration using BDT} \\
Though the energy of cell deposit in EM calorimeterand and electron cluster is already calibrated in EM scale, 
it still suffers from residual due to the energy losss in the material upstream of the calorimeter, energy leakage out of the either the reconstructed clusters or EM calorimeter and so on.
A multi-variate algorithm (BDT regression) is employed, 
estimating the true energy from the various input including the raw energy of reconstructed electron, as well as other angular position, shower profile and hit information from other auxiliary detectors such hadronic calorimeter.
The full detail can be found in \cite{161_egammaCalibRun1} \cite{egammaCalib2015}. \\

\item \textbf{Longitudinal calorimeter layer inter-calibration} \\
The scales along longitudinal layers is equalised in data with respect to simulation, prior to the determination of the overall energy scale, in order to ensure the correct extrapolation of the response in the full $\pt$ range. This is only applied to elecrons in data.

\item \textbf{Non-uniformity correction in $\phi$} \\
A set of corrections are applied to data, to account for various on-line instrumental effects not included in simulation such as non-optimal high voltage regions, geometric effects such as the inter-module widening or biases in the LAr calorimeter electronic calibration.

\item \textbf{Residual scale calibration on data / Resolution correction on simulated electrons.}  \\
The residual mis-calibration in data is corrected by shifting the energy scale so that it agrees with the expectation from simulation. This is done by comparing the mass of Z-peak in $Z \ra ee$ events.

It is found that the resolution in data is slightly worse than that in simulation using the same event sample.
The corrections are derived and applied to simulation to match the data. 
\end{description}
Numerous minor corrctions follow addtionally, which is detailed in \cite{161_egammaCalibRun1}. 
The calibration is widely validated using data events of $J/\psi \ra e e$ and $Z\ra ee$.

%\cite{162_ECALTB_ATLAS}

\fig[130]{ObjectDef/elecCalibFlow.pdf}
{Flow chart of electron calibration applied respectively MC and data \cite{161_egammaCalibRun1}.}
{fig::objDef::elecCalibFlow}

%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/electronReso_161.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/electronResoUnct_161.pdf}}
%    \caption{ (a) Energy resolution after the calibration (solid line) and resolution uncertainty (band).  (b) Breakdonw of   
%     \label{fig::objDef::electronReso} }
%\end{figure}
%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Muon} \label{sec::objDef::muons}
\subsubsection{Redconstruction} \label{sec::objDef::muons::reco}
Muon tracks are reconstructed independently from ID, referred as MS-tracks. 
The tracking begins with finding the hits inside each MDT/CSC chamber and forming small track segments per chamber. A Hough transferm is employed to convert the bending detector plnae geometry into flat plane. A straight-line fit are then performed on the flattened plane for the track segments. 
The hits in RPC and TGC are used to determine the coordinate orthogonal to the MDT/CSC detector plane. The search algorithm employ a loosened requirement on the compatibility of the track and the hits, to account for the muon energy loess by interaction with material.

The trajectory and momentum of muons are decided by a synergy between the reconstructed MS track and the measurement by the other detectors.
There are four different schemes of combination \cite{165_muonPerf2011_2012}:
\begin{description}
\item \textbf{Combined muons:} 
A MS track is matched to a reconstructed track in the ID, and the measurements of the momenta are combined.

\item \textbf{Segment-tagged muons:} 
  A fragmet of MS track is matched with an ID track, with the momentum taken from the ID track.

\item \textbf{Standalone muons:} 
  MS tracks found outside the ID acceptance ($2.5 < |\eta| < 2.7$), with the momentum quoted from the MS track.

\item \textbf{Calorimeter-tagged muons:}
  A special type of reconstuction dedicated to muons traveling to the inactive “crack of the MDT at $|\eta|<0.1$.
  The ID tracks with $\pt>15\gev$ associated calorimeter deposit consistent with a minimum ionizing particle are tagged, with the momentum of ID track.
\end{description}
In this analysis, the combined muons is always in defining muons, while the segment-tagged muons are used for correcting the MET calculation as described in Sec. \ref{sec::objDef::met}.


%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%

\subsubsection{Identification} \label{sec::objDef::muons::id}
Additional identification requirements are imposed to purify the sample of reconstruction muons.
Cuts on following three variables are appleied:
\begin{description}
\item $\sigma(q/p)$: Fitting error of a tracking parameter $q/p$ associated with the quality of measurement. \\
\item $\rho'$:     \mbox{\phantom{MM}}  $\pt$ difference between ID and MS track normalized by the $\pt$ of the combined track. \\
\item $\chi^2$:   \mbox{\phantom{MM}}  
  A generic measure of fit quality defined as normalised $\chi^2$ of the combined track fit.
\end{description}
The \texttt{Medium} working point defined in \cite{166_muonPerformance2015data} is used throughout the analysis, 
where only $\sigma(q/p)<7$ is required.  \\

Fig. \ref{fig::ObjDef::muonRecoIDEff} summarizes the performance of reconstruction and ID for muons.

%%%%%%%%%
\fig[160]{ObjectDef/muonRecoIDEff_166.pdf}
{ Simulated / measured efficiency for reconstuction and identification of muons, using $J/\phi\mu\mu$ and $Z\ra\mu\mu$ events \cite{166_muonPerformance2015data}.}
{fig::ObjDef::muonRecoIDEff}


%%%%%%%%%%
%\begin{figure}[h]
%  \centering
 %   \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%



\subsubsection{Calibration} \label{sec::objDef::muons::calib}
As the momentum of a muon track already well representing the particle-level momentum of muon, 
the scale calibration only subjects to a series of minor corrections, accounting for the imperfect knowledge of the magnetic field intergral inside the detector, and the energy loss of muons traverse the calorimeter and other materials between the interaction point and the MS.

The momentum correction is performed on each muon based on the formula below \cite{165_muonPerf2011_2012}:
\begin{align}
\pt^{\mathrm{Cor.}} = \frac{ s_0  + \pt^{\mathrm{MC}}  (1+s_1) } {1+ \Delta r_0  g_0  + \Delta r_1 
 \pt^{\mathrm{MC}} g_1  + \Delta r_2 \left( \pt^{\mathrm{MC}} \right)^2  g_2} 
\end{align}
where $\pt^{\mathrm{MC}}$ and $\pt^{\mathrm{Cor.}}$ represent respectively the momentum before and after the correction, and $g_m (m=0,1,2)$ are random numbers generated by an uniform PDF ranging from 0 to 1.
The numerator corresponds to the scale correction, and the denominator is responsible for the correction of resolution modeling by MC. The parametrization of denominator is based on the fact that muon resolution obeys a $\pt$ dependence of:
\begin{align}
\frac{\sigma(\pt)}{\pt} = \frac{a}{\pt} \bigoplus b \bigoplus c \cdot \pt.
\end{align}

The coefficients $s_i$, $\Delta r_i$ are determined bin-by-bin in $(\eta,\phi)$, 
by applying a template fit on $J/\phi\ra \mu\mu$ and $Z\ra \mu\mu$ events.

%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
 %   \caption{ (a)  (b). 
 %     \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Jet} \label{sec::objDef::jets}

\subsubsection{Jet Clustering} \label{sec::objDef::jets::clustering}
Jet recontruction starts employs the AntiKt algorithm \cite{141_antiKt} using the topo-clusters (TCs) calibrated with EM scale as input.
The basic step of the algorithm is to merge the proximate two TCs based on a distance measure defined by:
\begin{align}
d_{i,j} & = \min(p_{T,i}^{-2},p_{T,j}^{-2}) \frac{\Delta R^2_{i,j}}{r^2} 
\end{align}
where $i$ and $j$ denote the index of topo-clusters, and $\Delta R^2_{i,j}$ is the angular distance between the them.
$r$ is the cone parameter dictating the typical size of resultant jets, which is set to $r=0.4$ in the analysis.
The two TCs with smallest $d_{i,j}$ are merged in each step, and the iteration continues until it becomes:
\begin{align}
\min_{i,j} \left[ d_{i,j} \right] > \min_{i} \left[ p_{T,i}^{-2} \right].
\end{align}
The \antikt jet clustering is characterized by the negative power index on the $\pt$ in the metric $d_{i,j}$, 
where soft clusters are added in the cluster at the early stage of iteration.
This results in a well-defined boundary of jets, indicating that the jet clustering is insensitive to soft components inside the jet where the purtabation QCD does not provide robust prediction. This collinear- and infrared-safty is an extremely welcomed feature since it provides well-defined observables allowing one to straightwardly compare the theory and data, benefiting either the theoretical description and the jet calibration in experiment.

%FastJet package [142]
%AntiKt[143]

%%%reason?

%後で述べるように摂動論で高次効果の寄与の一部をparton showerでmodelしているが、QCDのscaleに近づくに従い破綻するので、1GeV以下くらいのsoft/collinear radiationはまともに計算ができない。一般的にKt algorithmなどのstraightfowardなclusteringではjetの境界などはこれらのsoft radにsensitiveであり、total energyなどのobservableもaffectされる.
%一方でAntiKt algorithmはsoft clusterからmergeされるnatureのおかげでそれらに対してrobustである.
%理論とのstraghtforwardな比較ができるようになるというのが最大のadvantageであり

%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%


%%%%%%%%%
\subsubsection{Energy Calibration} \label{sec::objDef::jets::calib}
As the energy of TC is calibrated in the EM scale, clustered jet needs extra calibration to account for the hadronic interaction activity. Particle-level jets in simulated events (referred as ``truth jets'') are used for the reference of the truth energy. They are reconstructed using the anti-kt algorithm with $R=0.4$ using stable, final-state particles as input. The input particles are required to have a lifetime of $c\tau > 10m$. Muons, neutrinos, and particles from pile-up activity are excluded. Truth jets with $\pt>7\gev$ and $|\eta|<4.5$ are used for the calibration. In simulated events, corresponding reconstructed calorimeter jets can be found by geometrically matching in  terms of the $\Delta R := \sqrt{(\Delta \eta)^2+(\Delta \phi)^2}$ \\

A dedicated calibration procedure detailed in \cite{144_JESmeas_2015data} is employed
to restore the energy to that of truth jets reconstructed at the particle-level energy scale. It mainly proceeds as following stages:


%%%%%%%%
\begin{description}
\item \textbf{Origin correction} \\
The angular coordinates assigned to each topo-cluster is based on the origin defined by the designed IP position with which the actual hard-scatter vertex is displaced in $z$-axis direction. The jet orientation is recalculated based on the refined origin defined by the position of the reconstructed vertex that the jet is associated with \cite{JESmeas_unct_Run1_inclOC}. \\


%%%%%%%%
\item \textbf{Pileup subtraction} \\
The contribution of particles from pile-up jets either in the same bunch crossing (``in-time pile-up'') or those nearby (``out-of-time pile-up'') is removed using the technique of an area-based $\pt$ density subtraction \cite{145_areaBasedPUsub} applied at the per-event level, followed by a residual correction derived from the simulation. The correction is characterized as:
\begin{align}
\pt^{\mathrm{corr.}} = \pt^{\mathrm{reco.}} - \rho \times A - \alpha  \times (N_{PV}-1) - \beta \times \mu,
\label{eq::PUresidual}
\end{align}
where $\pt^{\mathrm{reco.}}$ and $\pt^{\mathrm{corr.}}$ are the transverse momentum before and after the correctino respectively. $A$ is the jet area which roughly corresponds to the area jet energy distributes in $\eta-\phi$ plane calculated using the ghost-association \cite{143_JetFindingReview}. $\rho$ is the average $\pt$ density from the contribution of pile. The idea is to treat the pile-up as an uniform noise level over the detector, and the contribution is proportional to the area the jet is overlaying to it. 
%%%% NEED TO CORRECT
The residual impact of pile-up is found to be linear in terms of the number of reconstructed primary vertices ($N_{\mathrm{PV}}$) and the average number of interactions per bunch crossings ($\mu$) independent of one another.
%These accounts for the underestimation by the area-based subtraction on the ``in-time'' and ``out-of-time'' contribution respectively. 
The linear coefficients $\alpha$ and $\beta$ are determined using the simulation as function of $\pt$ and $\eta$ of the jet.
%Fig. \ref{}

%%%%%%%%%
%\fig[160]{ObjectDef/PUrejection_144.pdf}
%{ Residual dependence of jet $\pt$ on pile-up; (a) number of reconstructed PV, and (b) average   \cite{144_JESmeas_2015data}.}
%{fig::ObjDef::PUrejection_144}



%%%%%%%%
\item \textbf{MC-based calibration} \\
The main energy calibration is provided by comparing the energy of reconstructed jets to the corresponding truth jets in the simulated di-jet events from \pythia. The energy response $R$ and $\eta$ response $R_\eta$ defined by
\begin{align}
R = \left< \frac{\pt^{\mathrm{reco.}}}{\pt^{\mathrm{truth}}} \right>, \,\,\,\, 
R_\eta = \left< \frac{\eta^{\mathrm{reco.}}}{\eta^{\mathrm{truth}}} \right>, 
\label{eq::jetResponse}
\end{align}
are calculated in various $\pt$ and $\eta$ bins. The obtained response is used for the scale that brings the energy of reconstructed jets to the particle-level energy scale. The conversion from the EM scale to the hadronic scale essentially happen in this stage.
%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/jetResponse_pt_144.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/jetResponse_eta_144.pdf}}
    \caption{ (a) Energy response and (b) $\eta$ reconstruction bias defined in Eq. \ref{eq::jetResponse} before the MC-based calibration. \cite{144_JESmeas_2015data}
      \label{fig::objDef::jetResponse} }
\end{figure}
%%%%%%%%%%





%%%%%%%%
\item \textbf{Global Sequential Calibration} \\
While only the information on topo-clusters are used for the jet energy determination so far, further improvements are achieved by applying corrections exploiting the global detector information from calorimeter, muon detector, and reconstructed tracks from inner detector. 

The procedure involves 5 independent stages, referred as the Global Sequential Calibration (GSC) \cite{140_jetEneMeas_TCcalib}, killing residual dependence of jet energy scale on the number of associated tracks or the spatial energy profile of the jet and etc. using the simulation. 

The most important function of GSC is adding robustness against varying jet flavors, in particular between quark-initiated jets and gluon-initiated, in jet energy measurement.


%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
 %   \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
 %   \caption{ (a)  (b). 
 %     \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%




%%%%%%%%%
\item \textbf{Residual in-situ calibration} \\
A residual calibration is derived using in-situ measurements applied only to data, accounting for the differences in the jet response between data and MC simulation.
%Such differences arise from the imperfect description of the detector response and detector material in MC simulation, as well as in the simulation of the hard scatter, underlying event, pile-up, jet formation, and electromagnetic and hadronic interactions with the detector. 
The differences is quantified using data events of $\gamma+\mathrm{jet}$ and $Z\ra \mu\mu + \mathrm{jet}$,
by balancing the $\pt$ of a jet against the well-measured counterpart objects as reference.
%Similar examination are also performed on di-jets events, using the balancing technique

\end{description}



%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/JESUnct2015_pt_144.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/JERUnct2015_pt_144.pdf}}
    \caption{ (a) Uncertainty on jet energy scale (JES), and (b) uncertainty on the relative resolution, with the breakdown of each sources being attached \cite{144_JESmeas_2015data}.
      \label{fig::objDef::JERUnct2015_pt_144} }
\end{figure}
%%%%%%%%%%



%%%%%%%%%
\subsubsection{B-tagging} \label{sec::objDef::jets::btag}
Hadron jets originating from $b$-quarks can be exclusively identified 
by taking advantage of the long lifetime ($c\tau\sim 450\um$) of $b$-hadrons,
creating distinct secondary decay vertices. 
Four independent sub-algorithms (IP2D, IP3D, SV, JetFitter) exist addressing unique b-finding power. 
Their outcomes are combined by inputing them into a BDT classifier (MV2), which output is used as the final discriminant. Each sub-algorithm works as following (widely referred from \cite{150_bTag_Run2_exp} \cite{151_bTag_Run2_perf} \cite{bTag_Run2_2015data}): 

\paragraph{Impact parameter based algorithm: IP2D and IP3D}
IP2D and IP3D are the likelihood based classifiers using the impact parameter information of tracks assiciated to the jets. 
The track level likelihood is defined in terms of the transverse impact parameter $d_0$ and its significance $\sigma(d_0)$ (and longitudinal impact parameter $z$ for the case of IP3D), and modeled using MC respectively for the tracks in the $b$-jet and light-flavor jet. The jet-level likelihood is calculated by taking the prodect over all the associated tracks to the jet.
The IP2D (IP3D) is then defined by the likehood ratio between the $b$-jet and light-flavor jet hypothesis.


\paragraph{Secondary vertex finding algorithm: SV}
The SV algorithm \cite{152_SV} explores secondary vertex finding algorithm in an explicit manner. 
After a set of qualification requirements on tracks in the jet, all the seed tracks are paired testing the consistency with the two-track vertex hypotheses. Found vertices consistent with the decays of other long-lived particles (such as $K_s$ or $\Lambda$), photon conversions or hadronic interaction with a material are rejected. As further requirements, the sum of the two impact parameter significances of the two tracks is required greater than 2, and vertices with the invariant masses exceeding $6 \gev$ are removed given the masses of the b- or c-hadrons. 
Vertex with the highest invariant mass is chosen if multiple candidates are found. %%%(要出典)


\paragraph{Decay chain multi-vertex algorithm: JetFitter}
\texttt{JetFitter} \cite{153_JetFitter} is a kinematic fittering algoriths, exploiting the topological structure of weak b- and c-hadron decays inside the jet and attempt to reconstruct the full b-hadron decay chain. Using the Kalman fitter, it finds a common line to which the PV and the bottom and charm vertices belong, approximating the b-hadron flight path, as well as their positions. The notable advantage of this approach is that the vertices of b- and c-hadron can be reconstructed, even when only a single track is attached to any of them.

\paragraph{Combinating algorithm: MV2 }
A Boosted Decision Tree (BDT) is used to combine the output from the four algorithms.
The input variables includes
the likehood values from IP2D and IP3D,
properties of reconstructed secondary vertex (mass, position etc.) and the associated tracks providing by SV,
and the information of fitted vertices including sebsequent decays of b-hadrons from JetFitter.
The full list can be found in \cite{150_bTag_Run2_exp}.

The output distribution and the performance is shown in Fig. \ref{fig::objDef::MV2c}.
Although the input information between the algorithms is highly correlated, the combined performance shows drastic improvement over those of either single algorithm.

Multiple working points are defined to provide different relative discrimination power agasint light-flavor jets and c-jets.
For example, MV2c10 (MV2c20) are designed to address more rejection power towards c-jets, trained using the background sample with light-flavor jets admixtured with c-jets by $10\%$ ($20\%$). 
The MV2c10 working point is used in the analysis.



%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%



\fig[160]{ObjectDef/MV2c_150.pdf}
{Left plot presents the output BDT distribution for signal (b-quark jets) and backgrounds (light flavor and c-quark jets). The score of MV2c20 is shown in which c-jets rejection is reinforced.  The middle and right plot respectively show the signal efficiency vs light flavor jet rejection, and vs c-jet rejection.
 \cite{150_bTag_Run2_exp}}
{fig::objDef::MV2c}


%b-jetはsemi-leptonic decayでneutrinoが出るので通常resolutionが悪め
%calib?


%%%%%%%%%
\subsubsection{Pile-up Jet Tagging and Rejection} \label{sec::objDef::jets::JVT}
\newcommand{\pv}{\mathrm{PV}}
\newcommand{\pttk}{\pt^{\mathrm{trk}_k}}
\newcommand{\pttl}{\pt^{\mathrm{trk}_l}}
Significant fraction of reconstructed jets are originated from pile-up, particularly when they are low-$\pt$.
In order to suppress the contamination, a pile-up jet rejection is applied using the Jet Vertex Tagger (JVT) discriminant \cite{155_JVT} exploiting the vertex information.  \\

JVT is based on a 2D-likelihood function in terms of the corrected Jet Vertex Fraction (corr. JVT) and $R_{\pt}$:
\begin{align}
\mathrm{corrJVF} & := \frac{ \sum_k \pttk (\pv_0)  }{\sum_l \pttl (\pv_0) + \sum\pt(\mathrm{PU})/ (\kappa \cdot n_{\mathrm{trk}}^{\mathrm{PU}} )  }
,  \,\,\,\,\,  \sum\pt(\mathrm{PU}) = \sum_{n \ge 1} \sum_k  \pttk (\pv_n)  \nn \\
R_{\pt} & := \frac{\sum_k \pttk (\pv_0) }{\pt^{\mathrm{jet}}},
\end{align}
where $\pv_0$ denotes the hard-scatter vertex and  $\pv_j (j \ge 1)$ the other primary vertices presumably dut to the in-time pile-up interaction. 
%$n_{\mathrm{PU}}$ is the total number of pile-up tracks per event with the scaling factor $\kappa = 0.01$ determined by correlation with nl l pT (PVn) and ntrk . 
JVF (Jet Vertex Fraction) was a variable originally used for the pile-up supression in Run1 \cite{JVF} defined by the fraction of charged tracks associated to the HS vertex:
\begin{align}
\mathrm{JVF} := \frac{ \sum_k \pttk (\pv_0)  }{ \sum_l \pttl (\pv_0) + \sum\pt(\mathrm{PU}) }.
\end{align}
While the performance of JVF is sensitive to the pileup since $\sum\pt(\mathrm{PU})$ scales linearly according to number of pileup, $\sum\pt(\mathrm{PU})$ is devided by the number of PU tracks $n_{\mathrm{trk}}^{\mathrm{PU}}$ in the corrJVF to kill the linear dependency, together with the scale factor $\kappa=0.01$ conserving the absolute normalization of the PU term.
$R_{\pt}$ is the charged energy fraction in the jet, design to address to the jets with small number of tracks leading to low corrJVF value.
The 2D-likelihood profile is calulated for the HS jets and pile-up respectively, and the JVT is defined as ratio.

Fig. \ref{fig::objDef::JVTdist} shows the typical separation.
The JVT selection JVT$>0.57$ is applied for jets with $20 \gev < \pt< 60\gev$ and $|\eta|<2.4$, in which the pile-up jets dominately populates.  \\


% performance, (eff, rej)
%

\fig[160]{ObjectDef/JVTdist_155.pdf}
{Left two plot display the disctribution of input variables for JVT; corrJVF and $R_\phi$. corrJVF$=-1$ represents the jets with no associated tracks. The right plot is resultant output likelihood score,  JVT  \cite{155_JVT}.
}
{fig::objDef::JVTdist}

%\fig[80]{ObjectDef/JVTeff_155.pdf}
%{JVTeff. \cite{155_JVT}}
%{fig::objDef::JVTeff}



%%%%%%%%%%
%\begin{figure}[h]
%  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/.pdf}}
%    \caption{ (a)  (b). 
%      \label{fig::objDef::} }
%\end{figure}
%%%%%%%%%%



%%%%%%%%%
\subsection{Overlap Removal between Reconstructed Objects} \label{sec::objDef::OR}
Electrons, muons and jets are reconstructed in parallel, allowing the ambiguity that an identical particle is reconstructed or identified as multiple types of particles simultaneously. 
For instance, electrons are typically reconstructed either as electrons and jets. 
This is designed to provide flexibility in the object definition to satisfy various needs by analyses.  \\

A sequence of ``overlap-removal'' procedure is applied to resolve the ambiguity and avoid the double-counting, based on the angular distance $\Delta R = \sqrt{\eta^2+\phi^2}$ between them.  \\

The algorithm begins with the electron-jet overlap removal.
Any light-flavor jet \footnote{defined as reconstructed jets with MV2c10<0.1758} 
reconstructed within $\Delta R < 0.2$ with respect to identified electrons is rejected.
The electron is otherwise removed if the overlaping jet is b-tagged jet, 
to avoid rejecting b-jets due to the non-promt lepton nearby caused by the decays of b-hadrons. Next, to remove bremsstrahlung from muons followed by a photon conversion into electron pairs, electrons lying within $\Delta R < 0.01$ of a preselected muon are discarded.  \\

Subsequently, the contamination of muons from heavy-flavored hadron decays is suppressed by removing muons that lie within $\Delta R < \min(0.04 + (10 \gev)/\pt, 0.4)$ of any remaining jet, or within $\Delta R < 0.2$ of a b-tagged jet or a jet containing more than three tracks with $\pt > 500 \mev$.
In the former case, the $\pt$-decreasing angular separation mitigates the rejection of energetic muons close to jets in boosted event topologies. Finally, jets reconstructed within $\Delta R < 0.2$ of remaining electrons or muons are excluded. \\

The identification of hadronically decaying taus and photons are not exploited in the analysis, since they are not explicitly used as objects in event selections. Instead, those with sufficiently high transverse momentum pass the jet reconsruction as well as the JVT requirement, thus treated as jets in the analysis. \\
%%% table?


%%%%%%%%%
\subsection{Fake Leptons and the Isolation Requirement} \label{sec::objDef::fakeAndIsolation}
Light flavor leptons (electrons or muons) produced in LHC subject to two types; ``prompt leptons'' directly originated from the hard scattering via decays of real and virtual gauge bosons; ``non-prompt leptons'' generated via decays of heavy flavor hadrons (contains $b$ or $c$ quarks) and tau leptons, or pair creation of photons (mostly stemming from $\pi_0$ in jets). The leptons interested in the new phyics or EW physics always always refer to the prompt leptons, while non-prompt leptons are trivial and disturbing, degrading the use of leptons in the analysis. There are also a type of reconstructed leptons by wrongly identified pions from jets. In the thesis, these unwilling kinds of leptons (non-prompt leptons and pions) are simply referred as ``fake lepton'', and suppressed by employing the extra requirement described as follows.

\paragraph{Impact parameter requirement}
Non-prompt leptons are genearated in relatively displaced position with respect to the primary vertex. Therefore, the information of transverse impact parameters address a nice discriminating power. The selection used in the analysis is as Tab. \ref{tab::ObjDef::impactPar}.
While the $d_0$ and $|z_0 \sin{\theta}|$ of promt-leptons populate close to 0, those fornon-prompt leptons result in wide distributions, leading many of them to be rejected.

\tab{c|c|c}
{
\hline
                                     &   Electron &   Muon \\
\hline
\hline
$|d_0/\sigma_{d_0}|$                 &   $<5$       &   $<3$   \\
$|z_0 \sin{\theta}|$   &   $<0.5$ mm     &   $<0.3$ mm  \\
\hline
}
{
Summary of impact parameter requirements used in the analysis. 
$d_0$ and $z_0$ is the transverse and longitudinal impact parameter respectively.
$\sigma_{d_0}$ is the defined by the error matrix of the track fit
}
{tab::ObjDef::impactPar}

%%% fig?


\paragraph{Isolation}
While the path of flight of prompt-leptons rarely overlap with other particles, fake leptons generally fly closely by jets for their origin. Relatively higher jet activity around fake leptons is expected, therefore the isolaton requirement with respect to proximate cluster or tracks provide significant rejecting power of fake leptons. \\

Two isolation variables are defined:

\begin{description}
\item {\textbf{Calorimeter isolation} ($\etcone$)}: Sum of transverse energies by the calibrated topo-clusters 
with  $\Delta R<0.2$ with respect to the lepton. An $\et,\eta$ dependent pileup correction is applied. For electron, the energy leakage due to the bremstralung is compensated. 

\item {\textbf{Track isolation} ($\ptcone$)}: Sum of transverse momentum of tracks within the angular distance of $R=\min(0.2, 10\gev/\pt)$ with respect to the lepton. The variable cone size is intended to loosen the isolation cut for high $\pt$ leptons, based on the fact that most of fake leptons are below $20\gev$.
\end{description}

The isolation requirement is done by applying a cut in a 2D-plane of $\etcone$ and $\ptcone$. 
In the analysis, the \texttt{GradientLoose} working point is chosen, in which a $\pt$-dependent cut is applied designed to recover the efficiency in high $\pt$. Fig. \ref{fig::objDef::isoEff} shows the isolation efficiency respectively for electrons and muons. \\

%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/electronIsoEffFixedCutLoose_pt_156.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/muonIsoEffGradLoose_166.pdf}}
    \caption{ Measured and expected efficiency for isolation requirement for (a) electrons\cite{156_ElectronEffMeas_2015data} and (b) muons \cite{166_muonPerformance2015data}, both using the $Z\ra ee/\mu\mu$ events.
      The \texttt{FixedCutLoose} working point is shown for the electrons where $\etcone/\ET<0.2$ and $\ptcone/\ET$<0.15 is applied.
      \label{fig::objDef::isoEff} }
\end{figure}
%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Missing Transverse Energy} \label{sec::objDef::met}
Missing Transverse Energy ($\met$) is an extremely important proxy to new physics since it is contains the kinematical information of invisible particle. $\met$ is calculated by the transverse momentum imbalace of visible particles, using the reconstructed objects as well as isolated tracks that does not associated to any reconstructed objects referred as the soft term.
It is contructed by four independent terms as shown in Eq. \ref{eq::metDef}:
\begin{align}
\bm{E_\mathrm{T}^{\mathrm{miss.}}} :=  - \sum \bmet^{e} - \sum \bmet^{\mu} - \sum \bmet^{\mathrm{jet}} - \bmet^{\mathrm{soft}}.
%\met_{x,y} :=  - \sum \ET^{e}_{x,y} - \sum \ET^{\mu}_{x,y} - \sum \ET^{\mathrm{jet}}_{x,y} - \sum \ET^{\mathrm{soft}}_{x,y}.
\label{eq::metDef}
\end{align}
Input reconstructed objects for the first three terms in \ref{eq::metDef} are fully calibrated and the ambiguity between them is resolved by the overlap removal. 
%The overlap removal procedure used for $\met$ calcualtion slightly differes from the baseline method described in the previous sub-section in that  (要確認)
Jets with $\pt>20\gev$ are included in the jet term in the MET calculation, otherwise subjected to the soft term with the track momenta.
%unique fraction
Jets failed in the JVT selection is totally excluded from the MET calculation to prevent the contribution from pile-up. \\

The track soft term $\bmet^{\mathrm{soft}}$ (TST) \cite{175_MET_Run2_exp} accounts for the residual visible momentum mainly from soft jets and unidentified muons.
It is constructed by the tracks that are not associated to any jet, and are isolated by $\Delta R>0.2$ from any reconstructed EM clusters. The momenta of tracks found to associated with reconstructed muons are replaced into that by the combined ID+MS muon tracks. 

Tracks has its track momentum uncertainties larger than $40\%$, and high-$\pt$ tracks ($\pt>200 \gev$ in $|\eta|<1.5$ or $\pt>150 \gev$ in $|\eta|>1.5$) with questionable quality of monemtum measurement satisfying following conditions are removed to prevent potential large error in the calculation:
\begin{align}
\ptcone/\pt>0.1, \,\,\, \mathrm{and} \,\,\, \frac{\etcone}{\pt+\ptcone}<0.6, \,\,\,  \mathrm{and} \,\,\, \frac{\ptcone}{\pt+\ptcone}<0.6
\end{align}
% isolation variable defined in previous sub-section


%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/metReso_177.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/ObjectDef/metUnct_177.pdf}}
    \caption{ (a) Pile-up dependency of reslution on the met soft term, and (b) the absolute resolution,
      simulated or measured using $Z\ra \ell\ell$ events in which the soft terms is zero with ideal measurement \cite{177_MET_data2016}. 
      \label{fig::objDef::metPerformance}
 }
\end{figure}
%%%%%%%%%%



%%%%%%%%%
\subsection{Object Definition in the Analysis} \label{sec::objDef::objDef}
The requirements for objects used in the analysis is summarized in Tab. \ref{tab::objDef::summary}.
For electrons and muons, two types of working point are defined;
``baseline'' is the loose selection criteria oriented to veto extra prompt leptons in the event; 
``signal'' is the tighter wokring point aiming to reject fake leptons where the impact parameter cut, isolation requirement and tighter identification are imposed in on top of the baseline requirement.
Signal regions are defined with exactly one baseline and signal lepton, given that the targted singal events contain exactly one prompt lepton. Jet used in the analysis is uniquely defined. JVT cut is required to avoid the impact by pile-up on the analysis. \\
%% OR通ってないjet使うこともできればいう


\begin{table}[hpt]
\caption{Summary of all baseline and signal object selection. 
In addition to the listed criteria, objects are required to pass the reconstruction, identification and overlap removal.
The $\pt$-threshold is based on the transverse momentum after calibration.
}
\centering
\begin{tabular}{l|l|l}
  \toprule
  \hline
  \textbf{Electrons}	& Baseline			& Signal \\
  \hline
  $p_{T}$		& $p_{T}>7\gev$	                & $p_{T}>10\gev$ \\
  Identification        & Loose \footnote{See Sec. \ref{sec::objDef::electrons::id}} 
        		& Tight \footnote{See Sec. \ref{sec::objDef::electrons::id}} \\
  Isolation		& -				& \texttt{GradientLoose} \\
  Impact parameter cuts & -				& $z_0 < 0.5 \mathrm{mm}$, $|d_0|/\sigma(d_0)<5$ \\

  \hline
  \textbf{Muons}	& Baseline			& Signal \\
  \hline
  $p_{T}$		& $p_{T}>6\gev$	                & $p_{T}>10\gev$ \\
  Identification	& Medium \footnote{See Sec. \ref{sec::objDef::muon::id}} 
                        & Medium \footnote{See Sec. \ref{sec::objDef::muons::id}}  \\
  Isolation		& -				& \texttt{GradientLoose} \\
  Impact parameter cuts & -				& $z_0 < 0.5\mathrm{mm}$, $|d_0|/\sigma(d_0)<3$ \\
  \hline
  \multicolumn{3}{l}{\textbf{Jets}} \\
  \hline
  Clustering Algorithm  & \multicolumn{2}{l}{\Antikt ($\Delta r=0.4$)}  \\
  $p_{T}$		& \multicolumn{2}{l}{$p_{T}>30\gev$}  \\
    JVT			& \multicolumn{2}{l}{JVT$>0.57$} 	 \\
  $b$-tag	        & \multicolumn{2}{l}{MV2c10 77\% efficiency working point} \\
  \hline
\end{tabular}
\label{tab::objDef::summary}
\end{table}

\section{Monte-Carlo Simulation}
Monte-Carlo (MC) simulation is a highly powerful toolkit providing theoretical prediction on particle generation and expected evet kinematics, as well as detector response. The simulated event samples are used extensively from studying signal/background separation, performance evaludation to background estimation. \\

The MC event generation is based on the differential cross-section in terms of 4-momentum space of outcome particles (``phase space'') provided by theory. Randomly generated events over the phase space are accepted on a rate proportional to the differential cross-section in the phase space point. \\

In this section, an overview is given for the detail of the implemetation including the phenomenological description of particle interactions in LHC, cross-section calculation and other kinematical effects in event generation (widely referred from \cite{ATLAS_generator}\cite{SkandsQCD}), and the summary of the simulated samples used in the analysis.  \\


\subsection{Phenomenology in a $pp$-collision}
The processes involved in a $pp$-collision are schematized in Fig. \ref{fig::Samples::ppCollisionScetch}.
%%%
\fig[150]{Samples/ppCollisionScetch.pdf}
{Schematic of involved phenomenology in a $pp$-collision.}
{fig::Samples::ppCollisionScetch}
%%%

Thanks to the nature of asymptotic freedom of strong interaction, a proton-proton interaction can be fully characterized by a picture of parton-parton interaction at the LHC energy. 
The differenial cross-section describing transtion from an initial state with two partons ($a, b$) into a certain final state ($F$) is respresented by: 
\begin{align}
%\frac{d\sigma_{a,b \ra F}}{d\mathcal{O}} & = \int d\Phi \,\, |\mathcal{M}_{a,b \ra F}|^2 \,\, \delta \left( p^\mu_a + p^\mu_b - \sum_{i \in f} p^\mu_i   \right) 
\frac{d\hat{\sigma}_{a,b \ra F}}{d\bm{y}} & = \frac{1}{2 \hat{s}_{ab}} d\Phi \,\, |\mathcal{M}_{a,b \ra F}|^2  
\label{ppXsec}
\end{align}
where $\bm{y}$ respresents momenta of final state particles; $\mathcal{M}_{a,b \ra f}$ the matrix-element (ME); $d\Phi$ the phase space factor; and the flux factor $1/2\hat{s}_{ab}$.
% phase space factorについてもうちょい

The cross-section Eq. \ref{ppXsec} is then capsulated by the Parton distribution function (PDF) to translate from the parton-level cross-section to that of $pp$-interaction:
\begin{align}
\frac{\mathrm{d}\sigma_{pp\ra F}}{\mathrm{d}\bm{y}} & = \sum_{a,b\in (q,\bar{q},g)} \int_0^1 \mathrm{d}x_a \int_0^1 \mathrm{d}x_b \,\, f_i (x_a) f_j (x_b) \,\, \frac{d\hat{\sigma}_{a,b \ra F}}{d\bm{y}}.
\label{ppXsecWithPDF}
\end{align}
$x_{a,b}$ denotes the momentum fraction of protons carried by the constituent partons $a,b$, and $f_i(x)$ is the proton PDF function defined by the propability density which $x$ obeies. $a$ and $b$ are finally added up with possible parton flavors, reflecting our ignorance about the initial parton flavors.
Note that this convolution is not the addition at the amplitude level in $\mathrm{M}$ but rather a statistical procedure, which is guaranteed by the factorization theorem. \\

Resultant quarks and gluons in the final state undergo hadronization, in which they are transformed into a collection of fragmented hadrons (``hadron jet''). This is particular the nature about strong interation known as ``confinement'' where the running coupling constant becomes larger for longer distance scattering and eventually diverges at the Laudau pole $Q^2\sim 200\mev$. Naively this will lead to infinite purtubative amplitudes of processes with $Q^2\sim 200\mev$, including small angle diffraction and pair production of quark and anti-quark out of vacuum. 
\footnote{This is picture is incorrect giving the breakdown of perturbation, neverless enough to give an idea of the universal tendency toward non-perturbative region.}
Those instantaneously generated partons are recombined eventually into hadrons with singlet color quantum number. Theoretically, hadronization can be characterized using the an universal frametation function $D(z)$ in the same internal structure of parton distribution function, representing the probability of finding a hadron with momentum fraction of z with respect to that of seed parton. \\

Addtionally, often additional jets acompany from splitting legs of intial and final state partons. 
They are referred as initial state radiation (ISR) or final state radiation (FSR). \\

Finally, the protons providing the hard scattering partons are completely destroyed, no longer keeping the form as protons.
The remnants will experience they own hadronization, resulting in a splash of permeating hadrons known as ``beam remnant''.
Also, multiple parton-level hard scatterings (multiple parton interaction; MPI) occasionally take place within a single proton-proton interaction, where usually at least one of them ends up in cheap QCD scattering leaving low-$\pt$ jets.
These sub-processes resulting soft renmants as the background of main HS are incusively referred to ``underlying events''.

%The underlying event is a hadronic activity from a collision between partons of colliding hadrons
%not contribute to a hard process. The underlying event includes effects from the hadronisation of
%beam remnants and multiple 2 ! 2 parton interactions. 

%quarkとgluonはQCDのcharacteristical negative beta functionによってanti-screeningが起こって


\subsubsection{Parton Distribution Function}
As PDF is purely determined by non-purtabative dynamic of QCD at lower scale.
As the input for event simulation, it is usually taken from global fitting on experimental data of deep inelastic scaterings (DIS) or hadron-hadron collision.
\footnote{The first principle calculation is strictly speaking doable by lattice QCD. Some results are presented by \cite{latticePDF}.}
Several collaborations have performed combined fits to the datasets mostly from HERA and Tevatron, with different paramerization and fitting scheme. 
The following sets recently provide results: PDF4LHC \cite{PDF4LHC}, NNPDF \cite{NNPDF}, CT14 \cite{CT14}, MSTW \cite{MSTW} and AZNLO \cite{AZNLO}. 
The uncertainties mainly results from instrumental uncertainties in the input data, uncertainties on the strong coupling constant and the functional form of parameterization. 
%Fig. \ref{} shows ...


\subsubsection{Fixed-Order QCD Calculation}
The matrix element in Eq. \ref{ppXsec} is computed based on the QCD and EW theory, with truncated orders of purtabation.
While the leading term in the purtabation (lowest order; ``LO'') dominates over the phase space, 
the inclusion of high-order terms is significantly important for new physics search.
The is because of the much smaller cross-section of signal procutions with respect to the SM processes,
forcing one to explore the phase space where the bulk SM component is suppressed, 
in order words the region where the LO contribution is largely suppressed and the higher-order effects addresses,
to achive a reasonable S/N. \\

The full calculation of higher-order terms are challenging givining the skyrocket increased number of loop diagrams.
Currently, the cross-section calculation is available upto next-to-next-to-leading order (NNLO) or NNNLO for typical SM processes happening in LHC, and upto NLO level in event generation. As the largest contribution of the higher order effects are from ISR and FSR, there are a class of geneartors in the market particlualry focusing on computing the diagrams with the additional radiations (``multi-leg generators''). Saving the computing resources by omitting the loop diagrams, they can typically afford upto 4-9 additional partons at maximum. \\


\subsubsection{Parton Showering}
Aside with the straightforward  QCD matrix element calculation,
the parton shower (PS) regime is another useful approach of describing dynamtic of addtional partons emission.
It is based on following two important notions:

\begin{itemize}
\item Soft or collinear emission provide dominant contribution to extra parton emission from a parton.
In a parton-level process: $ee \ra q\bar{q} g$ for a minimal example, the differential cross-section can be expressed:
\begin{align}
\frac{d\sigma_{q\bar{q}g}}{dx_1 dx_2} = \sigma_{q\bar{q}} \,\, \times \,\, \frac{\alpha_s}{2\pi} \frac{4}{3} \frac{x_q^2+x_{\bar{q}}^2}{(1-x_q)(1-x_{\bar{q}})}, \,\,\,\, x_i := 2E_i/\sqrt{s}
\end{align}
with $\sqrt{s}$ being the center-of-mass energy of the $ee$ system.
The singularities correspond to collinear emission of the gluon ($x_q \ra 1$ or $x_{\bar{q}} \ra 1$) or soft gluon emission ($x_q \ra 1$ and $x_{\bar{q}} \ra 1$).
These collinear and soft singularlities are universal to QCD, independent from type of processes. \\

\item In the soft/collinear regime, the cross-section with an additional parton radiation ($\mathrm{d}\sigma_{n+1}$) can be factorized by a product of the original cross-section ($\mathrm{d}\sigma_{n}$) and splitting factor $P_{i \ra j k}$:
\begin{align}
\mathrm{d}\sigma_{n+1} = \mathrm{d}\sigma_n \left[ \sum_{j,k} \,  \frac{\alpha_s}{2\pi} \, \frac{dq}{q} \, \frac{dz}{z} P_{i \ra j k} (z) \right],
\end{align}
where the indices $i,j$ represent respectively the parent parton before and after the splitting, and $k$ the emitted parton. 
$z$ is the momentum fraction the emitted parton carrying from the parent, and $q$ is the momentum transfer between the parton $i$ and $j$. $P_{i \ra j k}$ is known as the Dokshitzer–Gribov–Lipatov–Altarelli–Parisi (DGLAP) evolution equations \cite{D_DEGLAP}\cite{L_DEGLAP}\cite{AP_DEGLAP} computed from generic QCD analyses as:
\begin{align}
P_{q \ra qg} & = \frac{4}{3} \frac{1+z^2}{1-z}, \\
P_{q \ra gq} & = \frac{4}{3} \frac{1+(1-z)^2}{z}, \\
P_{q \ra gg} & = 3 \frac{z^4+1+(1-z)^4}{z(1-z)}, \\
P_{q \ra q\bar{q}} & = \frac{z^2+(1-z)^2}{2},
\end{align}
\end{itemize}

Based on these, the PS regimes allows one to recursive calulate in a picture of stepwise evolution, in contrast to that in the scattering amplitude approach where either initial and final state must be steadily defined beforehand. The probablity of emitting an extra parton at each step can be then represented, anagolous to the life time of instable particle decay, using the Sudakov form factors \cite{Sudakov}:
\begin{align}
S_{i}(q_1, q_2) = 1 - \exp{ \left( -\sum_{j,k} \int_{q^2}^{q_{\mathrm{max}}^2} \frac{\mathrm{d}Q^2}{Q^2} \int_{z_{\min}}^{z_{\max}} \frac{\alpha_s}{2\pi} P_{i \ra j k}(\hat{z}) \mathrm{d}\hat{z}  \right)},
\label{eq::sudakov}
\end{align}
where $q_1$ and $q_2$ denote the virtuality of parent parton between and after splitting respectively.
%sudakov fac. どう計算されたか
The FSR in event simulation is implemented by stochastic evolultion of final state parton legs on the probability Eq. \ref{eq::sudakov}, with givning an arbitrary initial virtuality $Q$. The evoluton is terminated typically until the virtuality becomes $\sim 1\gev$. ISRs are simulated in similar manner but with backward evolution with increasing virtuality $q$ along the evolution.
Generated sub-branches during the backward evolution are then evolved forward. The procedure is schematized as Fig. \ref{fig::Samples::backwardEvolution}.

%%%
\fig[110]{Samples/backwardEvolution.pdf}
{Schematic of the backward evolution implemented in ISR simulation. The evolution starts from hard-collided parton with increasing virtuality $q$ along the evolution. Partons splitted from it are then evolved forward.}
{fig::Samples::backwardEvolution}
%%%

Vaious implementation for the evolution exist, leading to a subtle difference in the final state kinamtics.
The impact of the difference are quoted for theoretical uncertainty in the analysis. The qualification and assigned uncertainty will be discussed Sec. \ref{sec::Uncertainty}. \\

%It also particularly goes along with MC simulation since it does not require any try and acceptance so that no seed events will be discarded, enabling a highly computationally efficient generation. \\

Note that this shower evolution is fully pertubative yet include contribution from essentially arbitary orders of purtabation series (upto $n$-th order, where $n$ is number of parton branch splitting), however it does only take care of contribution from collinear and soft singularity. This is the main motivation for multi-leg generators that provides hard ME-level addtional partons to complement. One issue about this combined approach is the potential double-counting. The correction procedure commonly refers to ``matching'' or ``merging'', and are largely categoriezd into two types: separating the regions that each ME and PS is responsible for, in terms of phase space or scale. The most widely used algorithm is provided by Catani-Krauss-Kuhn-Webber (CKKW) \cite{CKKW_orig} algoriths or Michelangelo L. Mangano algoriths (MLM) \cite{MLM}. ; Generating all jets by PS, and correct it by normalizing into the ME differential cross-section (ME correction).
\\

%\begin{itemize}
%\item 
% The CKKW ...
%MLM ..

%\item 
%This type of scheme is employed by \pythia and .
%\end{itemize}


\subsubsection{Hadronization}
%(jav need change)
A phenomenolocal approach is usually preferred in simulating hadronization, although the it can be formulated through an universal fragmentation function that has the same internal structure as parton distribution. 
The most famous model is the string model \cite{LundStringModel} where the confinement between partons is represented by a gluonic string. 
For a quark-antiquark pair, as the partons move apart, the string is stretched leading to an increase in potential energy. 
When the energy becomes of the order of hadron masses, it becomes energetically favorable for the string to break and create a new quark-antiquark pair. 
The two segments of string will be repeatedly pulled and break again, until all energy of initial quarks is converted into newly generated fragments.
%Fragmentation function $D(z)$ is used to formulate the behavior ... \\


%\subsubsection{Underlying Events}
%A good understanding of underlying events is important from intrumental point of view, since it affects the calibrations of tracks %and objects. Several models and parameter tunes are available and are examined by data. Fig. \ref{}.
% A14 ~\cite{ATL-PHYS-PUB-2014-021} for pythia 8
% P2012\cite{Skands:2010ak}  for  \pythia6.428~\cite{Sjostrand:2006za}


\clearpage
\subsection{Event Generators and the Simulated Dataset}
%The generators used in the analysis are as below:
%\begin{itemize}
%\item \sherpa
%\item \powhegbox
%\item \mgmc
%\end{itemize}

Signal and background event are generated using preferred generators and setups. 
Tab. \ref{tab::Samples::generators} summarizes the configurations for datasets used in the analysis.
Given that the analysis typically explores the phase space with many jets, simulation on physics processes yielding less jets (e.g. $\wzjets$) or only soft jets at tree level (e.g. gluino production with compressed mass spectra) need dedicated modeling of ISR and FSR, therefore the multi-leg generators (\sherpa, \mgmc) are preferred in general. 
% powhegはなんでpoheg boxがいいってことになってんの？

%\tabsmall{c|cccc}{
\tab{c|cccc}{
  \hline
  Physics process   &  Generator    &  $n_{\mathrm{ME}}^{\mathrm{a.p.}}$                      &  PDF set       & PS/UE  \\
  \hline
  \hline
  SUSY processes    &  \mgmc 2.3.3 \cite{Alwall:2014hca} (LO)                                &  2                   &  NNPDF2.3 LO   & \pythiaeight \cite{Sjostrand:2007gs}  \\
  $\wzjets$         &  \sherpatwo \cite{Gleisberg:2008ta} (NLO)                               &  2(NLO), 4(LO)       &  NNPDF3.0 NNLO\cite{Ball:2014uwa}  & \sherpa        \\
  $\ttbar$          &  \powhegbox\mbox{\phantom{k}}v2 \cite{Alioli:2010xd} (NLO)  &  1         &  CT10 \cite{Lai:2010vv}          & \pythiasix \cite{Sjostrand:2006za}    \\
  Single-top ($Wt$-ch.) &  \powhegbox\mbox{\phantom{k}}v2 (NLO)    &  1                       &  CT10          & \pythiasix      \\
  Single-top ($s$-ch.)  &  \powhegbox\mbox{\phantom{k}}v2 (NLO)    &  1                       &  CT10          & \pythiasix      \\
  Single-top ($t$-ch.)  &  \powhegbox\mbox{\phantom{k}}v1 (LO)     &  1                       &  CT10f4        & \pythiasix      \\
%  Single-top ($Zt$-ch.) &  \mgmc 2.2.1 (LO)                        &  1                       &  CTEQ6L1\cite{Pumplin:2002vw}   & \pythiasix   \\
  Di-bosons              &  \sherpa 2.1.1  (LO)                    &  3-4                      &  CT10          & \sherpa        \\
  $\ttbar+W$   &  \mgmc 2.2.3  (LO)                               &  2                        &  NNPDF2.3 LO   & \pythiaeight    \\
  $\ttbar+Z$   &  \mgmc 2.2.3  (LO)                               &  1                        &  NNPDF2.3 LO   & \pythiaeight    \\
  $\ttbar+WW$   &  \mgmc 2.2.3  (LO)                              &  0                        &  NNPDF2.3 LO   & \pythiaeight   \\
  \hline
}
{Setup of simulated SUSY signal and the Standard Model background samples. 
$n_{\mathrm{ME}}^{\mathrm{a.p.}}$ is the number of simulated addtional partons in the higher order QCD processes.
PS and UE are the arrebration of parton shower and underlying events respectively. }
{tab::Samples::generators}


% A14 ~\cite{ATL-PHYS-PUB-2014-021} for pythia 8
% P2012\cite{Skands:2010ak}  for  \pythia6.428~\cite{Sjostrand:2006za}
%
% pythia6 -> ME correction
% pythia8 -> CKKW-L
% Sherpa  -> CKKW
%

The simulated samples are normalized by cross-section independenly calculated typically with higher orders and soft gluon resummation upto Next-to-Leading Logarithm (NLL) or Next-to-next-to-Leading Logarithm (NNLL). Tab. \ref{tab::Samples::xsec} shows the summary.

\tab{c|ccc}{
  \hline
  Physics process   &  Cross-section $[\mathrm{pb}]$   &  Order  & Authors \\
  \hline
  \hline
  SUSY processes    & See Fig. \ref{fig::Samples::xsec_GG} &   NLO+NLL   & \cite{Beenakker:1996ch,Kulesza:2008jb,Kulesza:2009kq,Beenakker:2009ha,Beenakker:2011fu} \\
  $\wjets (\ra \ell\nu)$    & 20079                        &   NNLO      & \cite{Gavin:2010az}  \\
  $\zjets (\ra \ell\ell)$   & 1950                         &   NNLO      & \cite{Gavin:2010az}  \\
  $\ttbar$                  & 993.8                        &   NNLO+NNLL & \cite{topxs:2014} \\
  Single-top ($Wt$-channel) & 75.57                        &   NNLO+NNLL & \cite{wtxs:2014oha}  \\
  Single-top ($s$-channel)  & 10.32                        &   NLO       & \cite{Kant:2014oha} \\
  Single-top ($t$-channel)  & 216.95                       &   NLO       & \cite{Kant:2014oha} \\
%  Single-top ($Zt$-channel) & 0.24                         &   LO        & \mgmc 2.2.1   \\
  Di-bosons                  & 45.42                        &   NLO       & \cite{ATL-PHYS-PUB-2016-002}\\
  $\ttbar+W/Z/WW$           & 1.36                         &   NLO       & \cite{Lazopoulos:2008,Campbell:2012}\\
  \hline
}
{Cross-section used for the simulated processes.}
{tab::Samples::xsec}



% -------------- xsec_GG
\begin{figure}
  \begin{center}
    \includegraphics[width=140mm]{figures/Samples/xsec_GG/xsec_GG.pdf}
    \captionof{figure}{Cross-section for gluino pair production. Calculation is performed at NNLO+NNLL accuracy.}
    \label{fig::Samples::xsec_GG}
  \end{center}
\end{figure}
%-------------------------------     


\noindent Further caveats particular to each process are stated as below.
% ($W$/$Z$+jets)~\cite{ATL-PHYS-PUB-2016-003}

\paragraph{$W/Z$+jets} 
%Events containing $W$ or $Z$ bosons with associated jets are simulated using the \sherpa2.2.1 generator~\cite{Gleisberg:2008ta}
The matrix elements are calculated using the \sherpa2.2.1 generator~\cite{Gleisberg:2008ta} up to two partons at NLO and four partons at LO\@ using the Comix~\cite{Gleisberg:2008fv} and OpenLoops~\cite{Cascioli:2011va} generators. 
They are merged with the \sherpa2.2.1 parton shower~\cite{Schumann:2007mg} with massive $b$ and $c$ quarks using the ME+PS@NLO prescription~\cite{Hoeche:2012yf}.
The CKKW scheme is used for ME/PS matching with matching scale set to $30\gev$.


% ttbar, 1top  $Wt$ and $s$-channel~\cite{ATL-PHYS-PUB-2016-004},
\paragraph{Tops: $\ttbar$/single-top}
The EvtGen v1.2.0 program is used to describe the properties of the bottom and charm hadron decays in the \ttbar\ and the single-top-quark samples. 
The $h_{\rm damp}$ parameter, which controls the \pt\ of the first additional emission beyond the Born configuration, is set to the mass of the top-quark. 
The main effect of this is to regulate the high \pt\ emission against which the \ttbar\ system recoils. The top-quark mass is set to $172.5 \gev$ for all the samples. 

\paragraph{Di-bosons: WW/WZ/ZZ}
The fully-leptonic processes are simulated with five final states ($\ell\ell\ell\ell$, $\ell\ell\ell\nu$, $\ell\ell\nu\nu$, $\ell\nu\nu\nu$, $\nu\nu\nu\nu$). The intermediated states are not specified therefore the contribution from Drell-Yan-like off-shell diboson and the interference between different diboson processes (e.g. $WW\ra\ell\ell\nu\nu$ and $WZ\ra\ell\ell\nu\nu$) are taken into account. The semi-leptonic diboson processes are simulated with designated intermediated boson states namely $W$ or $Z$.
%The processes are calculated for up to one parton (for $ZZ$) or no additional partons (for $WW, WZ$) at NLO and up to three partons at LO using the Comix and OpenLoops matrix-element generators.
The cross-section is calculated at NLO order for each diboson channel $WW,WZ,ZZ$ ~ to which the samples are normalized to.


%For the $t\bar{t}+W/Z/WW$ processes~\cite{ATL-PHYS-PUB-2016-005}
\paragraph{${\bf \ttbar+W/Z/WW}$}
All processes are simulated by \mgmc 2.2.3 at LO interfaced to the \pythia 8.186 parton shower model, 
with up to two ($t\bar{t}+W$), one ($t\bar{t}+Z$) or no ($t\bar{t}+WW$) extra partons included in the matrix element. 
The EvtGen v1.2.0 program~\cite{EvtGen} is used to describe the properties of the bottom and charm hadron decays. 
The ATLAS shower and underlying-event tune A14 is used together with the NNPDF2.3~LO PDF set. 
The events are normalised to their NLO cross-sections~.


\paragraph{SUSY signals} \label{sec::Samples::SUSY}
The EvtGen v1.2.0 program~\cite{EvtGen} is used to correct the description of the bottom and charm hadron decays. 
Decay of EW gauginos are done in \pythia , based on phase space with no consideration of the spin.
The CKKW-L matching scheme~\cite{CKKW} is applied for the matching of the matrix element and the parton shower, with the correspoding scale parameter set to 1/4 of the gluino mass. 
The cross-section uncertainty are taken from an envelope of cross-section predictions using different PDF sets and factorisation and renormalisation scales, as described in Ref.~\cite{Borschensky:2014cia}, considering only the four light-flavor left-handed squarks ($\tilde{u}_L$, $\tilde{d}_L$, $\tilde{s}_L$, and $\tilde{c}_L$). Fig. \ref{fig::Samples::xsec_GG} shows the calculated cross-section and the associated error. \\

Model parameters irrelevant to SUSY masses are fixed to arbitrary reasonable values, 
since here we assume they do not change the kinematics as disscussed in Sec. \ref{sec::Introcution}.
The mixing parameters are set so that LSP and NLSP are bino- and wino-like. \\



\subsubsection{Pileup simulation}
All simulated events are generated with a varying number of minimum-bias interactions overlaid on the hard-scattering event to model the multiple proton-proton interactions in the same and the nearby bunch crossings.
The minimum-bias interactions are simulated with the soft QCD processes of \pythia8.186 using the A2 tune \cite{ATLAS:2012uec} and the MSTW2008LO PDF set~\cite{Martin:2009iq}. 
Corrections are applied to the samples to account for differences between data and simulation for trigger, identification and reconstruction efficiencies. \\



\subsubsection{Detector Simulation and Emulation}
The detector response to genearated particles is simulated by a full ATLAS detector simulation model \cite{ATLASFullSimu:2010wqa} based on \textsc{Geant4}~\cite{Agostinelli:2002hh}, for the background samples. \\

The ATLAS fast simulation ~\cite{atlfast} is used for signal models marked as $\checkmark$ in Tab. \ref{tab::Introduction::modelsBV}-\ref{tab::Introduction::models3B} in Sec. \ref{sec::Introduction::targetModels}, as the economical alternative. This is based on a parameterisation of the performance of the electromagnetic and hadronic calorimeters and on \textsc{Geant4} elsewhere. The difference between the full simulation is found to be marginal after examining a number of referece signal points.
The subquent procedures are identical to what is processed for the data sample: particle reconstruction and identifications, as described in Sec. \\

For the signal models with no $\checkmark$ in Tab. \ref{tab::Introduction::modelsBV}-\ref{tab::Introduction::models3B}, no detector simulation nor reconstruction is performed. Instead the effect is emulated by smearing the energy of truth-level particles and clustered jets, based on the resolution parametrized using the full simulated samples. The object identification is emulated by randomly accepting the candidates at the rate of the parametrized efficiency. The modeling is extensively tested by comparing the kinematic distributions with the fast simulated samples. The discrepancy is found sufficiently small, staying within 5$\%$ in general and never exceed $10\%$. \\

% benchmark models
% non-benchmark models -> no dector simulation, use emulation


\subsection{Design of SUSY Signal Grid for Interpretation} 
Obtained exlusion limits are presented in form of contours in a 2-dimensional plane, usually in terms of SUSY masses. This is done by genearting a set of signal samples with various SUSY masses covering the whole plane with discrete steps, referred as a signal grid, and the results of the hypothetical test for the points are interpolated, providing continuous limit in the end. \\

For limits on the direct decay models, $\mG$ and $\mLSP$ are chosen as x and y-axis respectively to represent (referred as ``\dire'' grid). The cases with 1-step decay models is a bit tricky, since they involve the third mass; the intermediate EW-gaugino $\chaone$ or $\neutwo$. The full 3-dimensional presentation is not realistic from computational cost of view, due to the enormously increased number of grid points to cover the whole grid. Therefore, a couple of sensible 2D-slices are made that sufficiently capture the essense of the 3D-grid. ``\xhalf'' is the grid with the intermediate EW-gaugino mass is set to midmost between gluino and the LSP, while $x$ is defined as a parameter representing the relative mass splitting:
\begin{align}
  x := \dmcn/\dmg, \,\,\,\,\,\,\, x \in [0,1]. \nn
\end{align}
The \varx grid is then designed to complement the hole in high or low $x$, where the LSP mass is fixed to $60\gev$ and the gluino mass and the intermediate EW-gaugino mass are set free. There are two additional grids \DMtw and \DMth in which the intermediate EW-gaugino an the LSP are compressed ($\dmcn=20, \, 30\gev$ respectively), respecting the dark matter relic constraint in discussed in Sec. \ref{sec::Introduction::DMconstraint}. Note that these DM grids are not considered in models with $\neutwo$ decaying to higgs, since higgs is too far off-shell thus $\neutwo$ never almost decays via higgs in the situation. \\
To summarize, four types of signal grid are designed in the analysis, as shown in Tab. \ref{tab::Samples::signalGridDef}. 


%\tabsmall{ c | c c c c}
\tab{ c | c c c c}
{
\hline
Grid name   & x-axis   & y-axis         & Slicing                & Note\\
\hline
\hline
\dire       & $\mG$   & $\mLSP$         &  -                     & - \\
\hline
\hline
\xhalf      &  $\mG$  & $\mLSP$         &  $\dmg = \dmcn$        & - \\
\varx       &  $\mG$  & $\dmcn / \dmg$  &  $\mLSP=60\gev$        & - \\
\DMtw,\DMth &  $\mG$  & $\mLSP$         &  $\dmcn=20,30\gev$     & For models without \\
            &         &                 &                        & $h$-mediated $\tilde{\chi}_2^0$ decays. \\
\hline
}
{List of signal grids used for limit setting. \dire is for the direct decay model, and the others are for the 1-step decay models. The latter is four-fold: \xhalf, a grid with EW gaugino mass fixed to the middle of gluino and LSP; \varx, in which LSP mass is fixed to 60$\gev$; \DMtw and \DMth are grids with $\dmcn=20,30\gev$ which are considered only in models without $\neutwo$ decay into higgs. }
{tab::Samples::signalGridDef}



\clearpage
\section{Event Selection} 

\subsection{Trigger Selection} \label{sec::Samples::trigger}
The missing ET trigger (MET trigger) is primarily used throughout the analysis.
Since the lowest unprescaled trigger kept evloved accroding to the increased instantaneous luminosity during the 2016 data taking, a number of different triggers are used in combination.
The list of the triggers are shown in Tab. \ref{tab::SRdefinition::METtriggers},

\tabsmall{ccc|cc}{

\hline
Period      &  Peak lumi. [cm$^{-2}$ s$^{-1}$]  &  Int. lumi. [fb$^{-1}$]  &   L1 (HLT) item       &  L1/HLT/Off-line threshold [GeV]  \\
\hline
2015        &  0.50 $\times 10^{34}$                                     &  3.19                         &   L1XE50 (xe70\_mht)   &  50 / 70 / 200  \\
2016 A-D1   &  0.99 $\times 10^{34}$                                     &  6.12                         &   L1XE50 (xe90\_mht)   &  50 / 90 / 200   \\
2016 D1-F1  &  1.03 $\times 10^{34}$                                      &  6.55                         &   L1XE50 (xe100\_mht)  &  50 / 100 / 200  \\
2016 F2-    &  1.21 $\times 10^{34}$                                      &  20.2                         &   L1XE50 (xe110\_mht)  &  50 / 110 /  200   \\
\hline
}
{Summary of MET triggers used in the analysis along the peak luminosity evolution. Corresponding on-line and off-line threshold are shown altogether.}
{tab::SRdefinition::METtriggers}

% needed description 
% off-line MET -> cluster based
% on/off-line definition
%Taking the advantage that most of the targeted signal models result in large missing ET (MET) due to the presecnce of LSPs, the lowest unprescaled MET trigger (HLT\_xe110\_mth\_XE50) is used for triggering events throughout the analysis. 

The efficiency curve as function of off-line $\met$ is shown in Fig. \ref{sec::SRdefinition::METtrig} with the example of HLT\_xe100\_mht.
Thanks to the fact that MET is calculated from global information of an event, rather than the feature of a single particular particle, the plateau efficiency amounts almost 100 $\%$. 
This is a significant advantage over the use of leptonic trigger where efficiency is typically $70\% \sim 90\%$. 
Generally the downside of MET trigger is on the other hand its slow turn-on in terms of the off-line MET that needs nearly $200 \gev$ to assure the plateau efficiency despite much lower trigger threshold ($<110$ GeV). This is due to the deteriorated resolution of on-line MET which is purely based on calorimeter clusters, with respect to the off-line one which is take into muons and soft tracks into account. The signal acceptance by the trigger requirement is  $>95\%$ except when gluino mass and LSP mass are compressed. Nevertheless, given that it is impossible for such signal to be discriminated against background without the MET generated by associated ISRs, the loss in trigger is not problematic. \\

%The turn-on also has dependency on event topology and kinematics due to varying MET resolution in particular sensitive to magnitude of jet activity, however this does not affect the plateau efficiency. Fig. \ref{fig::SRdefinition::metTrigEff} shows turn-ons with different jet multiplicity. \\

The single-lepton trigger (SLT) is also used for supplemental purpose, including the efficy measurement of MET trigger and closure tests of data-driven background estimation. The trigger turn-on is about $28\gev$ ($26\gev$) for single-electron (muon) in its transverse momentum and $30\gev$ ($28\gev$) is required as off-line threshold. \\


\fig[160]{SRdefinition/trigEff/metTrig_huajie.pdf}
{Turn-on of MET trigger HLT\_xe100\_mht simulated or measured using $\wjets$ events by performing the tag-and-probe techenique. (a) events with exactly one muon, and (b) events with exactly one electron.}
{sec::SRdefinition::METtrig}


% -------------- MET trigger eff.  
%\begin{figure}
%  \begin{center}
%    \includegraphics[width=100mm]{figures/SRdefinition/trigEff/metTrigEff_lepTrigTag.eps}
%    \captionof{figure}{Efficiency curve of MET trigger measured with data by tagging a lepton with $p_{T}(\ell_{1})>35\gev$ firing the single-lepton trigger.}
%    \label{fig::SRdefinition::metTrigEff}
%  \end{center}
%\end{figure}






\subsection{Event Cleaning and the Pre-selection} \label{sec::SRdefinition::eventCleaning}
Event cleaning is applied to get rid of funky data events that are either in bad quality due to inappropriate detector status and badly measured objects, or with objects stemming from somewhere other than the hard collision such as cosmic muons and beam-induced background. 
As those events could result in extraordinary observables, for instance extremely high jet pt or MET, they are generally critical for search analyses probing the high-end of kinematics where only a few background events in signal regions are in discussion where therefore even a single event of the accidental contamintion makes huge impact on the final result. The list of procedure and cut efficiencies are summarized in Tab. \ref{tab::SRdefinition::EvtCleaning}. 

\tab{ c | c | c }{
  \hline
                       Cut              &  Efficiency (Data) $[\%]$   &  Efficiency (MC, $\ttbar$) $[\%]$  \\
  \hline
  \hline
  Veto bad lumi-clocks                  & 95.12                            &  100.0        \\
  Veto bad DAQ events                   & 99.81                            &  100.0        \\
  Veto events with no primary vertex    & 100.0                            &  100.0        \\
  Veto events with cosmic muons         & 95.83                            &  98.52        \\
  Veto events with badly measured jets  & 99.49                            &  99.65        \\
  Veto events with badly measured muons & 99.99                            &  98.56        \\              
  \hline
}
{List of cuts applied as event cleaning. Data and MC shows different efficiencies up-to the top four since MC does not emulates bad data quality and cosmic muons in it.}
{tab::SRdefinition::EvtCleaning}

Lumi-blocks with more than $10\%$ of the detector in the bad status are firstly removed. 
Events affected by noise bursts in LAr and SCT,  corrupted data transmission in LAr and the Tile calorimeter are then vetoed subsequently. 

%Event cleaning for badly measured objects applied rather conservatively giving that they could easily generate large fake MET that makes itself highly signal-like.
Cosmic muon are vetoed by requiring the muon track passing reasonably close-by the primary vertex i.e.
$$
|z_0| < 1\, \mathrm{mm}, \,\,\, d_0<0.2\, \mathrm{mm}.
$$
The beam induced backgrounds are events with muons that are generated by the secondary cascades of protons traveling upstream of the interaction point. The energy depositions created by these muons can be reconstructed as jets with energy as high as the beam energy therefore becomes highly signal-like. To reject the fake jets, event with jets flagged as ``BadLoose'' described in \cite{BadJetCriteriaATLAS2015} are vetoed. \\

Badly measured high energy muons are also the source of fake high MET ranging upto a few TeV.
Bad muons are defined as muons with $\sigma(q/p) / (q/p) > 0.2$ where $q/p$ is the track fitting parameter and $\sigma(q/p)$ is the error.a
The entire events will be vetoed if containing at least one bad muon.


Fig. \ref{fig::SRdefinition::badMuonVeto} demonstrate the performance of bad muon veto. While bad muon events typically peak in $\Delta \phi (l,\met)$ since the fake MET aligns with the muon, it is exclusively resoluved by the veto. Also, the role of bad muon veto is shown to be very important in this analysis as the 1-muon high MET phase space generally suffers from severe contaminaton by bad muon events upto about 20$\%$ (90$\%$) with  $\met>1(2)\tev$.  \\

\begin{figure}[h]
  \centering
%    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/badMuons/lep1Pt_met1TeV.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/badMuons/met_met0TeV.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/badMuons/dPhi_met_lep_met1TeV.pdf}}
    \caption{(a) MET distribution after requiring exactly one signal muon and MET trigger, and (b) $\Delta \phi (l,\met)$ distribution with $\met>1\tev$ being applied. The pink histogram corresponds to events dropped by the bad muon veto. The veto looks working reasonably considering the apparent spike due to the fake MET: $\Delta \phi \sim \pi$ is cleared.}
    \label{fig::SRdefinition::badMuonVeto}
\end{figure}

The pre-selection is the common selection for all the signal regions in the analysis, which is defined as Tab. \ref{tab::SRdefinition::Preselection}.
\tab{ c }{
  \hline
  Event cleaning \\
  Pass the MET trigger and $\met>250\gev$ \\
  At least one signal electron (muon) with $\pt>7(6)\gev$. \\
  At least two jets with $\pt>30\gev$. \\
  \hline
}
{List of requirements for the 1-lepton pre-selection.}
{tab::SRdefinition::Preselection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Signal Region Definition}
\subsubsection{Binning Strategy}
To inclusively address to all the 45 decay models and all possible mass spectra, a set of taylored multi-bin signal regions (SRs) are employed.
Specifically, different decay models are covered by splitted the signal regions in terms of b-jet multiplicity (``categories''), 
and various scenario of mass spectra in the models are coped with the division in terms of kinematical cuts (``towers''). 
SR bins are basically designed to be exclusive for each other, aiming at an easy combination afterward so that no signals are lost due to the binning. \\

The definition of the b-jet based categories: b-vetoed (BV), b-tagged (BT) and 3B follows Tab. \ref{tab::SRdefinition::categoriesDef}. The main customers of these categories are repectively the models in Tab. \ref{tab::Introduction::modelsBV}, \ref{tab::Introduction::modelsBT} and \ref{tab::Introduction::models3B} in Sec. \ref{sec::Introduction::targetModels}, which are referred as ``BV'', ``BT'' and ``3B'' benchmark models from now on. 
The b-jet multiplicity for the reference signal models versus background at the pre-selection level is shown in Fig. \ref{fig::SRdefinition::nB}.
Note that despite a fraction of signal events falling into other categories than the benchmarked one, they will not be wasted thanks to the combined fit performed in deriving the final result.
As the S/N ratio and the background kinematics in BV/BT are found to be more or less similar, further kinematical selections in those categories are set to identical for simplicity. 
On the other hand, different seleciton strategy is adopted for the 3B categories since the background level is significantly lower and also the composition is very different. \\

%%%
\tab{ c | c | c }{
\hline
Category    & b-jet multiplicity   & Main background \\ 
\hline
\hline
B-vetoed (BV) & 0        & $\wjets$ \\
B-tagged (BT) & 1-2      & $\ttbar$ \\
3B            & $\geq 3$ & $\ttbar$, $\ttbar+cc/bb$ \\
\hline
}
{The definition of the b-jet based categories and the main backgrounds there.}
{tab::SRdefinition::categoriesDef}
%%%
\fig[110]{SRdefinition/discVar/nBJet30__Precut.pdf}
{B-tagged jet multiplicity for the standard model backgrounds and the reference signals (QQC1QQC1 for the BV, QQC1BTC1 for the BT and TTN1TTN1 for the 3B categories respectively) after the 1-lepton pre-selection.}
{fig::SRdefinition::nB}
%%%

\clearpage
The BV/BT categories are further divided into 4 ``towers'', to tackle the 4 typical configurations of the mass spectra for gluino and the LSP (and the intermediate EW gauginos in case of 1-step decays). The relation is schematized in Fig. \ref{fig::SRdefinition::binning_BTBV}, with the benckmark model ``QQC1QQC1'' being the example.
Each of them is further detailed as below:

\begin{enumerate}
\item The mass of intermediate EW gaugino is roughly in the middle of those of gluino and the LSP ($x \sim 1/2$).
This is the most standard configuration where particles from both gluino and the intermediate EW gaugino decays are hard enough to pass the criteria of hard lepton ($>35\gev$) and jets ($\pt>30\gev$). As the signals targeted by the BV/BT categories typically result in $4-10$ jets at the tree-level, a tower \textbf{6J} with $\nJetNoGev \geq 6$ is defined. 

\item Gluino and EW gauginos are all compressed. 
From either trigger and backgroud separation point of view, hard ISRs are indespensive for probing this type of signatures so that the $\tilde{g}\tilde{g}$ system gets kicked and resulting in large MET. On the other hand, as the kicked gluinos are typically enough heavy to be non-relativistic, the transcerse momentum of the boosted $\tilde{g}\tilde{g}$ system is almost solely converted into MET. As a result the particles from gluino decays stay soft. The \textbf{2J} tower consisting of a soft lepton, at least two hard jets and large MET is defined for targeting the signature.

\item,4 The intermediate EW gaugino and either gluino or LSP are compressed ($x \sim 0, 1$). 
There are also extreme cases where the intermediate EW gaugino mass is degenerate toward either of gluino or LSP and decoupled from the other. Two signal region towers: \textbf{$High-x$} and \textbf{$Low-x$} are employed to cover the scenarios. 
\end{enumerate}

Similar discussion holds for direct gluino decay models as well i.e. the tower \textbf{2J} covers the scenario of compressed mass spectra while the tower \textbf{6J} is used for general cases. \\

In contrast to the BV/BT category, the 3B does not undergo the additional classification in towers since the targeted signal models usually involve top quarks that can result in hard jets, leptons and MET. 
Therefore the kinematics does not dramatically vary between the mass configurations unless the top-quarks are on-shell.
The only exception is when gluino and the intermediate EW gaugino get compressed, and the top-quarks turn to off-shell ending up in soft decay particles.
However such events are then covered by the BT towers instead, thanks to the dropped $\geq 3$ b-jet acceptance according to the decreasing b-quarks' $\pt$. \\ 

To summarize, 5 towers (2J/6J/Low-x/High-x/3B) are defined in total out of 3 categories (BV/BT/3B) as in Tab. \ref{tab::SRdefinition::towersDef}. 
%The sensitivity converage by each signal region tower on the mass grid of the signal models are shown in Fig. \ref{fig::SRdefinition::towerCoverage}.

%%%%%%%%%%
\fig[160]{SRdefinition/EventSelection/binning_BTBV.eps}{The 4 signal region towers for the BT/BV categories, and their targeted mass configuration.}{fig::SRdefinition::binning_BTBV}

%%%
%\tab{ c | c | c | c | c }{
\tab{ c | c | c | c }{
\hline
%Category & Tower    & Electron (muon) $\pt$ [GeV]  &  $\nJet$ & Jet $\pt$ [GeV]\\ 
Category & Tower    & Electron (muon) $\pt$ [GeV]  &  $\nJet$  \\ 
\hline
\hline
      & 2J     & $\in [7(6), 35] $  & $\geq 2$ \\ \cline{2-4}
BV/BT & 6J     & $>35$              & $\geq 6$ \\ \cline{2-4}
      & Low-x  & $\in [7(6), 35] $  & $\geq 4$ \\ \cline{2-4}
      & High-x & $>35$              & $\geq 4$ \\
\hline
3B    & 3B     & $>15$              & $\geq 7$ \\
\hline
}
{List of defined towers in each b-category and t kinamtical selection required. \textbf{2J} and \textbf{6J}, \textbf{Low-x} and \textbf{High-x} are orthogonal to each other. \textbf{3B} are orthogonal to all the other towers.}
{tab::SRdefinition::towersDef}
%%%


\clearpage
Finally, the towers further experience the binning in terms of $\meffInc := \met + \sum_{i} p_T (j_i)$ to accommodate different absolute scale of mass splitting. 
The ``2J/6J'' and ``3B'' tower are segmeted into 3 and 2 bins respectively while ``Low-x'' and ``High-x'' are single-binned as their low $\meffInc$ bins have too much overlap with ``2J'' and ``6J'' in phase space which does not provide unique sensitivity. The bin widths of $\meffInc$ are set to be $400\gev-500\gev$ driven by the width of $\meffInc$ distribution for signals that the lower $\meffInc$ bins typically target ($\dmg = 1\tev \sim 1.5\tev$). The ``3B'' tower enjoys an exceptionally wider bin width with $750\gev$, compromising with limited of statistics in corresponding control regions. \\
%The lower bound of the last $\meffInc$ 

To conclude, the signal regions end up in 5 tower-structured bins as schematized as Fig. \ref{fig::SRdefinition::SRbinning}, where $3 \times 2$ bins in $\meffInc \times \mathrm{(BV/BT)}$ reside in the tower \textbf{"2J''} and \textbf{"6J''}, $1 \times 2$ bins in \textbf{"Low-x''} and \textbf{"High-x''}, and 2 $\meffInc$ bins in \textbf{"3B''}. Since all the SRs bins in the towers "2J/6J/3B" or "Low-x/High-x/3B" are statistically independent, they can be straightforwardly combined in a simultaneous fit. 
%%% In providing the result, we perform the fit for both two combined towers, and quote one giving the best sensitivity
%
%The left plots of of Fig. \ref{fig::SRdefinition::expLimitQQC1QQC1}-\ref{fig::SRdefinition::expLimitTTN1TTN1} present the mass spaces that each signal tower is supposed to address the sensitivity in the mass grids of benchmark models (footnote: in fact these are the expected exclusion limit by the finalized the signal regions after all the optimization, but also good enough to give some idea how each signal region interplay and complement the sensitivity each other).
Fig. \ref{fig::SRdefinition::towerCoverage1}-\ref{fig::SRdefinition::towerCoverage2} schematize the mass regions in the signal grids that each signal region tower or bin is supposed to address the sensitivity for the benchmark models.

%%%%%%%%%%%
\fig[110]{SRdefinition/EventSelection/SRbinning.pdf}{Tower structure and the $\meffInc$ binning of signal regions.}{fig::SRdefinition::SRbinning}
%%%%%%%%%%%

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.7\textwidth]{figures/SRdefinition/EventSelection/TowerCoverage_BVBT_x12.pdf}}
    \subfigure[]{\includegraphics[width=0.7\textwidth]{figures/SRdefinition/EventSelection/TowerCoverage_BVBT_dM.pdf}}
    \caption{ 
     Sensitivity converage by individual signal region towers or $\meffInc$-bins in the 
    (a) the \dire and \xhalf grid, and in (b) the \DMtw, \DMth  grid of the BT/BV benchmark models. Dashed contours and shaded areas schematize the regions that the individual tower or $\meffInc$-bin addresses the sensitivity.}
    \label{fig::SRdefinition::towerCoverage1}
\end{figure}

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.6\textwidth]{figures/SRdefinition/EventSelection/TowerCoverage_3B_x12.pdf}}
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/SRdefinition/EventSelection/TowerCoverage_BVBT_varx.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/SRdefinition/EventSelection/TowerCoverage_3B_varx.pdf}}
    \caption{ 
     Sensitivity converage by individual signal region towers or $\meffInc$-bins in the 
    (a) \dire, \xhalf, \DMtw and \DMth grids of the 3B benchmark models, (b) the \varx grid of the of the BT/BV benchmark models, and in (c) the \varx grid of the 3B benchmark models. Dashed contours and shaded areas schematize the regions that the individual tower or $\meffInc$-bin addresses the sensitivity.}
    \label{fig::SRdefinition::towerCoverage2}
\end{figure}


\clearpage
% ------ Definition of Variables
\subsubsection{Discriminating variables}
Kinamtical variables used for background rejection as well as defining control regions are overviewed. The distributions of backgrounds overlayed with benchmark signals at the preselection are presented in Fig. \ref{fig::SRdefinition::distVar1} - \ref{fig::SRdefinition::distVar2}. In addtion to the pre-selection, a soft lepton ($\pt(\ell)\in[6,35]$) is required in Fig. \ref{fig::SRdefinition::distVar2} (b), $\meffInc>1500\gev$ in for Fig. \ref{fig::SRdefinition::distVar2} (b), and $\nBJetNoGev \geq3 \,\,\, \mt>125\gev$ are applied in Fig. \ref{fig::SRdefinition::distVar2} (c) and (d). The definition and the purpose of the variables are as below: \\

\paragraph{${\bf \nJetNoGev}$}
Jet multiplicity often shows the great discriminating power since the standard model processes suffer a sharpe cut-off.
However one should mind that the optimum cut is significantly dependent on the gluino decay mode, and also that the aggressive cut will enhance the contribution from higher order effect, putting the modeling at the risk of large theoretical uncertainty.
Therefore, it is kept to a moderated use as means of background rejection. \\

\paragraph{${\bf \met}$}
Signal events result in large $\met$ reflecting the presence of hard additional undetected LSP when $\dmg$ is large.
At analysis level, this is also true for the compressed case given that the MET via ISRs is nevertheless required for the trigger sake as described above.

\paragraph{${\bf \meffInc}$} 
$\meffInc$ is the variable best reflecting the magnitude of absolute mass splitting $\dmg$, providing the best separation against backgrounds.
Meanwhile it is also noticable that the magnitude of $\meffInc$ is almost uniquely determined by $\dmg$, regardless of the relative mass splitting and gluino decays, 
therefore the optimal cut in $\meffInc$ is highly universal.
%better than applying cuts on jet pt individually giving the low xsec for signal that cant afford in acceptance.


\paragraph{${\bf \mtFull}$} 
Invariant mass of $\met$ and the lepton with the z-momentum set to 0.
Analogous to ordinary invariant mass peaking at the mass of the parent particle, the end point of $\mt$ represents the parent mass when they share the same origin.
Since SM 1-lepton process is always with a leptonically decaying W-boson without additional hard missing particles, the bulk component experiences a sharpe cut-off in $\mt$ around $m_W = 81.4 \gev$, 
therefore the cut above $m_W$ is tremendously effective.


\paragraph{${\bf \met/\meffInc}$}
$\met/\meffInc$ separates backgrounds and signals targeted by the \textbf{2J} and \textbf{High-x} where jet activity is relatively low compared with the magnitude of MET required.


\paragraph{Aplanarity}
Aplanarity is a variable characterizing the 3-dimensionality of an event in terms of the final state particles. It is defined as the thertial eigenvalue of the normalised jet momentum tensor $S$:
\begin{align}
  S^{\alpha \beta} & := \frac{\sum_i p_i^{\alpha} p_i^{\beta} }{\sum_i |\bm{p_i}|^2 }, \nn \\ 
  %
  P^{-1}SP & = \left(
  \begin{array}{c c c }
    \lambda_1  & & \\
    & \lambda_2 & \\
    & & \lambda_3 \\
  \end{array}
  \right), \,\,\, \lambda_1 > \lambda_2 > \lambda_3, \nn \\
  \mathrm{Aplanarity} & := \frac {3}{2} \times \lambda_3,
  %
\end{align}
where $P$ stands for the $3\times3$ matrix diagonalizing $S$, $\lambda_i$ for the eigenvalues of $S$.
It ranges from $0<A<1/2$; $A=0$ corresponds to events with jets distributed in the common plain, and $A=0.5$ represents the isotropically distributed event topology.
Aplanarity is an effective discriminator after requiring tight selection in $\meffInc$ or $\met$, where the remnant SM events (particularly $\wjets$) are typically heavily kicked by hard ISR radiations, leading to a highly linear event topology in their center-of-mass frame. These events end up in a planar topology in the lab frame once getting boosted toward the beam direction, as a result populating in low aplanarity region accordingly. 
On the other hand, the decay of gluino pairs keep relatively spherical thus the aplanarity distributing rather flatly, which reflects the fact the gluinos are too heavy to be boosted.


\paragraph{${\bf \nJetNoGev/\lepOnePt}$}
Since the hardness of lepton and jets are positively correlated in normal processes in SM, it is relatively rare to end up in a soft lepton and hard jet activity simultaneously, while it is the case for the compressed gluino signature. A variable $\nJetNoGev/\lepOnePt$ helps visualize the different correlations, and used in the \textbf{2J} signal region towers to improve the sensitivity of the compressed gluino signatures.


\paragraph{${\bf \mindPhiFourJet}$}
vA variable indended to reject the remnant $\ttbar$ events after requiring tight selection of $\meffInc$ and $\met$. 
As such $\ttbar$ events typically have hard ISR jets to boost the $\ttbar$ system, the jets from $\ttbar$ decays and associated soft radiation tend to be collimated each other. Conversely, the jets from the gluino decays almost never get collimated as due to the heavy mass of gluino.


\paragraph{Topness}
One of the most important background in 1-lepton analysis is di-leptonic $\ttbar$ events with a hadronically decaying tau lepton or a lepton that fails the baseline requirement. To reject those events, a $\chi^2$-based di-leptonic $\ttbar$ tagger ``topness'' has been designed in context of scalar-top search since Run1 \cite{Topness}. 
The $\chi^2$ function is defined as:
\begin{align}
& S(p_{\mathrm{W}}^x, p_{\mathrm{W}}^y, p_{\mathrm{W}}^z, p_{\nu}^z)  \nn  \\
& = \chi^2(m_{t,1}^2) + \chi^2(m_{t,2}^2) + \chi^2(m_{W,1}^2) + \chi^2(\hat{s}(\ttbar))  \nn  \\
& = \frac{  \left( m_t^2 - (p_{b,1}+p_{\ell}+p_{\nu})^2                      \right)^2   }{a_t^4}  \nn \\
& + \frac{  \left( m_t^2 - (p_{b,2}+p_\mathrm{W})^2                          \right)^2   }{a_t^4}  \nn  \\
& + \frac{  \left( m_\mathrm{W}^2 - (p_{\ell}+p_{\nu})^2                     \right)^2   }{a_\mathrm{W}^4}  \nn \\
& + \frac{  \left( 4m_t^2- (p_{\ell}+p_{\nu}+p_{b,1}+p_{b,2}+p_\mathrm{W})^2 \right)^2   }{a_{\ttbar}^4},
\label{eq:SRdefinition::topness}
\end{align}
assuming an event topology as shown Fig. \ref{fig::SRdefinition::topness_diagram} where one of the lepton are totally undetected and the momentum does fully contribute to MET. \\
It consists of four gaussian constraints imposing the mass constraint of top-quark and W-boson, and the center-of-mass for the $\ttbar$ system being close to its minimum threshold ($2m_t$). The width parameters are set to $(a_t, a_W, a_{\ttbar})=(15,5,1000) \gev$, accounting for the Breit-Wigner widths of top-quark and W-boson as well as the tail of $\hat{s}(\ttbar)$ distribution. Although there are three missing particles in the topology, the number of unknown degree of freedom can be reduced into 4 by combining the missing lepton ($\ell_2$) and the paired neutrino ($\nu'$) into a single onshell W-boson and imposing the vectoral sum of transverse momenta of missing particles being equal to $\met$. 
Topness is then defined as the minimum $\chi^2$ when scanning over the four DOFs parametrized by $\bm{p}_{\mathrm{W}}$ and $p_{\nu}^z$:
\begin{align}
\mathrm{Topness} := \min_{p_{\mathrm{W}}^x, p_{\mathrm{W}}^y, p_{\mathrm{W}}^z, p_{\nu}^z} \ln[S].
\end{align}
Events in the topology assumed are supposed to have solutions ($p_{\mathrm{W}}^x, p_{\mathrm{W}}^y, p_{\mathrm{W}}^z, p_{\nu}^z$) that satisfy the four constraints at the same time while scanning, however it is not necessarily the case for the other type of events. Fig. \ref{fig::SRdefinition::topness_diagram} shows typical separation between di-leptonic $\ttbar$ and signals. Although di-leptonic $\ttbar$ does have a fraction of unfortunate events on the pile of higher values due to the fact that the energy of missing leptons or tau leptons does not entirely contribute to MET, the majority resides on the left pile while signals typically populate  more in the opposite one. 

%%%%%%%%% diagram for illustrating topness 
\fig[100]{SRdefinition/EventSelection/topness_diagram.pdf}
{Di-leptonic $\ttbar$ topology assumed in the topness calculation where one lepton is tagged ($\ell_1$) and the other lepton ($\ell_2$) is not identified as any objects with its momentum fully contributing to MET. Topness is defined as minimum summed $\chi^2$ of three mass shell constraints for the top, anti-top and the W-boson decaying into $\ell_1$, as well as one pseudo-mass constraint in terms of the $\ttbar$ systems (labeled as pink circles), while scanning over the momenta space of missing particles. The degrees of freedom by $\ell_2$ and the associated neutrino ($\nu_2$) are combined into a 4-momentum $p_W$ with the mass fixed to $m_W = 81.2\gev$, and the scan is performed in terms of $p_{\mathrm{W}}^x, p_{\mathrm{W}}^y, p_{\mathrm{W}}^z$ and $p_{\nu}^z$ from $-4\tev$ to $4\tev$ respectively.}
{fig::SRdefinition::topness_diagram}
%%%%%%%%%%%

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/nJet30__Precut.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/lep1Pt__Precut_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/met__Precut_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/meffInc30__Precut_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/mt__Precut_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/LepAplanarity__Precut_log.pdf}}
    \caption{ 
    Distributions of discriminating variables for refence signal and backgrous, at the preselection level.
    (a) $n_J$, (b) Lepton $\pt$, (c) $\met$, (d) $\meffInc$, (e) $\mt$ and (f) aplanarity are respectively shown.
    \label{fig::SRdefinition::distVar1}       
    }
\end{figure}

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/nJetOverLepPt__Precut_softLep.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/metOverMeff__Precut_meff1500.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/min_dPhi_4j__Precut3B_MT125.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/SRdefinition/discVar/topNess__Precut3B_MT125.pdf}}
    \caption{ 
    Distributions of discriminating variables at the preselection level.
    Soft lepton requirement: $\pt(\ell)\in[6,35]$ is applied for (b),
    and $\nBJetNoGev \geq3 \,\,\, \mt>125\gev$ is applied for (c). 
    \label{fig::SRdefinition::distVar2}
    }
\end{figure}


The trend of the kinematical variables over the mass grids are shown in Fig. \ref{fig::SRdefinition::kineMap_QQC1QQC1_x12}-\ref{fig::SRdefinition::kineMap_TTN1TTN1}. The color scale (z-axis) indicates the mean of the distribution in the variables, for the signal process in the mass point designated by the xy-coordinate. Three QQC1QQC1 grids (``x=1/2'', $''\mLSP=60\gev''$ and $''\dmc=30\gev''$) and one TTN1TTN1 grid are displayed as the benchmark model for BV/BT signal regions and the 3B signal regions respectively. 

One can find that the variables related to trasverse momenta of outgoing particles such as $\meffInc$, $\lepPt$ and $\met$ simply scale with the mass spliting, while the other variables such as aplanarity and $\metOverMeff$ etc. are sensitive to the relative mass spilitting, therefore helpful in defining SR \textbf{Low-x}/\textbf{High-x}.
\clearpage
\input{tex/SRdefinition/fig_kineMap.tex} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage		
\subsubsection{Cut Optimization}
The cut values for the kinematic variables listed above are optimized, including the lower $\meffInc$ cut for the highest $\meffInc$-bin. Reference signal points are defined in Tab. \ref{tab::SRdefinition::refSigPointsOptm}, to which the sensitivity is optimized. The optimization procedure proceeds as fowlling.

\begin{enumerate}
\item The binning of $\meffInc$ is roughly decided so that the sensitivities for all the reference points in the same tower are maintained.
\item Cuts values in other variables are then optimized by a simultanous grid scan using matchinary. The initial values are chosen based on the target mass regions of each signal region (as dipicted by Fig. \ref{fig::SRdefinition::towerCoverage1}-Fig. \ref{fig::SRdefinition::towerCoverage2}), and the typical kinematics of such signals as shown in Fig. \ref{fig::SRdefinition::kineMap_QQC1QQC1_x12}-\ref{fig::SRdefinition::kineMap_TTN1TTN1}.
The sensitivity as the referece of the optimiaton is defined by the combined significance of $\meffInc$ bins such as :
\begin{align}
& Z_{N,\mathrm{comb.}} =\sqrt{\sum_i Z_{N,i}^2}, \nn \\
& Z_{N,i} := S_i/\sqrt{B_i+\alpha^2 B_i^2}, \label{ZNcomb}
\end{align}
where $Z_{N,i}$ is the significance provided by a single $\meffInc$ bin, with $S_i$, $B_i$ being the signal and background yields in the $\meffInc$ bin. $\alpha$ is the parameter respresenting the systematics uncertainty on background estimation in the $\meffInc$ bin, where flat $30\%$ is assigned during the optimization. The cut between BT and BV bins in the same tower and $\meffInc$-bin are always set to common.

\item All the cuts including the $\meffInc$ binning are re-optimized by purtubating them from the optimum configuration obtained in the previous step simultaneously.

\item Optimum cuts are different bwteen reference points in the same $\meffInc$ tower. An adjustment is therefore applied for the best compromization, as well as to avoid the over-optimization on sepecific signal points.

\item Another minor adjustment is done afterwards, requrired from the context of background estimation. Some of the cuts are loosened to facilitate the control region definition.
\end{enumerate}

\tab{ c c c}{
      \hline
      &   Model    & $(\mG,\mC,\mLSP)$,$(\mG,\mLSP)$ [GeV] \\
      \hline
      \hline
      \textbf{2J BV}  & & \\
      \hline
      &   QQC1QQC1 & (1550,580,550)  \\
      &   QQC1QQC1 & (1065,1025,985) \\
      &   TTN1TTN1 & (1000,915)      \\
      \hline
      \textbf{2J BT}  & & \\
      \hline
      &   QQC1BTC1 & (1400,830,800)  \\
      &   QQC1BTC1 & (1550,780,750)  \\
      \hline
      \textbf{6J BV}  & & \\
      \hline
      &  QQC1QQC1 & (1945,1105,265)  \\
      &  QQC1QQC1 & (1850,1350,850)  \\
      &  QQC1QQC1 & (1700,1300,900)  \\
      \hline
      \textbf{6J BT}  & & \\
      \hline
      &  QQC1BTC1 & (1850,1050,250) \\
      &  QQC1BTC1 & (1700,1300,900)  \\ 
      \hline
      \textbf{Low-x BV}  & & \\
      \hline
      &  QQC1QQC1 & (1700,460,60)    \\ 
      &  QQC1QQC1 & (1600,260,60)    \\
      &  QQC1QQC1 & (1700,530,500)     \\                                
      \hline
      \textbf{Low-x BT}  & & \\
      \hline
      &  QQC1BTC1 & (1700,730,700) \\
      &  QQC1BTC1 & (1700,530,500) \\
      \hline
      \textbf{High-x BV}  & & \\
      \hline
      &  QQC1QQC1 & (1800,1600,60) \\
      &  QQC1QQC1 & (1800,1460,60) \\
      &  QQC1QQC1 & (1800,1260,60) \\
      \hline
      \textbf{High-x BT} & & \\
      \hline
      &  QQC1BTC1 & (1850,1750,60)    \\
      &  QQC1BTC1 & (1850,1450,60)    \\
      \hline
      \textbf{3B}  & & \\
      \hline
      &  TTN1TTN1 &  (2000,0)     \\ 
      &  TTN1TTN1 &  (1900,800)   \\ 
      &  TTN1TTN1 &  (1500,1000)   \\
      \hline
}
{The reference signal points for each signal regions to which the selection is optimized to.}
{tab::SRdefinition::refSigPointsOptm}


Finalized definiton of signal regions are shown in Tab. \ref{SRdefinition::regionDef2J}-\ref{SRdefinition::regionDef3B}. The $\meffInc$ distribution in the optimized signal regions are displayed in Fig. \ref{fig::SRdefinition::SRmeffInc2J}-\ref{fig::SRdefinition::SRmeffInc3B} for backgrounds with the referece signal points overlaid. The segmentation of $\meffInc$-bin is found to successfully address the sensitivity in different mass region in the signal grid. \\

The optimized selection is also validated by a set looking at the kinematic distribution Fig. \ref{fig::SRdefinition::N1plots_2JMEFFInclBV}-\ref{fig::SRdefinition::N1plots_3BMEFFIncl} in which the one of the cuts is loosened from the optimzed signal regions. The sensitivity is calculated as function of the cut position of the removed cut. The decided cuts are shown by the red arrows, which are more or less at the optimum position for all the reference signals.

\input{tex/SRdefinition/tab_SRdefinition.tex}
\input{tex/SRdefinition/fig_SR_meffInc.tex}
\input{tex/SRdefinition/fig_N1plots.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage				
\subsubsection{Expected Sensitivity}
The limits expected by the optimized signal regions are calcuated for the grids of reference models.
The expected exclusion limit with $L=36.1\ifb$ for the TTN1TTN1 \dire grid is shown in \ref{fig::SRdefinition::expLimitTTN1TTN1}. 
The dashed lines on the left plots indicate the exclusion provided by a single $\meffInc$ bin, and the solid lines being the limit given by respective signal region towers with combined bins. 
The ultimately sensitivity provided by the combined towers are shown in the right plots. 
Since the all five towers are not completly orthogonal (\textbf{2J} and \textbf{Low-x}, \textbf{6J} and \textbf{High-x} are partially overlapped), 
there are four possible way of combining orthogonal towers: \{\textbf{2J}, \textbf{6J}, \textbf{3B}\}, \{\textbf{2J}, \textbf{High-x}, \textbf{3B}\}, \{\textbf{Low-x}, \textbf{6J}, \textbf{3B}\}, and \{\textbf{Low-x}, \textbf{High-x}, \textbf{3B}\}.
The final result will be provided using the combination with best expected sensitivity. 
The expected sensitivity for QQC1QQC1 and QQC1BTC1 are presented in Fig. \ref{fig::SRdefinition::expLimitQQC1QQC1} and Fig. \ref{fig::SRdefinition::expLimitQQC1BTC1}. Nice complementality between the signal region towers are shown. No suspicious structure indicating local over-optimization onto specific mass region is found, ensuring the inclusive sensitivity of the search. 
% each SR does its best job, providing complemetal coverage in terms of mass configuration
% 3B
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.98\textwidth]{figures/SRdefinition/expLimit/canvas_symTTN1_x12.pdf}}
    \caption{Expected exlusion (95$\%$CL) for the benchmark model TTN1TTN1. The left plot shows the exclusion limit set by indivisual signal region $\meffInc$-bin (dashed) or a tower (solid). The contours in the right plot display the ultimate sensitivity provided by the combined fit. The hypothetical test will be cuarried out using the best performed combination, in deriving the final result. }
    \label{fig::SRdefinition::expLimitTTN1TTN1}
\end{figure}


\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_symQQC1_x12.pdf}}
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_symQQC1_varx.pdf}}
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_symQQC1_dM30.pdf}}
    \caption{Proejcted expected exlusion (95$\%$CL) for the benchmark model QQC1QQC1 onto the (a)$x=1/2$ (b)$\mLSP=60\gev$ (c) $\dmc=30\gev$ grid. The contours in the right plot display the ultimate sensitivity provided by the combined fit. The hypothetical test will be cuarried out using the best performed combination, in deriving the final result. }
    \label{fig::SRdefinition::expLimitQQC1QQC1}
\end{figure}

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_QQC1BTC1_x12.pdf}}
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_QQC1BTC1_varx.pdf}}
    \subfigure[]{\includegraphics[width=0.8\textwidth]{figures/SRdefinition/expLimit/canvas_QQC1BTC1_dM30.pdf}}
    \caption{Proejcted expected exlusion (95$\%$CL) for the benchmark model QQC1BTC1 onto the (a)$x=1/2$ (b)$\mLSP=60\gev$ (c) $\dmc=30\gev$ grid. The contours in the right plot display the ultimate sensitivity provided by the combined fit. The hypothetical test will be cuarried out using the best performed combination, in deriving the final result. }
    \label{fig::SRdefinition::expLimitQQC1BTC1}
\end{figure}

\section{Standard Model Background Estimation} 
This section presents the methods used for estimating backgrounds. After reviewing the breakdown in the signal regions and how they evade the event selection, two methods are introduced to estimate different background components, namely, \textbf{object replacement method} for estimating the di-leptonic components and \textbf{kinematical extrapolation method} for the rest.

% BG estm はsearch で一番大事
% robustな結果を出すためにかなりhigh levelなdedicated data-driven approachを採用した

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background Breakdown in the Signal Regions} \label{sec::BGcomposition}
The breakdown of physics processes in the signal regions are shown in Fig. \ref{fig::BGestimation::BGcomposition_splitLv2}. $\wjets$ and $\ttbar$ dominate over the b-tagged and b-vetoed regions respectively in general, while dibosons and single-top are sub-dominant. The \textbf{3B} towers are completely dominated by $\ttbar$, where 60 $\%$ of them are with heavy flavor jets via radiative gluon splitting ($\ttbar+cc/bb$) while the rest are with one light flavor jet or hadrinocaying $\tau$ faking into b-tagged jet ($\ttbar+b_{\mathrm{fake}}$).  \\

Backgrounds also follow the categorization depending on the mechanism they pass the selection as listed in Tab. \ref{tab::BGestimation::BGclass}, based on which the estimation method is decided.

\tab{|c|c|c|c|c|}{
  \hline
  \multicolumn{2}{|c|}{ Category }    & Origin                                                  &  Main physics process   & Estimation method \\
  \hline
  \multicolumn{2}{|c|}{``Semi-leptonic''}        & On-shell W with diluted $\mt$ & ($W$, $\ttbar$, $VV$) $\ra \ell \nu$ + jets   & Kine. extp. / MC  \\
  \multicolumn{2}{|c|}{}              & / High-mass Drell-Yan                                    &                         &                  \\
  \hline
  ``Di-leptonic'' & $\ell\ell_{\mathrm{mis.}}$ & "Out Acc."       & ($\ttbar$, $Wt$, $WW$) $\ra \ell \nu \ell \nu$ + jets          & Kine. extp.       \\
         &                            & "Mis. Reco."     &                                                                & Obj. rep.         \\
         &                            & "Mis. ID"        &                                                                & Obj. rep.         \\
         &                            & "Mis. OR"        &                                                                & Kine. extp.       \\
  \cline{2-5}
         & $\ell\tau_{\mathrm{h}}$    & 1 real-lepton + $\tau_h$ & $\ttbar$, $Wt$, $WW$ $\ra \ell \nu \tau \nu$ + jets            & Obj. rep.         \\
  \hline
  \multicolumn{2}{|c|}{``Fake''}        & 0 real-lepton + 1 fake-lepton.  & $W\ra\tau\nu$, $Z\ra\nu\nu$          & MC                \\
  \hline
}
{Background classification in terms of the origin.}
{tab::BGestimation::BGclass}
%

The \textbf{``semi-leptonic''} category is defined by events with exactly one real light flavor lepton ($e,\mu$). In the SM, these are uniquely provided by processes with leptonically decaying W-boson, such as from $\wjets$ and $\ttbar$. This is by far the dominant component at 1-lepton preselection level, however is drastically suppressed after a tight $\mt$ cut since the they are largely truncated at $m_{W}$. After the $\mt$ cut, the remnant events are typically either: 1) Drell-Yan process with virtual heavy intermediate W boson, or 2) events with badly measured MET leaidng to prolonged tail in $\mt$. The former contribution is typically larger although the latter addresses more with increasing jet activity, as shown in Fig. \ref{fig::BGestimation::Wmassline}. The dominant processes $\wjets$ and $\ttbar+Wt$ are estimated by a semi-data driven approach referred as ``kinmatical extrapolation method'' as detailed in following sub-section, while the other processes are taken from pure MC predictoin since they are minor. \\
%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.415\textwidth]{figures/BGestimation/BGcomponent/Wmassline_mt125.pdf}}
    \subfigure[]{\includegraphics[width=0.575\textwidth]{figures/BGestimation/BGcomponent/Wjets_onshellFrac_highMT.pdf}}
    \caption{ (a) Truth invariance mass $m(\ell,\nu)$ of high-$\mt$ $\wjets$ events. Ideally there are only high-mass Drell-Yan type of events, however due to the finite detector resolution, a fraction of on-shell W events with badly measured MET sneak into regions with $\mt>m_W$. (b) The fraction of on-shell events defined by $m(\ell,\nu) \in [60,125]$, as a function of the $\mt$ cut. It is generally below 50 $\%$, however increases with higher jet activity in the event. \label{fig::BGestimation::Wmassline} }
\end{figure}
%%%%%%%%%%

The \textbf{``di-leptonic''} category consists of processes with real two leptons including $\tau$, mainly from di-leptonic decaying $\ttbar$, $Wt$ and $WW$. The presence becomes highly significant with respect to the ``semi-leptonic'' after the $\mt$ cut, since the source of missing transverse momentum is multiple thus they have no reason to cut-off at $m_{W}$.  \\
They fall into 1-lepton regions through two channels, namely ``$\ell\ell_{\mathrm{mis.}}$'': events with two real light flavor leptons and one of them fails the ``baseline'' requirement ("missing lepton"), and ``$\ell\tau_{\mathrm{h}}$'': events with a real light flavor lepton and a hadronically decaying tau lepton. \\
%
The origin of ``missing lepton'' is further four-fold and symbolized as follow: 
\begin{description}
\item [``Out Acc.''] \mbox{} \\
 Leptons going outside the acceptance of ``baseline'' requirement i.e. $p_{\mathrm{T}}>7(6)\gev, |\eta|<2.47(2.5)$ for electrons (muons).
\item [``Mis. Reco''] \mbox{} \\
 Leptons within the $\pt-\eta$ acceptance but failing the reconstruction i.e a truth lepton that can not be associated with any of reconstructed electrons/muons in the xAOD container.
\item [``Mis. ID''] \mbox{} \\
 Reconstructed leptons within the $\pt-\eta$ acceptance but failing the electron/muon ID for the ``baseline'' requirement.
\item [``Mis. OR''] \mbox{} \\
 Reconstructed leptons within the $\pt-\eta$ acceptance passing the ID for "baseline" requirement, but killed in the overlap removal. 
%Technically, here it is defined as an identified lepton with the nearest non-b-tagged jet closer than $\Delta<0.4$.
\end{description}

One nice thing about this \textbf{``di-leptonic''} components is that 2-lepton regions are available for control regions in the estimation. Since no signal regions are set there, exactly the same phase space with respect to SRs can be exploited. This is performed by the ``object replacement method'' as detailed in the follwoing sub-section, although ``Out Acc.'' and ``Mis. OR'' are challenging for some technical reasons thus are estimated altogether with the  \textbf{``semi-leptonic''} events. \\
%

The third category \textbf{``fake''} involves events with a fake lepton, which is negligible except regions dealing with soft leptons (\textbf{``2J''} and \textbf{``Low-x''}). Dominant contribution is from $W\ra\tau\nu$ and $Z\ra\nu\nu$ which accompanies a large MET from neutrinos. While the contribution from the multi-jets process is supposed to be negligible, it is dedicatedly cross-checked since the impact could be hazardous due to the huge cross-section. This is done using a series of validation regions referred as VRs-QCD. The detail is found in Sec. \ref{sec::BGestimation::VRQCD}. \\
%(footnote) for definition of ``fake'' please refer Sec. \ref{sec::fakeLepton}.  

The relative popularity over the sub-categories in the signal regions are summarized in Fig. \ref{fig::BGestimation::BGcomposition_objRep} illustrates, where \textbf{``semi-leptonic''} and \textbf{``di-leptonic''} (particularly ``$\ell\tau_h$'') are overwhelmingly dominant in BV and BT signal regions respectively.


\clearpage
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.97\textwidth]{figures/BGestimation/BGcomponent/prod-01-825-03/BGcomp_splitLv2_SRBV_norm.pdf}}
    \subfigure[]{\includegraphics[width=0.97\textwidth]{figures/BGestimation/BGcomponent/prod-01-825-03/BGcomp_splitLv2_SRBT_norm.pdf}}
%    \subfigure[]{\includegraphics[width=0.7\textwidth]{figures/BGestimation/BGcomponent/BGcomp_splitLv2_SR3B_norm.eps}}
    \caption{ Background composition in terms of physics processes in the (a) BV, and (b) BT/3B signal regions. $\ttbar$ and single-top are merged as ``Tops'', and the semi-leptonic and di-leptonic components are respectively labeled as ``semi-leptonic'' and ``$2L+L\tau_h$''. \label{fig::BGestimation::BGcomposition_splitLv2} }
\end{figure}

\clearpage
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.97\textwidth]{figures/BGestimation/BGcomponent/prod-01-825-03/BGcomp_objRep_SRBT_norm.pdf}}
    \subfigure[]{\includegraphics[width=0.97\textwidth]{figures/BGestimation/BGcomponent/prod-01-825-03/BGcomp_objRep_SRBV_norm.pdf}}
%    \subfigure[]{\includegraphics[width=0.7\textwidth]{figures/BGestimation/BGcomponent/BGcomp_objRep_SR3B_norm.eps}}
    \caption{ Background breakdown in the (a) BV, and (b) BT/3B signal regions based on the classification in Tab. \ref{tab::BGestimation::BGclass}. While the BV signal regions are dominated by the ``semi-leptonic'' category, BT/3B signal regions are mainly with ``di-leptonic'' especially the ``$\ltauh$'' component.
 \label{fig::BGestimation::BGcomposition_objRep} }
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\subsection{MC vs Data Comparison and the MC mis-modeling} \label{sec::BGestimation::dataMC}
The reliability of MC on dominant background processes ($\wjets$ and $\ttbar$) is examined in pre-selection regions defined in Tab.\ref{tab::BGestimation::preselectionDef}. Each pre-selection region is intended to be dominated by the process being tested. 

\tab{c|c|c|c|c|c}{
  \hline
  Region name            & $\nLepbase$ & $\nLepsignal$ & $\lepOnePt$ [GeV]     & $\nBJet$  & Tested processes   \\
  \hline
  \hline
%  1L   (hardLep/softLep) & 1           & 1             & $>35$ / $[7(6),35]$ for $e$ ($\mu$) & -        & $\wjets$, $\ttbar$ \\
  1LBV (hardLep/softLep) & 1           & 1             & $>35$ / $[7(6),35]$ for $e$ ($\mu$) & $0$      & $\wjets$           \\
  1LBT (hardLep/softLep) & 1           & 1             & $>35$ / $[7(6),35]$ for $e$ ($\mu$) & $[1,2]$  & $\ttbar$           \\
  2LBT                   & 2           & 2             & -                     & $[1,2]$  & $\ttbar$           \\ 
  1L3B                   & 1           & 1             & $>15$                 & $\geq 3$ & $\ttbar+cc/bb$, $\ttbarFakeB$ \\
  \hline
}
{Definition of pre-selection regions and corrsponding tested physics processes. MET trigger requirement, event cleaning described Sec. \ref{sec::SRdefinition::eventCleaning}, $\nJetNoGev \geq 2$ and $\met>250$ are applied as common selection.}
{tab::BGestimation::preselectionDef}

\subsubsection{$\wjets$}
Fig. \ref{fig::BGestimation::DataMCPreselHardBV1} - \ref{fig::BGestimation::DataMCPreselHardBV2} show the kinematic distribution of the pre-selection region \textbf{1LBV hardLep} where $\wjets$ is enriched. While the bulk phase space is well-described by MC, there are a strking tendency of overestimation by MC in the tail. Discrepancy is mainly observed in distributions related to the jet activity, particularly in jet multiplicity when it is above 3. Considering that they here are all from ISR/FSR and the jet multiplicity in the event roughly corresponds to the number of QCD-order of the processes, this directly implies the mis-modeling in higher order effects beyond next-to-next-to-leading order (NNLO) level. This might not be surprising giving that the MC sample (Sherpa 2.2.1 generator) does not include loop diagrams beyond NLO and neither diagrams with more than 5 partons in the final state, due to computational limitation. \\

Therefore, an order-by-order cross-section correction should be helpful as the first aid. In fact, a simple MC reweighting in terms of jet multiplicity turns to work quite nicely. The reweighting function is derived by fitting linearly the observed data/MC in Fig. \ref{fig::BGestimation::DataMCPreselHardBV1} (a):
\begin{equation}
w = 1 - 0.1 \times (\nJetNoGev-2) \label{eq::BGestimation::rwgt_nJ},
\end{equation}
where $\nJetNoGev$ is the number of jets with transverse momenta greater than $30\gev$. While the jet multiplicity distribution is fully corrected by construction, the other descrepancies can be resolved almost perfectly as well, as shown in \ref{fig::BGestimation::DataMCPreselHardBV_rwgt1}. \\

Anthoer observed aspect of the mis-modeling is that it is more striking in terms of soft radiations rather than the hard ones. For example, the average jet transverse momentum distribution (Fig. \ref{fig::BGestimation::DataMCPreselHardBV_rwgt1} (c)) is well-modeled above $\sim 200 \gev$, while the slope of data over the MC for jet multiplicity distribution is rather persistent. This in fact backups that reweighting in other mis-modeled variables than jet multiplicity, such as $\meffInc$, actually does not work as successfully, since their tails are basically determined by hard jets. \\

Although this simple linear $\nJetNoGev$ reweighting demonstrated above seems qualitatively reasonable, it is not seriously used as correction due to the technical drawbacks that the optimum coefficients in Eq. \ref{eq::BGestimation::rwgt_nJ} has slight phase space dependence, and that it is difficult to validate them around signal regions since the data statistics is limited. However, it is still a good enough approximation as well as a useful reference expression to understand the behavior and corrlation of mis-modeling between variables. The reweighting with Eq. \ref{eq::BGestimation::rwgt_nJ} is then used for emulating the mis-modeling when designing the data-driven background estimation as described in the following sub-sections. \\

Variables that do not scale with transverse momenta of outgoing particles, such as $\mt$ or $\apl$, keep relatively well-modeled up to the tails. This feature seems more or less valid under tigher selections, therefore are used as the key variables extrapolating from control regions to signal regions in the kinematical extrapolation method as discussed later.

\clearpage
\input{tex/BGestimation/fig_DataMCPreselHardBV.tex}
\input{tex/BGestimation/fig_DataMCPreselHardBV_rwgt.tex}
%\input{tex/BGestimation/fig_DataMCPreselSoftBV.tex}
\clearpage
%%%%%%%%%%%%%


%%%%%%%%%%% VRZb
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRZb/mll__VRZb_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRZb/mll__VRZb__Zb130_log.pdf}}
    \caption{ .  \label{fig::BGestimation::VRZb} }
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{$\ttbar$}
Fig. \ref{fig::BGestimation::DataMCPreselHardBV1} - \ref{fig::BGestimation::DataMCPreselHardBV2} are the kinematic distributions in the pre-selection region \textbf{1LBT hardLep} dominated by $\ttbar$.
It is seen that MC is overshooting the data with increasing transverse momenta of outfoing particles such as jets, lepton and MET. \\


In particular, the mis-modeling in $\meffInc$ distribution is significant, which is concerning given that the signal regions are designed to exploit its shape. The leading souce of the mis-modeling is suspected to be in the description of ISR/FSR radiation. This is because hard jets ($\pt>200 \gev$) become more often non-$\ttbar$ origin in the tail of $\meffInc$, as demonstrated by Fig. \ref{fig::BGestimation::ISRFrac_ttbar}, although $\ttbar$ does have 2-4 jets in its tree-level decay. \\
This is in fact also supported by a series of MC reweighting studies shown in Fig. \ref{fig::BGestimation::slope_rwgt}  where linear reweighting in various top kinamtic variables is attempted to correct the the slope of data/MC in $\meffInc$. It turns that $p_{T}(\ttbar)$ is the variable most sensitive to the mis-modeling, while reweighting in other variables can only change the normalization but the slope. This strongly indicates that the primary problem is in the radiation recoiling the $\ttbar$ rather than in the internal kinamtics of the $\ttbar$ system. \\

%Therefore modeling of higher-order QCD/EW effects is again suspected as the cause of mis-modeling. \\
%Many explainatory higher-order effects have been proposed to account for the descrepancy, such as QCD-NNLO \cite{ttbar_NNLOQCD} or EW radiative correction \cite{ttbar_NLOEW}. \\
% top reweighiting の歴史

%%%%%%%%
\fig[100]{BGestimation/ISRFrac/ISRFrac_vs_meff.pdf}
{Fraction of ISR/FSR jets in the 4 leading jets with the largest transverse momenta, defined by $N_{\mathrm{events}}$($i$-th jets in the pt range that do not match either jets from ttbar decay by $\Delta R<0.2$)/$N_{\mathrm{events}}$(all $i$-th jets in the pt range).}
{fig::BGestimation::ISRFrac_ttbar}
%%%%%%%%

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/slope_rwgt/slopes_rwgt_topPt.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/slope_rwgt/slopes_rwgt_ttM.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/slope_rwgt/slopes_rwgt_ttPt.pdf}}
    \caption{ Response of data/MC in $\meffInc$ by reweighting $\ttbar$ events in terms of (a) average top transverse momentum
 ($(\pt(t)+\pt(\bar{t}))/2$), (b) invariant mass of $\ttbar$ system ($m_{\ttbar}$) and (c) transverse momentum of $\ttbar$ system ($\pt(\ttbar)$). $\pt(\ttbar)$ is found to be sensitive to the slope of $\meffInc$ and improve the data/MC discrepancy, while the other are only capable of shifting the normalization. 
\label{fig::BGestimation::slope_rwgt} }
\end{figure}


\clearpage
The $p_T(\ttbar)$-based reweighting is found also capable of restoring the descrepancy in other distribution other than $\meffInc$. Applying the reweighting function optimum for correcting the $\meffInc$:
\begin{align}
w = 1.05 \times \left[ 1 - 0.061 \,\times p_T(\ttbar) \right] \label{eq::BGestimation::rwgt_ttPt},
\end{align}
good data-MC agreement is seen in overall spectra regarding to jets and MET as shown in Fig. \ref{fig::BGestimation::DataMCPreselHardBT_rwgt1} - \ref{fig::BGestimation::DataMCPreselHardBT_rwgt2}. \\

The mis-modleing in lepton transverse momentum distribution seems to have the other origin, as sizable residual descrepancy is still seen. 
%This ends up a small descrepancy in $\mt$ as well, 
%This is thought to be related to the modeling of top polarization which is difficult when higher order diagrams are significantly involved. This \\


\clearpage
\input{tex/BGestimation/fig_DataMCPreselHardBT.tex}
\input{tex/BGestimation/fig_DataMCPreselHardBT_rwgt.tex}
%\input{tex/BGestimation/fig_DataMCPreselSoftBT.tex}
\clearpage

\clearpage
%jet activity while the distributions in other variables are relatively well-modeled
The same trend is observed also in the di-leptonic channel. Fig. \ref{fig::BGestimation::DataMCPresel2LBT1}-\ref{fig::BGestimation::DataMCPresel2LBT2} plot the kinamtic distributions in the 2-lepton b-tagged preselection region (\textbf{2LBT}), and constant slopes in data/MC are seen in jet transverse momenta and $\meffInc$ distributions.
It might worth noting that the slope in jet transverse momenta and $\meffInc$ can also be corrected by the same reweighting function Eq. \ref{eq::BGestimation::rwgt_ttPt} as the semi-leptonic case. Fig. \ref{fig::BGestimation::DataMCPresel2LBT_rwgt1}-\ref{fig::BGestimation::DataMCPresel2LBT_rwgt2} show the distributions with the reweighting applied, where the data-MC descrepancy related to jet kinematics are fairly recovered, and the lepton transver momentum enjoys much poorer restoration. 
This universality strongly implies that the cause of mis-modeling in $\ttbar$ is highly likely in the kinematics before the W-bosons decay, which is an important underlying assumption of the object replacement method to be described later. \\
%One finds the residual mis-modeling after the reweighting on the lepton transverse momentum is larger than the semi-leptonic 
%One thing remarkable about this correction is that the same coefficiencies seems to be also optimal for the di-leptonic $\ttbar$. Di-leptonic pre-selection region is also available for testing the $\ttbar$ modeling. 


\clearpage
\input{tex/BGestimation/fig_DataMCPresel2LBT.tex}
\input{tex/BGestimation/fig_DataMCPresel2LBT_rwgt.tex}
\clearpage

In constrast to variables that scale with transverse momenta of outgoing particles, $\mt$ and aplanarity look relatively well-modeled among ones used in signal regions definition. Therefore, the same estimation strategy is taken as the case of $\wjets$, which is namely extrapolating in these variabels from control regions to signal regions. However, note that the modeling of $\mt$ is not as perfect as the case in $\wjets$. While the tail in the $\mt$ distribution of $\wjets$ is only determined by jet energy reslution and the mass-line of W-boson, that of $\ttbar$ is dominated by the di-leptonic component whose $\mt$ simply scales with lepton transverse momentum and MET without any cut-off structure. As a result, the $\mt$ distribution of di-leptonic $\ttbar$ is affected by the mis-modeling rather more severely than the semi-leptonic component in general. The emerged data/MC discrepancy can be seen in Fig. \ref{fig::BGestimation::DataMCPresel2LBT2} (c). \\

To avoid the impact by the mis-modeling in $\mt$, in this analysis di-leptonic components are decided to be estimated by the other ``object replacement'' method as much as possible, and only small portion (``Out Acc.'' and ``Mis. OR'' in Tab. \ref{tab::BGestimation::BGclass}) of them is covered by the kinematical extrapolation. \\



%%%%%%%%%%%%%
Modeling of $tt+cc/bb$ and $\ttbarFakeB$ are exclusively examined using a preselected region with 3 or more b-jets (\textbf{1L3B}). Fig \ref{fig::BGestimation::DataMCPresel3B_rwgt1} - \ref{fig::BGestimation::DataMCPresel3B_rwgt2} displays the data-vs-MC comparison in the region. While the shapes seem to be affected by the same type of mis-modeling as observed in inclusive $\ttbar$ selection above, the normalization is also underestimated by about $20\%$ which is thought to be due to the modeling error of $\ttbar+cc/bb$ cross-section. \\

Reweighting function
\begin{align}
w = 1.4 \times \left[ 1 - 0.061 \,\times p_T(\ttbar) \right] \label{eq::BGestimation::rwgt_ttPt3B},
\end{align}
is found to well correct the descrepancy, in which only the normalization coefficiency varies from Eq. \ref{eq::BGestimation::rwgt_ttPt} accounting for the cross-section correctioin for $tt+cc/bb$. Meanwhile, the invariance of the slope coefficiency implies that the source of the shape mis-modeling is common to the bulk $\ttbar$ component as seen above. \\

Despite the $\ttbar$ components in 3B regions suffer from such even more complex mis-modeling than the bulk, the impact on the final result is not dramatic since the majority of them are di-leptonic components in the signal regions which can be estimated largely by the object replacement method.


\input{tex/BGestimation/fig_DataMCPresel3B.tex}
\input{tex/BGestimation/fig_DataMCPresel3B_rwgt.tex}
\clearpage


%\subsubsection{Dibosons}
%\input{tex/BGestimation/fig_DataMCPresel2LDB.tex}
%\input{tex/BGestimation/fig_DataMCPresel2LDB_rwgt.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Kinematical Extrapolation Method} \label{sec::BGestimation::kineExtp}
The most straightforward solution for the MC mis-modeling is to apply correction in control regions (CR) by normalizing the MC to the data there, and use the corrected MC to derive the background prediction in SRs. As this is essentially extrapolating MC in kinematics, this will be referred as ``kinematical extrapolation method'' in this thesis. \\ 

\subsubsection{Definition of Control Regions and Validation Regions}
The key assumption in this method is that the relative modeling of MC between CRs and SRs are correct. In other words, both CRs and SRs suffer from the same mis-modeling, so that the normalization factor measured in CR is applicable to SR. Therefore, the most important requirement in CR definition is having the similar phase space with respect to corresponding SR in terms of the mis-modeling. \\

The easiest relization of CR is to revert the SR cuts in kinematical variables that are well-modeled by MC. In this analysis, $\mt$, $\apl$ and topness (and also $\mindPhiFourJet$ for the ``3B'' tower) are chosen as the baseline extrapolation variables. 
A exception is in the``2J'' tower where $\met$ is used instead of aplanarity, since it is not used in the signal regions selection.
%since aplanarity is not used in definition. 
%The modeling of $\met$ is acceptable, since the ISR/FSR conribution in the ``2J'' tower is relatively low due to the loose selection in number of jets. \\ \\
%topness, min_dPhi_4j

A couple of minor modefications follow based on following supplemental requirements:
\begin{itemize}
\item CR statistics have to be sufficient. \\
Typically, about 10 times more data statistics in CRs with respect to SRs are desired to make the correction stable particularly in cases where multiple components are corrected simultaneously (in this analysis, $\wjets$ and $\ttbar+Wt$). 
For this sake, cuts in variables fatally sensitive to the mis-modeling is loosened in some of the CRs, even at some cost of being hit by the mis-modeling.
MET is for example always a good candidate to loosen since the gain in statitcs increase is large.
Although it is affected by the mis-modeling through jet transverse momenta which is known to be the most ill-modelied, 
the influence is much diluted through the vectoral summation of them, instead of the scalar sum. 
$\metOverMeff$ is also loosened in ``2J'' and ``High-x'' since it is in a form of ratio which is supposed to be robust against simultaneous variation of the enumerator and the denominator.
The impact by the mis-modeling due to these loosened cuts are evaluated in Sec \ref{sec::BGestimation::nonClosure_kineExtp}.
On the other hand, it is promissed that $\nJet$ and $\meffInc$ are never touched since they are critical to the mis-modeling. \\
%
\item Lower cut in $\mt$ to reduce the contribution from fake leptons. \\
Low-$\mt$ regions are typically have higher abandunce of events with fake leptons for $\wjets$ and $\ttbar$.
As the MC modeling on the fake rate is generally less reliable, $\mt > 30 \sim 40 \gev$ is applied in CR to get rid of the influence. 
\end{itemize}

CRs are defined for each tower and $\meffInc$ bins independently, however are shared between b-tagged and b-vetoed SR bins.  Normalization is applied only on $\wjets$, $\ttbar$ and single-top while raw MC prediction is quoted for diboson and the other minor backgrounds. $\ttbar$ and single-top share the normalization factors as their relative breakdown is similar in CRs and SRs. 
%definition of ``tops''
The normalization factors are determined by a simultaneous fit on the b-vetoed and b-tagged slice of a CR (``WR'' and ``TR'') in which $\wjets$ and $\ttbar$ is dominant respectively. During the fit, all the normalization factors and nuisance parameters characterizing theoretical and experimental systematics are allowed to flow. The detail of the statistical procedure is described in Sec. \ref{sec::Uncertainties::statistics}. \\

There are the third type of regions referred as ``validation regions'' designed to confirm the validity of the background estimation procedure by comparing with the data. They are typically set in between the CR and SR, with the cut in one of the extrapolation variable is freed with respect to CRs and kept for the other one. VRa and VRb respectively validates the extrapolation in $\mt$ and $\apl$ ($\met$ for ``2J''). Upper cut on $\mt$ is set in some VRa to suppress the signal contamination. VRs-QCD are the regions to examine the contribution from QCD multi-jet processes in SRs which is supposedly negligible. The detail is found in Sec. \ref{sec::BGestimation::VRQCD}. \\

The finalized CRs and VRs are summarized together with the correponding SRs in Tab. \ref{SRdefinition::regionDef2J} - \ref{SRdefinition::regionDef3B}, with the graphical schematics being shown in Fig. \ref{fig::BGestimation::regionsPlot}. While SRs are carefully designed to be orthogonal to CRs and VRs, it is allowed to have overlap between CRs and VRs once the CRs are found to have much larger statistics than that of the VRs so that the overlapped events have no influence to the normalization. For instance, CR adn VRa are overlapped in ``3B''. This is intended to secure the CR statistics, while the number of events in VRa is small enough so that they are still nearly statitically independent. \\
\clearpage

%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/regionsPlot/regionsPlot_myAna_2J.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/regionsPlot/regionsPlot_myAna_6J.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/regionsPlot/regionsPlot_myAna_Lowx.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/regionsPlot/regionsPlot_myAna_Highx.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/regionsPlot/regionsPlot_myAna_3B.pdf}}
 \caption{ Schematics of CR/VR/SR in each signal region tower. Two major extrapolation variables are chosen to illustrate the difference between the regions. Extrapolation in the other variables are explicitly mentioned in the label. Note that the control region in the ``3B'' tower contains the VRa in it. 
%to boost the CR statistics.
%, which does not disturb the role of validation given that it has 5 times more statistics as that of VRa. 
   \label{fig::BGestimation::regionsPlot} 
 }
\end{figure}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection{Signal contamination}
%\input{tex/BGestimation/fig_sigContami_kineExtp.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Evaluation of the Extrapolation Error} \label{sec::BGestimation::nonClosure_kineExtp}
Although the modeling on the extrapolation variables is confirmed in the pre-selection regions, 
the robustness of the extrapolation is not trivial in the phase space where the signal region selections are applid,
as long as the mechanism of the MC modeling is not fully understood. 
In fact, there is correlation between the well-modeled variables ($\mt$, $\apl$ etc.) and the ill-modeled ones ($\nJetNoGev$, jet transverse momenta, $\meffInc$ etc.) that are not evident at preselection level, however addressing in some particular phase space. 
The extrapolation is also affected by the loosened cuts in variables that are already known to be poorly modeled such as MET, so the associated uncertainty needs be quantified. \\

In this sub-section, the extrapolation error is evaluated by injecting an artificial variation in MC compared to the observed MC mis-modeling, and then meausure the yield change in a CR and the corresponding SR. Ideally, they show the same response against the injected variation, so that the normalization in CR can perfectly compensate the effect of mis-modeling in SR. Otherwise, the relative difference in their yield variation directly corresponds to the amount of extrapolation error. \\

Fig. \ref{fig::BGestimation::valid_extp_2J} - \ref{fig::BGestimation::valid_extp_3B} present the results where the $\wjets$ and $\ttbar$ MC are varied by reweighting the events with:
\begin{align}
 w & = 1 - x \times (\nJetNoGev-2), \mbox{\phantom{MMMM}}\,\,\,\,\,\, x \in [0,0.18]  \mbox{\phantom{MMMM}} (\wjets) \nn  \\
 w & = 1 - x \,\times p_T(\ttbar)/100\gev, \,\,\,\,\,\,\,\,           x \in [0,0.09]  \mbox{\phantom{MMMM}} (\ttbar),
\label{eq::BGestimation::injected_MCvariation}
\end{align}
respectively. The vertical axis on the top pannels show the amount of relative change that CR or SR experience by the injected MC variation as a function of $x$. The relative variation in CR (orange) compares to the normalization factor actually obtained via the fit to data, while that in SR (blue) to the ideal normalization factor need to fully correct the SR. The bottom pannel display the ratio, namely the resultant extrapolation error. The realistic $x$ is approximately $x_W=0.1$ and $x_{\ttbar}=0.06$ for $\wjets$ and $\ttbar$ respectively, based on the observation of data/MC in Sec. \ref{sec::BGestimation::dataMC} as Eq. (\ref{eq::BGestimation::rwgt_nJ}), (\ref{eq::BGestimation::rwgt_ttPt}) and (\ref{eq::BGestimation::rwgt_ttPt3B}). B-tagging requirement is removed to maintain sufficient statistics, assuming the kinamtics are invariant with it. For the $\ttbar$ process, component estimated by the object replacement method is excluded from the test. The extrapolation error from CRs to corresponding VRs are shown in Appendix Sec.\ref{sec::App::valid_extp_VR}. \\

Observed extrapolation error is generally small, which stays within 10$\%$ (20$\%$) for $\wjets$ ($\ttbar$) at the reference magnitude of mis-modeling ($x_W=0.1, \,\,\, x_{\ttbar}=0.06$). These are quoted as systematics error associated with the method in the fit, which is summarized in Tab. \ref{tab::Uncertainties::noClosure_kineExtp} of Sec. \ref{sec::Uncertainties::nonClosure}. \\

\clearpage
%\input{tex/BGestimation/fig_valid_extp.tex}
\input{tex/BGestimation/fig_valid_extp_SR.tex}


\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Result of Backgroun-only Fit} \label{sec::BGestimation::kineExtp::result}
The data yields in control regions are summarized in Tab \ref{tab::BGestimation::CRyields_2J} - \ref{tab::BGestimation::CRyields_3B}, accompanied with the pre-fit and post-fit prediction by MC. Note that only $\wjets$ and top backgrounds ($\ttbar$ and single-top) are normalized and the yield of other processes are kept during the fit. The effect of signal contamination in control regions is neglected therefore referred as ``background-only fit''. \\

Fitted normalization factors are summarized in Fig. \ref{fig::BGestimation::fittedSFs}. Generally they decrease with elevating $\meffInc$-bin, consistent to the data-MC observation where MC is systematically over-predict in high $\meffInc$ regions.
 
%---------------------- Fitted normalization factors ------------------- 
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=160mm]{figures/BGestimation/fittedSFs/SFs.pdf}
    \captionof{figure}{Fitted normalization factors for $\wjets$ and top backgrounds ($\ttbar$ plus single-top). The error bars represent combined systematic and statistical uncertainties. }
    \label{fig::BGestimation::fittedSFs}
  \end{center}
\end{figure}
%-------------------------------

\clearpage
\input{tex/BGestimation/tab_CRyields.tex}
\clearpage


%%%%%%%%%%%%%%%%a%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Object Replacement Method}
\subsubsection{Overview}
A potential concern over the kinematical extrapolation method is that it is still fully relies on MC in the extrapolation. In particular, in case of estimating the ``di-leptonic'' background by extrapolating in $\mt$, this follows that:

\begin{itemize}
\item The MC modeling itself is questionable. \\
As observed in Fig. \ref{fig::BGestimation::DataMCPresel2LBT2} (c), MC tends to be overestimating in the tail, reflecting the fact that in case of di-leptonic channel, $\mt$ scales with the lepton transverse momentum and MET in which MC is found to be is-modeling. \\

\item Different particles contribute to observables between the semi-leptonic and di-leptonic processes. For instance, MET is sourced by a single neutrino in the semi-leptonic channel while it is by a vectoral sum of two neutrinos in the di-leptonic one. More seriously, the number of ISR/FRS jets is different under the same jet multiplicity. For example in $\ttbar$, the semi-leptonic channel yields 4 jets by its decay while the di-lepnic channel can only lead to 2 (or 3 if hadronic decay product from $\tau$ is tagged as a jet). The differences are summarized in Tab. \ref{tab::BGestimation::1L2LkineComp}. Note that these differences also propagate to the other composite variables using jets and MET (e.g. $\meffInc$ and $\mt$ etc.). Therefore, applying the same selection between CRs and SRs no longer guarantee that CRs grasp the same phase space as SRs. 
\end{itemize}

%As long as $\ttbar$ is concern, there is no striking difference found in the behavior in terms of data/MC between the semi-leptonic and di-leptonic component, but conceptually this is more conceptual

\tab{c|c|c|c}
{
\hline
                            &  SR                                                            & 1L CR                            & 2LCR \\
\hline
Dominant $\ttbar$ component &  $\ttbar \ra b\ell\nu_1 b \tau \nu_2, \tau\ra\tau_h\nu_{\tau}$  & $\ttbar \ra bqq b\ell \nu$       & $\ttbar \ra b\ell \nu_1 b\ell\nu_2$ \\
\hline
$\nJetNoGev$                &  $\sim 2(3) + n_{\mathrm{ISR/FSR}}$                                 & $\sim 4 + n_{\mathrm{ISR/FSR}}$  & $\sim 2 + n_{\mathrm{ISR/FSR}}$     \\
$\met$                      &  $|\bpt(\nu_1)+\bpt(\nu_2)+\bpt(\nu_\tau)|$                      & $|\bpt(\nu)|$                    & $|\bpt(\nu_1)+\bpt(\nu_2)|$          \\
\hline
}
{Comparison of constituents of MET and $\nJetNoGev$ between the semi-leptonic $\ttbar$ and di-leptonic $\ttbar$ as example. ``1LCR'' refers to the control regions used in the kinematical extrapolaton method, and ``2LCR'' is its 2-lepton version with the same kinematical selection. Note that the other composite variables using jet and MET (e.g. $\meffInc$ and $\mt$ etc.) are also affected by the difference accordingly.
}
{tab::BGestimation::1L2LkineComp}

The use of 2-lepton control regions (2LCRs) is then natually motivated. However, the region-based approach where CRs apply similar kinematical selections with respected to SRs does not dramatically improve the situation, since a fixed region can not express the behavior of taus or missing leptons that differ event-by-event, as schematized in Tab. \ref{tab::BGestimation::1L2LkineComp}. \\

Instead, the method of event-by-event emulation, introduced in this sub-section referred as ``object replacement method'', can perfectly accommodate the problem. 
It is an integrated method consisting of:
\begin{itemize}
\item "missing lepton replacement" to estimate a part of $\ell\ell_{\mathrm{mis.}}$ events ("Mis. Reco." and "Mis. ID"),
\item "tau replacement" to estimate $\ell\tau_{\mathrm{h}}$,
\end{itemize}
where one of the lepton of data events in 2LCR is replaced into a virtual missing lepton or a simulated hadronic tau decay respectively, as outlined in Fig \ref{fig::BGestimation::objRep::schematic1}. 
The detector responses and behavior in object reconstruction of those replaced objets are carefully emulated so that the replaced event can directly mimic the events in the signal regions.   \\
%Therefore, there is no extrapolation in kinematics anymore but only in objects, and the modeling of kinematic tail can be fully deiven by data.


The object replacement method is a nealy full data-deiven method, since the used of MC is limited in an area of tau decays and modeling of instrumental effects, such as lepton efficiency and jet energy scale.
The MC modeling is highly reliable where the data/MC agreement is closely examined and the discrepancies are typically sub-percent level which are also mostly well-understood. 
The reliance of MC ensures the extrapolation much more robust, compared with the kinamatical extrapolation method where the mis-modeling in kinematic tail is always critical. \\


Note that the whole method relies on the orthogonality between kinematics and object properties:
\begin{align}
  & \frac{d\sigma(\ell\ell)}{d\bm{x}}
  \propto \frac{d\sigma(\ell\ell_{\mathrm{ID}})}{d\bm{x}} 
  \propto \frac{d\sigma(\ell\ell_{\mathrm{mis.}})}{d\bm{x}} 
  \label{eq::BGestimation::objRep::orthKineObj}
\end{align}
and the lepton universality:
\begin{align}
  & \frac{d\sigma(\ell\ell)}{d\bm{x}}
  \propto \frac{d\sigma(\ell\tau)}{d\bm{x}},
  \label{eq::BGestimation::objRep::lepUniv}
\end{align}
where $\ell\ell_{\mathrm{ID}}$ and $\ell\ell_{\mathrm{mis-ID}}$ represents the seed events and the missing lepton events repectively, and $\bm{x}$ symbolises kinematical variables. 
Particularly, the kinematics-object orthogonality (Eq. \ref{eq::BGestimation::objRep::orthKineObj}) is of paramount importance, since it allows to extrapolate the object properties measured in a very inclusive phase space into any phase space including extreme cases such as the signal regions in this analyis. As long as the lepton reconstruction and identification is concerned, the statement is more or less true
%Though with some minor exceptions, this statement is highly robust when 
because their result generally obeys the statistical behavior of detector responses such as fluctuating number of hits or energy deposit, which does not depend on global event kinematics, but rather on the nature of the particle itself (usually only on its momentum) as well as the local material configuration in the detector. Therefore, it is usually enough to paramerize the efficiency of reconstruction or identification simply by the momentum ($\pt, \eta, \phi$) of the particles. This is however not the case when coming to the probability of lepton being beyond the $\pt-\eta$ acceptance (``Out-Acc'') or being dropped in the overlap removal (``Mis. OR''), since they do depend on the momentum of parent particle or the proximity to the nearest jet. Hence, the class of seed events do not fully represent the kinematics of ``Out-Acc'' and ``Mis. OR''. This is the reason why these events can not covered by the object repalcement method. \\

% -------------- schematic
\includegraphics[width=160mm]{figures/BGestimation/ObjReplacement/method/schematic_replacement.eps}
\captionof{figure}{Schematic of the object replacement method.}
\label{fig::BGestimation::objRep::schematic1}
% --------------

%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsubsection{The Replacement Procedure and the Per-event Logic} \label{sec::BGestimation::objRep::perEvtLogic}
Fig \ref{fig::BGestimation::objRep::perEvtLogic} presents the work flow for the replacement procedure in a single seed event, which follows as below steps. \\

\begin{enumerate}
\item Pick up a 2LCR event ("seed event").
\item Replace a lepton of the seed event into a virtual missing lepton or a simulated hadronic decay of tau lepton, if the two leptons satisfy a certain criteria. This replaced event is called "sub-event".
\item (For tau replacement) Apply the calibration for the hadronic tau.
\item Re-calculate the event-level kinematics such as $\met$ or $\meffInc$ etc.
\item Assign a weight $\kappa$ for each sub-event as the transfer factor from 2LCR to 1L regions.
\item Change the roles (tagged/replaced) between the two leptons and repeat 2-5. Generated sub-events are filled in a single ``event-level histogram''.
\item (For tau replacement) Repeat the step 2-6 above by $N=50$ times and take the average, in order to fully accommodate the statistical nature of tau decay. Note that the number of iteration $N$ only defines the level of ``smoothing'' thus has no essential impact on the final result. The average is taken by scaling the $\kappa$ by 1/N.

\item Apply the analysis level selection (e.g. signal region selection when one wants to estimate the yield in the signal region) and the post-selection (discussed below) on the generated sub-events. 
\item Collect the accepted sub-events and fill them into an event-lebel histogram. 100$\%$ of statistical uncertainty is assigned for each bin of the event-level histogram, accounting for all the sub-events are generated from the common seed event.
\item Loop over all seed event and sum up all the event-level histograms with ordinary statistical treatment where the uncertainty is quadrutically summed for each bin of the histogram. 
\end{enumerate}

% -------------- perEvtLogic
\fig[160]{BGestimation/ObjReplacement/method/flow_replacement.eps}
{Work flow in the replacement procedure for a single seed event.}
{fig::BGestimation::objRep::perEvtLogic}
%-------------------------------

More detail and caveats about each step are as following:  \\

\noindent \textbf{Seed event selection and trigger} \\
For seed event selection, looser kinematical selection is generally preferred, to collect the seed events necessary for estimating the regions in interest as completely as possible. In particular, as MET and $\mt$ change their values the most during the replacement, those cuts have to be drastically relaxed with respective to signal regions. For instance, Fig. \ref{fig::BGestimation::seedMET} shows the MET distribution for corresponding seed events of the $\llmis$ and $\ltauh$ events with $\met>250\gev$. About $40\%$ of seeds are with seed MET below $250\gev$, meaning that it will be underestimated by $40\%$ if naively selecting seeds by $\met>250\gev$ in 2LCR. \\ 

While MET trigger is available for collecting the bulk events above its off-line threshold $\met>250\gev$, the single-lepton trigger (SLT) is introduced to complement the seeds events with $\met<250\gev$.
%In order to account for the different triggers efficiency with respect to the MET trigger that is used in the signal regions definition, the inversed trigger efficiency (Fig. \ref{fig::BGestimation::objRep::SLTeff}) is applied for events triggered by SLT, assuming the MET trigger is $100\%$ efficient. 
In spite of its relatively low efficiency $70\%-90\%$ and the off-line threshold of $\pt > 28 (26) \gev$ for single-electron (muon), SLT is still fully efficient for the seed events since there are two leptons being the candidate to fire the trigger. Eventually, as shown in Fig. \ref{fig::BGestimation::seedMET}, more than $95\%$ of the overall trigger efficiency can be maintained. \\

%%%%%%%%%%
\fig[110]{BGestimation/ObjReplacement/method/seed_collection/met_seed_trig.pdf}
{Seed MET distribution (gray) for the $\llmis$/$\ltauh$ events from $\ttbar$ resulting in $\met>250$. The seed MET is defined by the MET component only by neutrinos from top decays: $\left|\bpt(\nu_1)+\bpt(\nu_2)\right|$, which is roughly equivalent to the MET in corresponding seed events ($\ttbar \ra b\ell\nu_1 \bar{b}\ell\nu_2$). Over $95\%$ of the seed events are shown to be accepted by the combined trigger strategy defined in Tab. \ref{tab::BGestimation::objRep::def2LCR} (pink). }
{fig::BGestimation::seedMET}
%%%%%%%%%%

Although the enhanced backgrounds due to the lowered MET selection for 2LCR does not impact as much on the final result since most of them are skimmed out in the analysis-level selection applied after the replacement, the decent cut $\met>100$ is required to suppress the bulk background components in 2LCR (Z+jets, 1L+fake lepton etc.) and make sure avoiding the large uncertainty from MC subtraction. The seed event loss due to the selection $\met>100$ is negligible when estimating SRs/VRs.  \\

Tab. \ref{tab::BGestimation::objRep::def2LCR} shows the definition of common 2LCR, and Fig. \ref{fig::BGestimation::objRep::2LCR} are the kinamtic distributions of MC overlayed with data in the region. 

% ------------- 
\begin{table}[h]
  \begin{center}
    \caption{Definition of 2-lepton control region for MC closure test.}
    \begin{tabular}{ c }
      \hline
       $n_{\ell, \mathrm{baseline}}=2$, $n_{\ell, \mathrm{signal}}\ge 1$ \\
      \hline
       MET trigger,  $\met>250\gev$   \\
       or \\
       At least 1 signal lepton with $\pt>28 \gev$ firing the single-lepton trigger, $\met>100\gev$   \\
      \hline
    \end{tabular}  \label{tab::BGestimation::objRep::def2LCR}
  \end{center}
\end{table}
% ------------- 

\clearpage
% -------------- 2LCR
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/ObjReplacement/2LCR/met__2LCR_common_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/ObjReplacement/2LCR/nJet30__2LCR_common_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/ObjReplacement/2LCR/lep2Pt__2LCR_common_log.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/ObjReplacement/2LCR/mll__2LCR_common_log.pdf}}
 
   \caption{ MC and data distributions of (a) $\met$, (b) jet multiplicity, (c) sub-leading lepton transverse momentum (d) the di-lepton invariant mass in the common 2LCR defined in Tab. \ref{tab::BGestimation::objRep::def2LCR}.  \label{fig::BGestimation::objRep::2LCR}}    
\end{figure}
%----------------------------------
\clearpage



\noindent \textbf{Requirement on seed leptons for the repalcement} \\
A seed event with lepton $\ell_1 \ell_2$ have two choises of the replacement namely 1) keeping $\ell_1$/replacing $\ell_2$ or 2) keeping $\ell_2$/replacing $\ell_1$. The replacement is proceeded only if the lepton to-be-replaced (``replaced lepton'') and the lepton to-be-kept (``tag lepton'') satisfy a certain condition as noted below. Note that the replacement can happen twice from the identical seed event if the both combinations (tag,rep.)=($\ell_1 \ell_2$), ($\ell_2 \ell_1$) are eligible. \\

As the tag lepton eventually corresponds to the single lepton used in the analysis in 1-lepton regions, it has to undergo the consistent object definition as that used in signal region definition, which is namely in Tab. \ref{tab::objDef::summary}. On the other hand, no such requirement is needed for the replaced lepton, instead, looser definition is preferred from the CR statistics point of view. Therefore, only the baseline lepton requirement when estimating the b-inclusive or b-tagged regions, while the signal lepton requirement is still applied in case of estimating b-vetoed regions since the impact of fake lepton background in 2LCR is relatively large otherwise. The replaiton between and the working point of lepton definition is summarized in Tab. \ref{tab::BGestimation::objRep::seedLepReq}. \\ 

\tab{ c|cc }
{
\hline
            & B-tagged, b-inclusive  &  b-vetoed \\ 
\hline
\hline
Tag lepton  & signal                 & signal \\
Replaced lepton  & baseline                 & signal \\
\hline
}
{Lepton definition used for tag and replaced lepton versus the type of regions to be estimated.}
{tab::BGestimation::objRep::seedLepReq}


\noindent \textbf{Treatment of virtual missing lepton} \\
As mentioned in Sec. \ref{sec:objDef::OR}, electrons are usually also indentified as jets, and the doubly-counted object, either an electron or a jet, is discarded during the overlap removal. Therefore, electrons failing the reconstruction or identification will simply recognized as jets without experiencing the overlap removal. 
To emulate this effect, in case of replacing an electron in a seed event, the record that the electron is reconstructed as a jet candidate is retrieved, and the 4-vector of electron is replaced into the that of the jet candidate. As the jet candidate is fully calibrated in the hadronic scale, no more correction is needed. In some occasion, electrons do not have corresponding jet candidates typcally when the low transverse momentum is too low. In such cases, the electron is replaced into a missing particle with the 4-momentum of original electron. \\ 

Muons failing the reconstruction or identification are almost never identified as any other objects. Instead, they are included in the MET track soft term in the MET calculation, and in principle this needs to be emulated in the missing muon replacement. This is technically possible, however the bottleneck is that the muon track quality is totally different between well-identified muons and unidentified ones, and particularly it is diffucult to reproduce the resolution of bad muon track from good one with a meaningful correction. As it turns that simply including the 4-momentum replaced muon into the MET soft term even leads to worse performance than not including at all (as demonstrated in Fig. \ref{fig::BGestimation::objRep::mcClosure::metSoftTerm_mu}), replaced muons are decided to be simply treated as a virtual missing particle in the same momentum, and added in MET.
Although this rough treatment causes a non-zero error in the estimation as one will see in Sec. \ref{sec::BGestimation::objRep::NonClosure}, fortunately the impact on final estimation is marginal because the rate of missing muon events are generally very low, compared with the other components (missing electron events or $\ltauh$) due to the very high efficiency of muon reconstruction and identification. \\


\noindent \textbf{Simulation of tau decays and the $\tau_h$-to-jet calibration } \\
Tau decays are simulated by TAUOLA \cite{TAUOLA1} \cite{TAUOLA2} \cite{TAUOLA3} assuming the taus are unpolarized. This assumption is incorrect given the parent W-bosons are left-handed, however the impact on the final result is found to be marginal. This is discussed in Sec. \ref{sec::BGestimation::objRep::NonClosure}. Branching for leptonic decay is set to zero to reduce the number of loops. \\

Given that the analysis is without explicit tau selections, hadronic taus within the $p_{\mathrm{T}}$-$\eta$ acceptance undergo the reconstruction, b-tagging and calibration as an (b-tagged) anti-Kt4 jets, once they pass the JVT cut. On the other hand, the output of TAUOLA is merely a 4-vector of truth level hadronic tau. Therefore, following pseudo-calibration is applied for the truth-level $\tau_h$, to emulate the effect either of the detector response, jet calibration, and the b-tagging.\\

\begin{enumerate}
\item Scale the transverse momentum of truth $\tau_h$. \\
The scale of a truth $\tau_h$ to an anti-Kt4 jets is derived using the $t\bar{t}$ MC samples, 
by comparing the transverse momenta of truth hadronic taus and that of $\Delta R$-matched reconstructed jet by $\Delta R<0.2$
. It is defined by the mean value of the residual distribution (Fig. \ref{fig::BGestimation::objRep::tau_ptResidual}) and parametrized in terms of $p_{\mathrm{T}}$ and $\eta$ of truth hadronic taus (Fig. \ref{fig::BGestimation::objRep::tau_scale}). The scale is always positive and rises significantly in the low-$\pt$ limit, due to the fact that the anti-Kt4 jet contains extra underlying tracks inside that become the pedestal. The difference in the calibration between light jets and b-tagged jets are ignored. \\


\item Smear the $p_{\mathrm{T}}$ of hadronic tau. \\
After applying the scale above, smearing is subsequently adopted for to account for the detector resolution.
The resolution is taken from the Gaussian-fitted RMS of the residual distribution on which the scale above is defined as well (Fig. \ref{fig::BGestimation::objRep::tau_ptResidual}), and likewise parametrized as function of $p_{\mathrm{T}}$ and $\eta$ of truth hadronic taus (Fig. \ref{fig::BGestimation::objRep::tau_resol}). The smearing is applied based on the Gaussian profile centered at with RMS being the resolution. \\


\item Emulation on the JVT cut and b-tagging.  \\
After the sequence of the $\pt$-scaling and smearing, hadronic taus with $p_{\mathrm{T}}>30\gev$, $|\eta|<2.8$ are selected as the signal jet candidates.
Signal jets are then randomly identified from them, based on the efficiency of JVT cut derived from signal jet candidates matched with truth hadronic taus by $\Delta R<0.2$ in the simulated $\ttbar$ sample (Fig. \ref{fig::BGestimation::objRep::effJVT}). 
% tau to fake-bは結構あることをここでいう
A random b-tagging is further performed on the signal jets, by assigning a random b-tagging score (MV2c10) following according to the profile obtained from the $\ttbar$ MC sample using the same technique (Fig. \ref{fig::BGestimation::objRep::tau_bTagScore}). While the JVT cut efficiency is mapped as a function of $p_{\mathrm{T}}$ and $\eta$ of signal jet candidates, the b-tagging score profile is measured separately by different tau decay modes (1-prong and 3-prong).
\end{enumerate}

% ----------------- tauRF
\clearpage
\begin{figure}[htbp]
  \begin{center}
      \includegraphics[width=100mm]{figures/BGestimation/ObjReplacement/method/tauRF/ptResidual.eps} 
      \caption{The residual of tau momentum measurement: $\pt(\mathrm{reco. } \tau\mathrm{-jet})-\pt(\mathrm{tr. } \tau_{h})$/$\pt(\mathrm{tr.} \tau_{h})$ calculated using the simulated $\ttbar$ sample. $\pt(\mathrm{tr.} \tau_{h})$ is the transverse momentum of truth-level hadronic tau defined as $|\bm{p}(\tau)-\bm{p}(\nu_{\tau})|$ and $\pt(\mathrm{reco.} \tau-\mathrm{jet})$ is the corresponding reconstructed anti-Kt4 jet matched by $\Delta R<0.2$. }
      \label{fig::BGestimation::objRep::tau_ptResidual}
    %
    \begin{minipage}[t]{.45\textwidth}
      \centering        
      \includegraphics[width=85mm]{figures/BGestimation/ObjReplacement/method/tauRF/tauJet_scale.eps} 
      \caption{Scale of anti-Kt4 jets for truth hadronic taus, defined as the mean of the residual distribution \ref{fig::BGestimation::objRep::tau_ptResidual}. Both $\eta$-inclusive and $\eta$-dependent curves are derived.}
      \label{fig::BGestimation::objRep::tau_scale}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=85mm]{figures/BGestimation/ObjReplacement/method/tauRF/tauJet_resol.eps}
      \caption{Resolution of hadronic tau, defined by the Gaussian-fitted RMS of the residual distribution \ref{fig::BGestimation::objRep::tau_ptResidual}. Both $\eta$-inclusive and $\eta$-dependent curves are derived.}
      \label{fig::BGestimation::objRep::tau_resol}
    \end{minipage}
    %              
  \end{center}  
\end{figure}
\clearpage
\begin{figure}[htbp]
  \begin{center}
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=85mm]{figures/BGestimation/ObjReplacement/method/tauRF/heff_vs_recoJetPt_recoJetEta.eps}
      \caption{The JVT cut efficiency map for a reconstructed hadronic tau jet as function of its $p_{\mathrm{T}}$ and $\eta$, calculated using the $\ttbar$ MC sample. The efficiency is defined by the fraction of signal jet candidates $\Delta R$-matched to the truth hadronic tau by $\Delta R<0.2$ that pass the signal jet requirement. }
      \label{fig::BGestimation::objRep::effJVT}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=75mm]{figures/BGestimation/ObjReplacement/method/tauRF/hadTau_bTagScore.eps}
      \caption{Profile of b-tagging score (MV2c10) for signal jets originated from hadronic tau decays. Only the dependency on the decay modes (1-prong or 3-prong) is taken into account. The threshold for the b-tagging is at 0.44. 
The 3-prong events result in a higher fake rate into b-tagging jets, reflecting the secondary vertex stucture more resembling to that of b-hadrons. The simulated $\ttbar$ MC sample is used to derive the profiles. }
      \label{fig::BGestimation::objRep::tau_bTagScore}
    \end{minipage}
    %
  \end{center}
\end{figure}
%% ------------------------------------------------------------------------

\if 0
\noindent \textbf{Post selections and the signal contamination} \\
Generated sub-events experience the kinematical selections of the region in interest.
In addition to it, the cleaning cut is applied in order to reduce the signal contamination in 2LCR. 
% 
% The impact of signal contamination is generally not negligible since there are a class of target models yielding 3-4 W-boson and comparable 2L branching as 1L.
% BGと同じように1Lに入ったdi-leptonicなsignal eventsの量が推定される. まず断っておきたいのは, このcontamiによるexp. BGのincreaseは1Lのsignalの量に比べれば大した量にはならない（3-10倍いる）ので発見への影響はほとんどない。ただsignal regionとかの推定をしたい場合、SRでのexp. BGは数発とかいうレベルになるのでBGに比べたらnon-negligibleになることがしばしばあり、exclに関しては実際injected signal hypothesisに大してきちんとresponseが計算できるなら問題ではないが、signalのmodeling uncertaintyなどの影響も入ってくるので一般的に抑えられるなら抑えた方がいい。

%BGだけ残してsignalが普遍的に消えるような有効なcleaning cutはあるか？そこでここでの求めたいstandard model componentはほとんどWが二つ出るようなイベントで、WW\ra lvlvのようなevent topologyでは2つのMTが両方mW以上になることはほとんどないことに注目する。つまりVR/SRといったmt(tagLep,MET)>mWを要求している場合では、mt(repLep,MET)はmWであることがほとんどである。一方でSUSY signal全般としての性質としてsemi-leptonic decaying W以外にもMET sourceがあるためmTがmWでtruncateすることはない。なのでmtRepのupper cutは普遍的にeffectiveである。signal regionにおけるBG expに対するcontaminationの影響が30\%以下になるようupper cutを選んだ結果表のようになった。
%このcleaning cutで削れた分はMCで見積もったinverse eff.をかけてconpensateすることにした。
%MCによるとSR BVで最大でexp. BGが20%減るようなところもあるが、そういったところそもそもobj Repで求める数が少ないので問題ない。
%resultant signal contaminationは図~の通り。依然として場所によっては30-50%くらいいってしまうところもあるが、contamiが多いところは基本的にsignal regionに入る量も圧倒的に多いので、5 sigmal->4sigmaとかになるようなことしか起こらない。一方で微妙な感度を持ってるようなsignal pointはcontamiも少ないため感度に影響はない。なのでこれでokということにする。

%Post-selection imposed on replaced sub-events in estimating regions in each tower. Isolation requirement for the BV regions is intended to further reject the backgrounds in 2LCR which is mainly $\wjets/\ttbar$ accompanied with 1 fake lepton. The upper cut in $\mtRep$ are meant to prevent the impact of signal contamination. Upper cut in MET is additionally needed for the 3B tower to cope with higher expected signal contamination, providing the targeting signatures generally involve more top quarks with higher lepton multiplicity which leads to higher acceptance in 2-lepton regions. 
%are summarized in \ref{tab::BGestimation::objRep::postSelection}.

%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/ObjReplacement/method/dist_sigContami/dist_sigContami_SR6JMEFFIncl_mt_repLep.pdf}}
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/BGestimation/ObjReplacement/method/dist_sigContami/dist_sigContami_SR3BMEFFIncl_mt_repLep.pdf}}
 \caption{ 
%   $\mtRep$ for sub-events passing the (a) SR 6J $\meffInc$-inclusive selection, and (b) SR 3B $\meffInc$-inclusive selection.
   \label{fig::BGestimation::objRep::mtRep} 
 }
\end{figure}
%%%%%

\tab{ c|c|c }{
  \hline
  Region                                &  $\mtRep$ [GeV]  &  Efficiency        \\
  \hline
  \hline
  SR 2J $\meffInc^{\mathrm{bin} 1}$ BV  &  $<250$          &    0.96            \\
  SR 2J $\meffInc^{\mathrm{bin} 1}$ BT  &                  &    0.99            \\
  \hline
  SR 2J $\meffInc^{\mathrm{bin} 2}$ BV  &  $<250$          &    0.97            \\
  SR 2J $\meffInc^{\mathrm{bin} 2}$ BT  &                  &    0.98            \\
  \hline
  SR 2J $\meffInc^{\mathrm{bin} 3}$ BV  &  $<250$          &    0.96            \\
  SR 2J $\meffInc^{\mathrm{bin} 3}$ BT  &                  &    0.98            \\
  \hline
  \hline
  SR 6J $\meffInc^{\mathrm{bin} 1}$ BV  &  $<250$          &    0.93            \\
  SR 6J $\meffInc^{\mathrm{bin} 1}$ BT  &                  &    0.99            \\
  \hline
  SR 6J $\meffInc^{\mathrm{bin} 2}$ BV  &  $<200$          &    0.90            \\
  SR 6J $\meffInc^{\mathrm{bin} 2}$ BT  &                  &    0.94            \\
  \hline
  SR 6J $\meffInc^{\mathrm{bin} 3}$ BV  &  $<200$          &    0.80            \\
  SR 6J $\meffInc^{\mathrm{bin} 3}$ BT  &                  &    0.92            \\
  \hline
  \hline
  SR Low-x BV                           &  $<250$          &     0.94           \\
  SR Low-x BT                           &                  &     0.97           \\
  \hline
  \hline
  SR High-x BV                          &  $<250$          &     0.89           \\
  SR High-x BT                          &                  &     0.98           \\
  \hline
  \hline
  SR 3B $\meffInc^{\mathrm{bin} 1}$     &  $<250$          &     0.97           \\
  \hline
  SR 3B $\meffInc^{\mathrm{bin} 2}$     &  $<250$          &     0.87           \\
  \hline
  \hline
  VRa                                    &  $<300$         &     $>0.97$    \\
  VRb                                    &  $-$            &     1    \\
  \hline
}
{}
{tab::BGestimation::objRep::postSelection}
\fi

%%----------------------------------------------------

\noindent \textbf{Transfer factor} \\
A weight $\kappa$ is assigned to each sub-event, to account for the different probability of occurance between the seed event and the replaced sub-event. For instance, in the missing lepton replacement, this corresponds to the difference between probability of a lepton being identified and being failing the identification. The $\kappa$ is therefore the inefficiency over the efficiency:
$$
\kappa = \frac{
  1-\epsilon_{\mathrm{baseline}}  (\bpt(\ell_{\mathrm{rep.}}))
}{
  \epsilon_{\mathrm{rep.}} (\bpt(\ell_{\mathrm{rep.}}))
}.
$$
Note that the efficiency appearing in the enumerator is for the working point used for the second lepton veto (namely ``baseline''), and that in the denominator is for one used for requiring replaced lepton which can be either ``baseline'' and ``signal'' depending on cases (see Tab. \ref{tab::BGestimation::objRep::seedLepReq}). \\

As for the tau replacement, the transfer factor is 
$$
\kappa = \frac{
  \mathrm{Br}(\tau\ra\tau_{\mathrm{h}}\nu)
}{
  2N\epsilon_{\mathrm{rep.}}(\ell_{\mathrm{rep.}})
},
$$
where N is number of iterations per replacement, and $\epsilon_{\mathrm{rep.}}$ the efficiency for working point used for requiring replaced lepton. The factor 2 originates from the fact that two channels ($e\ell$ and $\mu\ell$) are available as seeds for estimating a single channel $\tau\ell$ (see Tab. \ref{tab::BGestimation::objRep::seedRepFlavor}). \\

Letting $N_{\mathrm{acc.}}$ the typical number of accepted sub-events after the post selection, $N_{\mathrm{acc.}}/\kappa$ gives a rough idea on the effective statistics in CR with respect to the SR. It is typically $5-10$ for the missing lepton replacement, and $\sim \mathrm{Br}(\tau\ra\tau_{\mathrm{h}}\nu) = 1.4$ for the tau replacement. It will be enhanced by about factor of 2 when regions $\mt>100$ are interested since $\mt(\ell_{2},\met)$ is almost never exceed $m_W$ for standard model processes thus half of the sub-events will be discarded. Therefore, the effective CR statistics is found about 3 times more than the SR statistics. This factor of 3 gain in statistics is in fact not very sufficient as it immediately leads to $20\%-50\%$ statistical uncertainty by itself in typical signal regions where only a few events are expected. Therefore CR statistic is always the biggest source of uncertainty in this method. \\


\tab{ c|c|c}
{
 \hline
 Seed   &   Estimated by mis. lep. rep.  &  Estimated by tau rep.    \\
 \hline
 \hline
 $e^+e^-$ & $e^+e^-_{\mathrm{mis}}$       & $e^+\tau^-$   \\
          & $e^+_{\mathrm{mis}}e^-$       & $\tau^+e^-$   \\
 \hline
 $e^+\mu^-$ & $e^+\mu^-_{\mathrm{mis}}$       & $e^+\tau^-$   \\
          & $e^+_{\mathrm{mis}}\mu^-$         & $\tau^+\mu^-$   \\
 \hline
 $\mu^+e^-$ & $\mu^+e^-_{\mathrm{mis}}$       & $\mu^+\tau^-$   \\
          & $\mu^+_{\mathrm{mis}}e^-$         & $\tau^+e^-$   \\
 \hline
 $\mu^+\mu^-$ & $\mu^+\mu^-_{\mathrm{mis}}$   & $\mu^+\tau^-$   \\
          & $\mu^+_{\mathrm{mis}}\mu^-$       & $\tau^+\mu^-$   \\
 \hline
}
{The flavor correspoondence between the seed events and the estimated component in case of $\ttbar$.}
{tab::BGestimation::objRep::seedRepFlavor}


\noindent \textbf{Lepton efficiency} \\
The lepton efficiency used in the trasfer factor calculation is calculated using $t\bar{t}$ MC sample as well. The efficiency of ID/baseline/singal lepton requirement is respectively defined as the fraction of truth leptons that are $\Delta R$-matched with reconstructed passing the ID / identified / signal lepton requirement by $\Delta R<0.2$. Leptons overlapped with jets (if the nearest jet closer than $\Delta<0.4$) are excluded since their efficiency is biased. The effifiencies are parametrized as a function of lepton flavor ($e/\mu$), $p_{\mathrm{T}}$ and $\eta$ of truth leptons. The data/MC scale factor measured by $Z\ra ee/\mu\mu$ are applied. The resultant efficiency maps are shown in Fig. \ref{fig::BGestimation::objRep::lep_efficiency}. \\

%% ------------------------------------------------------------------------
\begin{figure}[htbp]
  \begin{center}
    %
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=75mm]{figures/BGestimation/ObjReplacement/method/lepeff/el_trPtEta_truthToID.eps}
      \hspace{10mm} (a)
      \label{fig::BGestimation::objRep::heff_mc_el_truthToID}
    \end{minipage}
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=75mm]{figures/BGestimation/ObjReplacement/method/lepeff/el_trPtEta_truthToSig.eps}
      \hspace{10mm} (b)
      \label{fig::BGestimation::objRep::heff_mc_el_truthToSig}
    \end{minipage}
    %
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=75mm]{figures/BGestimation/ObjReplacement/method/lepeff/mu_trPtEta_truthToID.eps}
      \hspace{10mm} (c)
      \label{fig::BGestimation::objRep::heff_mc_mu_truthToID}
    \end{minipage}
    \begin{minipage}[t]{.45\textwidth}
      \centering
      \includegraphics[width=75mm]{figures/BGestimation/ObjReplacement/method/lepeff/mu_trPtEta_truthToSig.eps}
      \hspace{10mm} (d)
      \label{fig::BGestimation::objRep::heff_mc_mu_truthToSig}
    \end{minipage}
    %
    \caption{Off-line selection efficiency used in transfer factor calulation. (a) Efficiency of electrons passing reconstruction and ID. (b) Efficiency of electrons passing signal lepton requirement.  (c) Efficiency of muons passing reconstruction and ID. (d) Efficiency of muons passing signal lepton requirement.}
    \label{fig::BGestimation::objRep::lep_efficiency}
  \end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Event-level histogram and the statistical treatment} \\
Multiple sub-events are generated by both missing lepton replacement and tau replacement from a single seed event. Those passing the analysis selections are collected and filled into a common histogram, referred as ``event-level'' (this corresponds to a one-bin histogram when one only wants to estimate the yield in a particular region). To account for their full statistical correlation between the filled sub-events, $100\%$ error is then assigned to each bin of the event-level histogram. 
The summed event-level histograms over all seed events will be the desired distribution. While the statistical error on each bin is simply the quadratic sum of those over the all event-level histograms, there is generally also the inter-bin correlation since the bins of event-level histograms are not statistically independent between each other. This correlated uncertainty in fact needs to be modeled when performing the combined fit with multiple signal bins, which is examined and summarized in Sec. \ref{sec::BGestimation::objRep::binCor}.


% ---------- Evt level histogram
\begin{center}
\includegraphics[width=80mm]{figures/BGestimation/ObjReplacement/method/evtLevel_histogram.eps}
\captionof{figure}{An example of event-level histogram. 100$\%$ uncertainty is assigned for each bin to account for the fact that all the entries are from the same seed. Final estimation is given by the sum of the event-level histograms over all seed events.}
\label{fig::BGestimation::objRep::evtLevelHist}
\end{center}
% ------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%% MC Closure test %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsubsection{Closure Test using $t\bar{t}$ MC Samples} \label{sec::BGestimation::objRep::mcClosure} 
The methodologies are tested by comparing yields in regions with exactly one baseline lepton, between the estimation using the seed events in 2LCR and the actual $\llmis$/$\ltauh$ events. 
The test is referred as ``closure test'' where the level of disagreement (non-closure) indicates the generic accuracy about this method. The evaluated non-closure is assigned as systematics uncertainty. 
In the MC closure test, simulated $\ttbar$ sample is used in both seed events and the $\llmis$/$\ltauh$ events. All the other processes are abesnt thus no subtraction is taken.
The common 2LCR selection as defined in Tab. \ref{tab::BGestimation::objRep::def2LCR} is applied for seed events selection, except that the MET cut is removed in order to boost the statistics. \\

Fig. \ref{fig::BGestimation::objRep::mcClosure_MisLep_el} $\sim$ \ref{fig::BGestimation::objRep::mcClosure_TauRep_emu} show the result with $\pt>35\gev$ is required for the tag lepton. The test result for the case with a soft lepton ($\pt\in[6,35]\gev$) is displayed in the Appendix.
%and fig.\ref{fig::BGestimation::objRep::mcClosure_softLep_MisLep_el} $\sim$ \ref{fig::BGestimation::objRep::mcClosure_softLep_TauRep_emu} are for the soft lepton regions. 

Good closure is seen in overall kinamtics. Non-closure generally stay within $10\%$ ($5\%$), and never exceeds $30\%$ ($10\%$) significantly for the missing lepton replacement (the tau replacement).
Although the closure of missing lepton replacement is worse than that of tau replacement, it is not worrisome since the contribution of $\ell\ell_{\mathrm{mis.}}$ is typically $5\sim10$ times smaller than $\ell\tau_{\mathrm{h}}$. \\

Closure tests are also performed in phase space close to signal regions. 
Fig. \ref{fig::BGestimation::objRep::mcClosure::MisLepTauRep_regionYieldsBT} $\sim$ \ref{fig::BGestimation::objRep::mcClosure::MisLepTauRep_regionYieldsBV} 
are the btag/bveto-splitted closure in various regions requiring high MET, $m_{T}$, $m_{\mathrm{eff.}}$ etc. The non-closure stay within 30$\%$ (10$\%$) for the missing lepton replacement (the tau replacement).\\

% ---------- mcClosure::plot::hardLep
\input{tex/BGestimation/ObjReplacement/figs_mcClosure_MisLep_el.tex}
\input{tex/BGestimation/ObjReplacement/figs_mcClosure_MisLep_mu.tex}
\input{tex/BGestimation/ObjReplacement/figs_mcClosure_TauRep_emu.tex}

% ---------- mcClosure::plot::softLep
%\input{tex/BGestimation/ObjReplacement/figs_mcClosure_MisLep_softLep_el.tex}
%\input{tex/BGestimation/ObjReplacement/figs_mcClosure_MisLep_softLep_mu.tex}
%\input{tex/BGestimation/ObjReplacement/figs_mcClosure_TauRep_softLep_emu.tex}
%\input{tex/BGestimation/ObjReplacement/figs_mcClosure_All_emu.tex}

\input{tex/BGestimation/ObjReplacement/figs_mcClosure_regionYields.tex}


\clearpage
\subsubsection{Source of non-closure} \label{sec::BGestimation::objRep::NonClosure} 
Visible non-closures are found in some distributions such as MET and jet transverse momenta, and the cause is nailed down as following:

\begin{description}
\item [Kinematical bias triggered by the two lepton requirement in seed event selection (All)] \mbox{} \\
Though the orthogonality between kinematics and object properties (Eq. \ref{}) generally hold as a good approximation, there is still some exception. The most notable example is when the parent particles of the two leptons in a seed event are heavily boosted, the leptons get collimated and overlapped each other. This leads to a deteriorated reconstruction/ID efficiency, therefore selecting events with exactly two leptons already discard the seed events in such phase space. 
The estimated spectra is biased and generally become softer. Electrons address more severe effect because the efficiency drop in the boosted environment is more distinct than the case of muons.

\if 0
Fig \ref{fig::BGestimation::objRep::mcClosure::biasFrom2Lreq} show the explicit comparison of $t\bar{t}$ kinematics of events passing or failing the offline selection.

% -------------- mcClosure::biasFrom2LSelection
In both methods, the primary source of non-closure originated from ...
\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_b1Pt_2Lvs1L_tagMu_varElConeOR.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_t1Pt_2Lvs1L_tagMu_varElConeOR.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_ttPt_2Lvs1L_tagMu_varElConeOR.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_b1Pt_2Lvs1L_tagEl_varElConeOR.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_t1Pt_2Lvs1L_tagEl_varElConeOR.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_ttPt_2Lvs1L_tagEL_varElConeOR.eps}}
%%
%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_b1Pt_2LvsLtauh_tagMu_varElConeOR.eps}}
%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_t1Pt_2LvsLtauh_tagMu_varElConeOR.eps}}
%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_ttPt_2LvsLtauh_tagMu_varElConeOR.eps}}
%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_b1Pt_2LvsLtauh_tagEl_varElConeOR.eps}}

%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_t1Pt_2LvsLtauh_tagEl_varElConeOR.eps}}
%    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/biasFrom2Lreq/tr_ttPt_2LvsLtauh_tagEL_varElConeOR.eps}}
    \caption{Comparison of di-leptonic $t\bar{t}$ kinematics with various offline selections. Only $e\mu$ are used for simplicity. Black lines show the kinematics of events with exactly two baseline leptons (``2 baseLep''), while the purple lines are case with events failing the requirement (``1 baseXX + 1 missing YY''). The other colors are the breakdown of purple ones by different causes of ``missing''. All histograms in the top raws are normalized to unity. Bottom raws are the ratio with respect to ``2 baseLep'' from which we can see    \label{fig::BGestimation::objRep::mcClosure::biasFrom2Lreq} }
\end{figure}
\fi

% -------------- mcClosure::metSoftTerm
\item [Treatment of missing muon]  \mbox{} \\
While the emulated mising muons are completely regareded as invisible particles in the replacement algorithm, the momenta of real unindentified muons do contribute to MET since their tracks are often included in the track soft term. This imperfect emulation leads to a non-closure around MET-related variables in the missing muon replacement. Naively thinking, this can be improved by simply stopping adding the missing muons momenta into MET. However, this is unfortunately not the case, as shown in Fig. \ref{fig::BGestimation::objRep::mcClosure::metSoftTerm_mu} where the improvement is limited in bulk region of the MET spectrum and the closure in the tail gets even worse. This is mainly because the poor momentum resolution of high pt unidentified muons is not emulated in the replacement. As the implemetation of the full emulation is too costly compared with the small portion of missing muons backgrounds in the estimated regions, it is decided to keep the original treatment. Instead, the $30\%$ of non-closure error is additionally quoted to the estimation of the missing muon background. \\

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/metSoftTerm/MisLep_mu_met.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/metSoftTerm/InclSoftTerm_MisLep_mu_met.eps}}
    \caption{The MC closure of MET distrubution for the missing muon sub-events, with (a) the default treatment of missing muons where they are fully counted as invisible particles, and (b) the alternative method where the momenta of missing muons are fully included in the MET soft term and no addtion is applied to MET.
      \label{fig::BGestimation::objRep::mcClosure::metSoftTerm_mu} }
\end{figure}


% -------------- mcClosure::polCorrection
\item [Wrong assumption on tau polarization (tau replacement)] \mbox{} \\
For technical simplicity, tau leptons are assumed to be unpolarized during the decay, which is not true given that tau leptons in consideration are mostly generated through weak decays of W-bosons. For example, the case of Fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x} (a) shows the visible tau fraction $x := E(\tau_{h})/E(\tau)$, a variable sensitive to tau polarization, for taus in the $\ttbar$ process in a blue line, and for the case of unpolarized hypothesis in a red line. This discrepancy is known to eventually propagated to the non-closures in the tail of MET and $\mtFull$ such as the left plots in Fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x} and fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x_mt}. On the other hand, these non-closure can be cured by a simple reweighting in terms of $x$, as they are purely caused by the issue of polatization modeling. Obtaining the reweighting function by fitting the non-closure in $x$ with a third polynomial as shown in Fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x} (c), nicely recovered closures in MET and $\mtFull$ are confirmed as in the right plots in Fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x} and fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x_mt} respectively.  \\

This $x$-reweighting is however not brought into practice, because the $x$-profile varyies by the physics processes (e.g. $\ttbar$, $Wt$ or $WW$ etc.) and the information of their relative breakdown needs to be provided from MC which uncertainty is not easy to evaluate.
%Although the fitting function is found to be almost phase space independent as shown in Fig. \ref{fig::BGestimation::objRep::mcClosure::rwgt_x} (c)
Fortunately, since the impact of this non-closure is marginal in estimating VRs and SRs ($<5\%$), it is decided to be left as it is.

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/TauRep_emu_tr_hadTau_x.eps}}
    \subfigure[]{\includegraphics[width=0.3\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/rwgt_x_TauRep_emu_tr_hadTau_x.eps}}
    \subfigure[]{\includegraphics[width=0.35\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/fit_hadtaux.eps}}
    \caption{Reweighting in terms of the visible tau fraction $x := E(\tau_{h})/E(\tau)$. (a) $x$ distribution before the reweighting , (b) $x$ distribution after the reweighting. (c) An ad hoc fit of the reweighting function by third order polynomial. The reweighting function is almost invariant in terms of phase space.  \label{fig::BGestimation::objRep::mcClosure::rwgt_x} }
\end{figure}

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/TauRep_emu_mt.eps}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/rwgt_x_TauRep_emu_mt.eps}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/TauRep_emu_met.eps}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/ObjReplacement/mcClosure/source_nonClosure/taupol/rwgt_x_TauRep_emu_met.eps}}
    \caption{ (a) $m_{\mathrm{T}}$ and (c) MET distribution before the reweighting in $x$, and (b)(d) after the reweighting.  \label{fig::BGestimation::objRep::mcClosure::rwgt_x_mt} }
\end{figure}

%\begin{figure}[h]
%  \centering

%\end{figure}

\end{description}


\clearpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsubsection{Subtraction of Bogus Sub-events} \label{sec::BGestimation::objRep::subtraction}
Although the object replacement method is designed to estimate the di-leptonic decays of $\ttbar+Wt$ and $WW$ which are the dominant ``di-leptonic'' backgrounds in b-tagged and b-vetoed regions, it is also applicable to estimate the other minor backgrounds such as $\ttbar+W$. 
Basically any leptons in 2LCR are eligible to be replaced, since the replaced sub-event could exist for most of the case.
However, there are couple of exceptions: if the replaced lepton is from $Z$, sub-events of tau replacement will lead to a bogus topology of $Z\ra \tau_h \ell \,\, (ell=e,\mu)$ which never happens, thus these sub-events (bogus sub-events) are need to be subtracted.  \\

Likewise, seed leptons from leptonic tau decays ($\tau \ra \tau_{\ell} \nu\bar{\nu}$) have the same issue 
that tau replacement leads to bogus sub-event where tau decays into tau again. Replacing fake lepton will only end up in bogus sub-events. 
The summary of legal and illegal replacement is given in Tab. \ref{tab::BGestimation::objRep::relProc} where bogus sub-events are label as $``\times''$.
Note that the decision is made on each sub-event level (not seed event level), therefore even $W(\ra \ell \nu)+\ell_{\mathrm{fake}}$ can be seed events as long as one replaces $\ell$ rather than $\ell_{\mathrm{fake}}$. \\

While the sub-traction takes place on sub-event basis, if can be only done statistically i.e. evaluate total contribution from bogus sub-events and subtract once. The largest source of bogus sub-events are seed events with $\tau_{\ell}$.
The contribution is quite large, accounting for $10\%\sim20\%$ of the estimated yields by the tau replacement. 
Threfore, a naive MC subtraction could introduce culprits from the MC mis-modeling, for example on $\ttbar$ as overviewed in Sec. \ref{sec::BGestimation::dataMC}. Instead, to avoid the impact, the subtraction is done in a form of ratio, such as:
\begin{align}
  y^{\mathrm{Data}}_{\ell} 
  & = y^{\mathrm{Data}}_{\ell+\tau_{\ell}} \times \frac{y^{\mathrm{MC}}_{\ell}}{y^{\mathrm{MC}}_{\ell}+y^{\mathrm{MC}}_{\tau_{\ell}}}  
%  & = y^{\mathrm{Data}}_{\ell\ell+\ell\tau_{\ell}} - \frac{y^{\mathrm{Data}}_{\ell\ell}}{y^{\mathrm{MC}}_{\ell\ell}} \times y^{\mathrm{MC}}_{\ell\tau_{\ell}}
\end{align}
where $y^{\mathrm{Data}}_{\ell}$ ($y^{\mathrm{Data}}_{\ell+\tau_{\ell}}$) denote the total yield estimated by tau replacement using data before (after) the subtraction, and $y^{\mathrm{MC}}_{\ell}$ ($y^{\mathrm{MC}}_{\tau_{\mathrm{\ell}}}$) the contribution from legal (bogus) sub-events of tau replcement estimated by MC.
%need some more explanation

The subtraction of the $\ell\ell_{\mathrm{fake}}$ is a little sensitive as MC modeling on fake leptons is less reliable in general. Therefore, relatively more aggressive suppression is applied at the stage of seed selection (Tab. \ref{tab::BGestimation::objRep::seedLepReq}) by requiring tighter isolation, in case that it could be addressing. \\


\begin{table}[h]
  \begin{center}
    \caption{Correspondence between origin of seed lepton and estimated components by the missing lepton replacement or the tau replacement. $X$ represenets any arbitrary particles. $''\times''$ indicates that the generated sub-events represent non-existing processes (``bogus sub-events'') that requires the subtraction. The subscripts ${\mathrm{mis.}}$ denote missing leptons (leptons categorized in ``Mis. Reco'' and ``Mis. ID'' defined in Tab. \ref{tab::BGestimation::BGclass}).
}

    \begin{tabular}{  c | c | c  }
      \hline 
      Parent of seed lepton &   Sub-events of mis. lep. rep.  &  Sub-events of tau rep.  \\
%
      \hline 
      \hline      
      $W (\ra\ell\nu)$ &       $W+X, \, W\ra \ell_{\mathrm{mis.}}\nu$       &      $W+X, \, W\ra\tau_{\mathrm{h}} \nu$ \\
      \hline
      $Z (\ra\ell\ell)$ &       $Z+X, \, Z\ra \ell_{\mathrm{mis.}}\ell$     &      $\times$ \\
      \hline
      $\tau (\ra \tau_\ell \nu)$ &              $\tau_{\ell,{\mathrm{mis.}}}+X$             &      $\times$ \\
      \hline
      Fake &       $\times$       &      $\times$ \\
      \hline
    \end{tabular}  \label{tab::BGestimation::objRep::relProc}
  \end{center}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%% Data Closure test %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsubsection{Closure Test using Data in the Loose Validation Regions.} \label{sec::BGestimation::objRep::dataClosure}
In order to demonstate the procesdures beyond the ideal MC closure tests done in Sec. \ref{sec::BGestimation::objRep::mcClosure} such as the subtraction,
%where only di-leptonic $\ttbar$ is taken into account, 
another validation study is done using the data events.

Since the nominal VRs (Tab. \ref{SRdefinition::regionDef2J} - \ref{SRdefinition::regionDef3B}) tends to have too tight selections with small data statistics, a set of high-$\mt$ regions ``VR-objRep'' with relatively loose selections are deliberately defined, in which the object replacement estimation and data is compared. 9 complemental bins are defined as in Tab. \ref{tab::BGestimation::objRep::VRobjRep}. \\

It is populated by $\llmis / \ltauh$ events with the purity of $\sim 50\%$, and the rest of backgrounds that are not covered by the object replacement (namely the ``semi-leptonic'', ``2L-Out. Acc'' and ``2L-Mis. OR'' components) are estimated by a kinematics extrapolation where the MC of $\wjets$ and $\ttbar+Wt$ is normalized in the corresponding control region bins (``CR-objRep'') which are only different in $\mt$ with respect to VR-objRep, as defined in Tab. \ref{tab::BGestimation::objRep::VRobjRep}. An upper cut in aplanarity is set in either the VRs and the CRs so that the signal contamination is subdued. Statistical uncertainty from the control region statistics, and flat $5\%$ non-closure error is assigned for the object replacement estimation in all the VR bins. \\

\tab{c|ccccc}{
  \hline
               &  $\nJet$  &  $\met$ $[\gev]$   &   $\mt$ $[\gev]$ (CR-objRep)  &   $\meffInc$ $[\gev]$   &  Aplanarity \\ 
  \hline
  \hline
  bin-1        & $\geq 4$  &  $>200$   &   $>125$  ($\in[60,125]$)     &   $>1500$      &  $<0.03$    \\        
  bin-2        & $\geq 4$  &  $>200$   &   $>125$  ($\in[60,125]$)     &   $>2000$      &  $<0.03$    \\        
  bin-3        & $\geq 4$  &  $>200$   &   $>175$  ($\in[60,125]$)     &   $>1000$      &  $<0.03$    \\        
  bin-4        & $\geq 4$  &  $>200$   &   $>400$  ($\in[60,125]$)     &   $-$          &  $<0.03$    \\        
  bin-5        & $\geq 4$  &  $>200$   &   $>400$  ($\in[60,125]$)     &   $>1000$      &  $<0.03$    \\        
  bin-6        & $\geq 4$  &  $>300$   &   $>175$  ($\in[60,125]$)     &   $-$          &  $<0.03$    \\        
  bin-7        & $\geq 4$  &  $>400$   &   $>175$  ($\in[60,125]$)     &   $>1000$      &  $<0.03$    \\        
  bin-8        & $\geq 6$  &  $>400$   &   $>400$  ($\in[60,125]$)     &   $-$          &  $<0.03$    \\        
  bin-9        & $\geq 6$  &  $>200$   &   $>125$  ($\in[60,125]$)     &   $>1500$      &  $<0.03$    \\        
  \hline
}
{Definition of VRs(CRs) objRep. MC of $\wjets$ and $\ttbar+Wt$ are normalized in corresponding CR-objRep.}
{tab::BGestimation::objRep::VRobjRep}

The result is presented in Fig. \ref{fig::BGestimation::objRep::dataClosure::result}. The agreement with data is found within the uncertainty.

% -------------- dataClosure::result
\includegraphics[width=180mm]{figures/BGestimation/ObjReplacement/dataClosure/hist_regionYields_myVRsOnly_wide_metTrig0_NoSys_mode0.eps}
\captionof{figure}{Closure test in VRs objRep bins. The white component shows the yield of the $\llmis \, \ltauh$ events that are estimated by the object replacement, while the colored represents the ``semi-leptonic'' (purple) and  ``2L-Out. Acc / Mis. OR'' components (orange) respectively. The bottom row plots the ratio between the estimated yield and actual number of data. The gray dashed band shows the uncertainty in the estimation which is statistical error due to the CR statistics and flat $5\%$ non-closure for the object replacement.}
\label{fig::BGestimation::objRep::dataClosure::result}
%-------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Signal contamination}
%\input{tex/BGestimation/fig_sigContami_objRep.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% QCD %%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\subsection{Multi-jet Validation using Data} \label{sec::BGestimation::VRQCD}
Among the ``fake'' backgrouds defined in Tab. \ref{tab::BGestimation::BGclass}, 
the multi-jets background including QCD di-jet and full-hadronic decays of $\vjets$ or $\ttbar$, is ignored in the estimation since it is supposed to be negligible after requiring one signal lepton and $\met>250$ in the events, based on the MC study and the past Run2 ATLAS 1-lepton analyses \cite{strong1L_3p2fb_paper}\cite{strong1L_ICHEP2016_CONF}.
However, the cross-check is always worthwhile since the impact could be fatal once it turns to contribute because of its huge cross-section. 
The other components, donimated by $W\ra\tau\nu$ and $Z\ra\nu\nu$, are estimated by the kinamtical extrapolation method in which the normalization factors in Fig. \ref{fig::BGestimation::fittedSFs} are applied for $\wjets$ and top MC. Note that the normalization factors are intended to correct the mis-modeling in the hard process kinematics, but not the modeling on the fake rate of lepton candidates where MC is known to be sometimes unreliable. 
Therefore, a data-driven validation is performed in a set of specific validation regions (VR-QCD) to check those estimation. \\

VR-QCDs are defined by inverting the isolation requirement on the final state lepton with respect to the SRs, as shown in Tab. \ref{SRdefinition::regionDef2J} - \ref{SRdefinition::regionDef3B}. 
The abundance of ``fake'' components is enhanced by around factor of 10 with respect to the SRs, due to the high rejection factor of isolation that is typically $10-20$ ($5-10$) for fake electrons (muons). \\

Fig. \ref{fig::BGestimation::VRQCD1} - \ref{fig::BGestimation::VRQCD2} are the result for each $\meffInc$ bin of VRs-QCD. Nice agreement between the estimation and data is seen overall, implying the good MC modeling on fake lepton. Note that the multi-jets process is not included in MC thus the contribution would emerge as excess in data if it is significant, which is fortunately not the case. \\

\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCR2JMEFFInclBT.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCR2JMEFFInclBV.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCR6JMEFFInclBT.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCR6JMEFFInclBV.pdf}}
    \caption{ VR-QCD for towers (a) 2JBT (b) 2JBV (c) 6JBT (d) 6JBV.  \label{fig::BGestimation::VRQCD1} }
\end{figure} 


\begin{figure}[h]
  \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCRLowxBT.pdf}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCRLowxBV.pdf}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCRHighxBT.pdf}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCRHighxBV.pdf}}
    \subfigure[]{\includegraphics[width=0.4\textwidth]{figures/BGestimation/VRQCD/meffInc30__QCDCR3BMEFFIncl.pdf}}
    \caption{ VR-QCD for towers (a) LowxBT (b) LowxBV (c) HighxBT (d) HighxBV  (e) 3B.  \label{fig::BGestimation::VRQCD2} }
\end{figure} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VR plots %%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Unblinded Validation Regions}
The background estimation is inclusively tested in validation regions VRa and VRb defined in Tab. \ref{SRdefinition::regionDef2J} - \ref{SRdefinition::regionDef3B},
where the phase space are close enough to the signal regions, giving the sensible demonstration of the estimation. \\

Tab. \ref{tab::BGestimation::VRyields_2J} - \ref{tab::BGestimation::VRyields_3B} show the data yields and the expected background together with the breakdowns. 
The components estimated by the object replacement are merged and denoted as ``Di-leptonic'' in the tables, 
while the yields for the other components provided by the kinamtical extrapolation are exlusively listed by physic processes. 
The errors are all post-fit uncertainty with the nuissance parameters profiled (detail found in Sec. \ref{sec::Results::statistics}). \\

The visualized comparison between data and background expectation is illustrated in \ref{fig::BGestimation::VRPulls}, together with the pulls defined by the number of gaussian-equivalent deviation. 
Although the expected backgrounds are found to be systematically underestimating, the tension with respect to data never excessd $2\sigma$, which is still consistent to ascribing to the effects that the systematic uncertainties are paying for.
For instance, the trend of underesimating $\wjets$ in some of the VRb (in particular 2J) can be understood by the imperfect extrapolation from CRs due to the correlation with the ill-modeled variations, as discussed previously in Sec. \ref{sec::BGestimation::kineExtp}. 15$\%$ of uncertainty is in fact assigned for this effect (based on Fig. \ref{fig::BGestimation::valid_extp_VRb2J}, with the the mis-modeling parameter $x$ to be at $\sim0.1$), but no correction is applied for the mean value of the estimation. Therefore, underestimation upto $1\sigma$ is expected by construction. Similar uncorrected systematic effects are also known in the object replacement where the non-closures discussed above always lead to underestimation, which give rises a small systematical underestimation in phase space with extremely hard kinematics.


\clearpage
\input{tex/BGestimation/tab_VRyields.tex}

% -------------- VRpulls
\fig[170]{BGestimation/PullVRsSRs/histpull_VRs.pdf}
{(Top) Observed data and the estimated yields in the nominal validation regions (VRa/VRb). 
The white component is the backgounds estimated by the object replacement method, while the colored ones are by the kinamtical extrapolation method. The dashed band represents the combined statistical and systematic uncertainty on the total estimated backgrounds. (Bottom) Pull between the data and the estiamtion. Pulls in regions dominated by $\wjets$ and tops are painted by pink and blue respectively.}
{fig::BGestimation::VRPulls}
%-------------------------------                                                                     


%%%%%%% Summary of post fit uncertainties %%%%%%%%%%%%%%%%                                                                                                                                                                                                                
%\begin{figure}
%  \begin{center}
%    \includegraphics[width=170mm]{figures/Result/systSummary/syst_summary_VRs.pdf}
%    \captionof{figure}{
%    Post-fit systematic uncertainty with respective to the expected yield in VRa/VRb.
%    Total systematics uncertainty is shown by the filled orange histogram, and the breakdowns are by dashed lines.
%    While the systematics in b-tagged bins are purely dominated by control region statistics, it is comparable to the other sources in the b-veto bins. 
%    }
%    \label{fig::Result::systSummary}
%  \end{center}
%\end{figure}
%-------------------------------    
\clearpage
\section{Systematic Uncertainties}  \label{sec::Uncertainties}
Uncertainties associated with background estimations and the signal modeling is dedicatedly discussed in this section.
They are largely three-fold: instrumental uncertainties, theoretical uncertainties and the the generic uncertainties for the background estimation methods. \\

%%%%%%%%%%%%%%%%%%%%%%%%% Instrumental %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Instrumental Uncertainty}
Instrumental uncertainties are the systematic uncertainty regarding to the experiment, including the imperfection of calibration and mis-modeling of detector response and so on.
%The explicit list of implemented uncertainties are summarized in the Appendix Sec. \ref{sec::Uncertainties::fullListSyst}.  \\
  
\subsubsection{Jets} 
Despite the dedicated calibration procedures as described in Sec. \ref{sec::objDef::jets}, the residual uncertainty on the jet energy scale (JES) is often the largest source of intrumental uncertainty. While 87 independent uncertainties are modeled from each step in the sequential calibration,
those showing the similar behavior are then combined. In the analysis, 8 combined nuisance parameters are input in the fit.
The sub-leading jet uncertainty is on the jet energy resolution (JER). While JER is measured using different multiple physics processes, the uncertainty is quoted by the diffence in the outcomes.
Systematics dealing with the flavor tagging are also important since the analysis deeply relies on the classification in b-tagged jet multiplicity. 
\\
There are a couple of additional uncertainties dealding with the jet eta-scale and the modeling of the JVT (Jet Vertex Tagger, Sec. \ref{sec::objDef::jets}) profile etc. that are also taken into account, though the impacts on the final result are usually negligible. \\

%%%%%%%% resol
%The resolution of the reconstructed jet is measured by the dijet balance technique. The dijet balance technique measures the asymmetry A between the transverse momenta pj1,pj2 of the dijet
%system. The asymmetry A is defined by A = (pj1  pj2 )/(pj1 + pj2 ). Its standard deviation A gives TTTT

%%%%%%% btag
%To evaluate systematic uncertainties, several validations are performed by di↵erent training configuration and alternative background samples with observed data(tt ̄ ! eμ + bb sample). It is assigned of roughly 5 ⇠ 10% for inclusive |⌘| region over pbT = 20 GeV ⇠ 300 GeV [151].


\subsubsection{Electrons}
Electrons involve three efficiency uncertainties on reconstruction, identification and isolation, as well as the uncertainties on the energy scale and resolution modeling.
The efficiencies are measured by exploting the $Z\ra\ell\ell$ process with the tag-probe technique as described in Sec. \ref{sec::objDef::electrons}, and the uncertainties are derived from the difference between the expected measured efficiencies by MC and the observed ones.
The uncertainties on the energy scale and resolution are evaluated from the non-closure of the LAr response simulation with respect to data collected in Run2.  \\

%resolution uncertainty -> morinaga thesis (5.4.2)
%determined by test beam etc.
%extrapolate


\subsubsection{Muons}
Four efficiency uncertainties and two separated scale uncertainties are associated to muons.
All the uncertainties are derived from the difference between the expectation and observed measurement outcome using $Z\ra\mu\mu$ process by the tag-probe technique similarly to the case of electrons. 
The efficiency uncertainties involve the reconstruction, identification, isolation and TTVA (Tracks-To-Vetex-Association), while the two scale uncertainties corresponds to the statistical and systematic uncertainty in the measurement. \\


\subsubsection{MET} 
On top of the propagated uncertainties on the scales and resolutions of the reconstructed objects, MET sufferes from addtional uncertainty regarding to the modeling of track soft term.
This is measured using the $Z (\ra\ell\ell)+\mathrm{jets}$ events, by comparing the expected momentum profile of soft terms and the observed ones.  \\


\subsubsection{Implementation and the Impact} 
Instrumental uncertainties are implemented by generating the corrsponding MC variations in which the scale, resoluton or the efficiency for objects are tuned event-by-event.
The expected yields in the CRs and SRs (VRs) by the variated samples are then input into the fit, and the difference with respect to those of nominal sample is taken as the $1\sigma$ deviation by the systematics. \\

%The impact by the scale variation on the background estimation is generally sizable since the tails of kinematics distributions are highly sensitive to it.
%However, due to the nomalization applied for the main background
The impact on kinematical extrapolaton is relatively sizable, amounting upto $5\% \sim 15\%$ in signal regions, though the nomalization applied for the main background in CRs helps a lot.
On the other hand, instrumental uncertainties have tiny impact on the estimation by the object replacement method, though they do affect the modeling of lepton efficiencies and the tau response that are fully based on MC.
The effect is examined by comparing the MC closure between the nominal setup and the cases with systematical variations being applied using the $\ttbar$ MC samples. 
%The results are shown in Fig \ref{fig::Uncertainties::objRepSys_kineVar1} $\sim$ \ref{fig::Uncertainties::objRepSys_regionYields} where the closure is shown as a function of kinematical variables as well as the yields in SR-like regions.
%No significant effect exceeding a few percent is found, therefore flat $5\%$ uncertainty is assigned on the gross estimation by the object replacement method. \\

The impact on signal modeling is generally marginal compared with the cross-section error and the shape uncertainties as described below.
Therefore, the instrumental uncertainties are not implemented for the non-benchmark models, otherwise the computation cost will skyrocket due to the enormous signal points.


\clearpage
%\input{tex/Uncertainties/fig_objRep_instSys.tex}



%%%%%%%%%%%%%%%%%%%%%%%%% Theo unct. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Theoretical Uncertainty} 
There are two types of uncertainties subjecting to theoretical uncertainty: the cross-section uncertainty affecting the global normalization, and the uncertainty on kinematics modeling affecting the acceptance referred as ``shape'' uncertainty.
Their impacts are evaluated either on signal yields and background expectation in the SRs and VRs, and implemented in the final fit. 
As for the backgrounds, uncertainties are assigned only for the components estimated by the kinematical extrapolation method, since the object replacement method experieces no theory dependency by construction. \\

The cross-section uncertainties are provided by the associated calculation error in the references that the cross-section is quoted by. The primary source contributing to it is the missing higher-order terms in the calculation, such as terms beyond NNLO for the NLO calculation, or the absence of soft gluon resummation. The other typical sources are from PDF, and measurement precision on standard model parameters, particularly in strong coupling constant and quark masses for higher order QCD correction.  \\
The shape uncertainties are evaluated using the MC samples with specific systematic variations applied. 
The recepe of systematics differ by physics processes and the generator used, and is carefully designed to minimize the double-counting as possible. \\

For the normalized backgrounds ($\wjets$ and $\ttbar$), the uncertainties on the extrapolation between CRs to SRs/VRs are considered as the only source of theoretical uncertainty, 
since the other uncertainties (cross-section uncertainty and the shape uncertainties on CR yields) will be cancelled through the normalization in CRs. 
On the other hand, the full uncertainties are assigned for the other non-normalized backgrounds ($\zjets$, di-bosons and $\ttbar+W/Z/WW$) and the SUSY processes, as they are free from any constraints in the analysis.
Note that all these theoretical uncertainties included are assigned on the post-fit yields without any constraint by the fit.
%contrary to the instrumental uncertainties discussed above. \\


\subsubsection{Normalized Backgrounds} 
%mtとかの分布はpreselection levelではnominalがかなり正しいことがわかってるので、まじめにやるならpre-selectionのdataでtheo variationのやつらをconstraintする必要がある
%ただCR->SRのmodelingはnominalが正しいということは仮定してない. てか正しいかわかんないからttPtとかでmis-modelingをinjectしたときにどれくらい間違えるかを見たりしてた。
%theo systに対しての応答見た訳ではないのでやっぱfull extentでつける必要あり (とはいえradHi/Loとかは実際恐らくmis-modelingの原因なのでdouble countはするっぽい)
%meffを緩めた時のtheo systの効果
%
The shape uncertainties for the normlized backgrounds ($\wjets$ and $\ttbar$) are given by computing the variation in the ratio of MC yields between in a CR and a SR (or VR), resulting from the systemtatical variations applied.
The uncertainties are evaluated in respective SR and VR, however, some of the cuts are removed to suppress the statistical fluctuation in MC to a sensible level, which is not trivial given that the evaluated variations are often at the level of $5\%-10\%$.
The b-jet requirement is then removed, based on the fact that it is genearally orthogonal to kinematics.
Though it is much less trivial, the $\meffInc$ cut can also be removed in addtion to it. 
This is because the kinamatical extrapolation method is by concept relying on the weak correlation in the behaviors between the extrapolating variables and the other presumably ill-modeled variables including $\meffInc$, thus the effect of loosened $\meffInc$ cut is supposed to be sub-dominant with respect to the systematic variations in interest.
%Moreover, even if the impact of loosened $\meffInc$ cut is significant,  
%Based on the extrapolation error estimated in Sec. \ref{sec::BGestimation::nonClosure_kineExtp}, 
%Thus, the effect of systematic variations in interest will not be hidden if they are more significant than $10\%$($20\%$) in SRs(VRs), otherwise the extrapolation error 
%そうはいっても20%くらいズレてるとこもある。
%確かに考えてるtheoSystがjetとかのmis-modelingと関係するなら、meffを緩めることによってmtの違いの評価に影響はする。でもその場合は前に評価したerrorでカバーできることになるので問題ない
%考えてるtheoSystがjetとかのmis-modelingと関係しないなら、恐らくmeffを緩めることによる影響はsub-dominantなので大丈夫。というわけでむしろ緩めた方がdoule-couningが防がれてよい。
%
Therefore, the evaluated systematics are common to all the bins in the same tower eventually. \\

The menu of theoretical variations for $\wjets$ are as following:
%Following systematic variations are considered by tuning the internal parameters in Sherpa:
%For $\wzjets$, Sherpa is used for both event generation and hadronization.
\begin{itemize}
\item Choice of renormalization, factorization and resummation scale for soft gluon. \\
%These scales are chosen to $\mu = m_W$ in the Sherpa default setting.
The $1\sigma$ up/down variations are genearted by independently shifting those scales from the default values $\mu_0$ to either $0.5\mu_0$ or $2\mu_0$ respectively.  

\item Choice of CKKW matching scale.\\
The decault matching scale for CKKW is $20\gev$, while it is set to $15\gev$ and $30\gev$ respectively for variations. \\
\end{itemize} 



%%%%%%%%%%
The theoretical variations considered for $\ttbar$ are as below:
\begin{itemize}
\item Choice of renormalization/factorization scale.\\
In \powhegbox generator, these scales are set to common default values of $\mu_0 = \sqrt{m_t^2+p_{\mathrm{T},t}^2}$ where $m_t$ and $p_{\mathrm{T},t}$ are the mass and trasverse momentum of top quark.
The $1\sigma$ up/down variations are generated by simultaneously shifting those scales by factor of 2 or 0.5 respectively.  
For the up variation, a parameter referred as ``hdamp'' \cite{ttbarGen_ATLAS_Run1} controling the amount of NLO radiation is addtionally shifted from the default value of $m_t$ to $2m_t$ for the internal consistency in \powhegbox. 

\item Parton shower scheme.\\
The dependency on parton showering scheme is evaluated by comparing the default scheme (\pythiasix) with one used in \herwig.
The difference is taken as $1\sigma$ variaton.

\item Interference between top-like $WWbb$ diagrams, and the inclusive $WWbb$ ones.\\
The diagrams of $\ttbar+Wt$ and the other $WWbb$ diagrams are allowed to interfere each other since they lead to the common final states.
This effect is a missed piece in the MC description, however is known to become significant in phase space where the bulk $\ttbar$ component is suppressed, for which signal regions are actually designed for.
In particular, the topness selection is essentially rejecting the $\ttbar$ with the both top quaks being on-shell, in other words, significantly enhancing the contribution from the off-shell tail of top quarks where the interference effect is adderssing. The impact is evaluated by comparing two truth-level MadGraph samples: one with the only diagrams of $\ttbar+Wt$, and the other with inclusive $WWbb$ diagrams.
The difference is taken as $1\sigma$ variaton.
\end{itemize}
%It turns that the spectra are generally quite seneitive to the renormalization scale variation.
The evaluated uncertainties are listed in Tab. \ref{tab::Uncertainties::theoSys_Wjets} and \ref{tab::Uncertainties::theoSys_ttbar} for $\wjets$ and $\ttbar$ respectively. Systematics contributing below 5$\%$ or 5 times less than that of the leading uncertainty in the region are ignored. \\

\input{tex/Uncertainties/theoSyst_norm.tex}



\clearpage
\subsubsection{Non-normalized Backgrounds}  
\paragraph{Cross-section uncertianty}
The cross-section uncertianty for $\zjets$, di-bosons and $\ttbar+W/Z/WW$ amounts upto level of $5\%$ \cite{VjetsXsecMeas_ATLAS_Run1}, $6\%$ \cite{BosonXsec_calc_ATLAS} and $13\%$ \cite{Alwall:2014hca} respectively. 
%These are evaluated based on the variation of renormalizaton and factorization scales as well as PDF. 

\paragraph{Shape uncertianty}
The shape uncertainties for non-normalized background components are dominantly seen in spectra regarding to jet activity, in particular jet-multiplicity and $\meffInc$, while the impact on the spectra of other variables are rather limited. Therefore, the shape uncertainties are evaluated in SRs/VRs with the cuts in $\mt$, $\apl$ and topness are removed, as well as the b-tagging requirement. \\

% zjetsはleptonをnuに置き換えた

The variations considered for $\zjets$ adn di-bosons are the same as those for $\wjets$ as described above, except for that CKKW matching for dibosons. The menu of variations for $\ttbar+W/Z/WW$ is minimal since it is the smallest backgrounds:
\begin{itemize}
\item Choice of renormalization and factorization scale
The $1\sigma$ up/down variations are genearted by simultaneously shifting these scales from the default value $\mu_0$ to $0.5\mu_0$ and $2\mu_0$.
\item Hard process description.
As $\ttbar+W/Z/WW$ have not dedicatedly measured in precision using data, additional uncertainty is quoted by comparing with the sample generated by the alternative hard process modeling by Sherpa.
\end{itemize}

The uncertainties derived for each $\meffInc$-bin of SR and VR, as in Tab. \ref{tab::Uncertainties::theoSys_Zjets}, \ref{tab::Uncertainties::theoSys_VV} and \ref{tab::Uncertainties::theoSys_ttV} for $\zjets$, di-bosons and $\ttbar$ respectively. Systematics contributing below 5$\%$ or 5 times less than that of the leading uncertainty in the region are ignored.  \\
%The renormalization scale variation yields the dominant impact for $\zjets$ and di-bosons, due to the fact that it gives the 

\input{tex/Uncertainties/theoSyst_nonnorm.tex}


\clearpage
\subsubsection{SUSY signal} 
The cross-section uncertainty of gluino pair production amounts up-to $15\% \sim 35\%$, as shown in Fig. \ref{fig::Samples::xsec_GG} in Sec. \ref{sec::Samples::SUSY}.\\
The shape uncertainty is evaluated by examining following systematic variations:
\begin{itemize}
\item Choice of renormalization and factorization scale.\\
The variations are genearted by independently shifting those scales by factor of 2 or 0.5 respectively. 

\item Parton shower tuning.\\
Five variations are genearted by tuning the MadGraph internal parameters dealding with parton shower. The Uncertainties are added in quadrature.
\end{itemize}
Seven reference mass points of the benchmark model QQC1QQC1 (Tab. \ref{tab::Uncertainties::refSigPoints} in Appendix) are tested. 
The shape uncertainty is typically marginal compared with the cross-section uncertainty, since the jet activity is predominantly sourced by jets from gluinos rather than the ISRs and FSRs. 
The only exception is found in low $\meffInc$-bins in SR \textbf{2J} where the target signals are with highly comppreseed mass splitting between gluino and LSP ($\dmg < 50\gev$) that have to rely on the additional radiation to enter the signal regions. In such case, the acceptance can vary upto by $20\%$ by the theoretical variation. Tab. \ref{tab::Uncertainties::theoSyst_signal} presents the assigned shape uncertainties, which are common to all the signal models and mass points.
\tab{ c | c c}{
  \hline
  &  Scale in Fac./Renom.  & Parton shower \\
  \hline
  \hline
  SR 2J $\meffIncFirst$   &  15 & 20    \\
  SR 2J $\meffIncSecond$  &  10 & 10    \\
  SR 2J $\meffIncThird$   &  -  & 5     \\
  The other regions       &  -  & -     \\
  \hline
}
{Shape uncertainties assigned for SUSY signal processes $[\%]$. The uncertainties are common to all the signal models.}
{tab::Uncertainties::theoSyst_signal}


%%%%%%%%%%%%%%%%%%%%%%%%% Syst on the method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Other Uncertainties} 
\subsubsection{Uncertainty Generaic to the Estimation Methods}  \label{sec::Uncertainties::nonClosure}
The generic errors of the estimation method need to be quoted as additional uncertainties in the backgroudn expectation. 
In the kinematical extrapolation method, this refers to the extrapolation error as already discussed in Sec. \ref{sec::BGestimation::nonClosure_kineExtp}. 
The assigned uncertainty to each SR and VR are decided as Tab. \ref{tab::Uncertainties::noClosure_kineExtp}, based on the 
as shown in Fig. \ref{fig::BGestimation::valid_extp_2J} - \ref{fig::BGestimation::valid_extp_3B} and the observed level of mis-modeling ($x_W=0.1, \,\,\, x_{\ttbar}=0.06$ in Eq. \ref{eq::BGestimation::injected_MCvariation}).


%%%%%%%%%%% kine extp 
\tab{ c | c c || c | c c || c | c c } 
{
  \hline
                         & $\wjets$ & $\ttbar$ &                           &   $\wjets$ & $\ttbar$ &                           & $\wjets$ & $\ttbar$ \\
  \hline
  SR 2J $\meffIncFirst$  &  15  & 5            &   VRa 2J $\meffIncFirst$  &  -  & 10              &   VRb 2J $\meffIncFirst$  &  10 & 5     \\
  SR 2J $\meffIncSecond$ &  15  & -            &   VRa 2J $\meffIncSecond$ &  5  & 10              &   VRb 2J $\meffIncSecond$ &  5  & 10    \\
  SR 2J $\meffIncThird$  &  15  & 20           &   VRa 2J $\meffIncThird$  &  -  & 20              &   VRb 2J $\meffIncThird$  &  5  & 10    \\
  SR 6J $\meffIncFirst$  &  -   & 5            &   VRa 6J $\meffIncFirst$  &  -  & 5               &   VRb 6J $\meffIncFirst$  &  -  & -     \\
  SR 6J $\meffIncSecond$ &  -   & 10           &   VRa 6J $\meffIncSecond$ &  -  & 5               &   VRb 6J $\meffIncSecond$ &  5  & 5     \\
  SR 6J $\meffIncThird$  &  -   & -            &   VRa 6J $\meffIncThird$  &  -  & 5               &   VRb 6J $\meffIncThird$  &  5  & 10    \\
  SR Low-x               &  10  & -            &   VRa Low-x               &  -  & 5               &   VRb Low-x               & 10  & 5     \\
  SR High-x              &  -   & 10           &   VRa High-x              &  -  & 30              &   VRb High-x              &  5  & 10    \\
  SR 3B $\meffIncFirst$  &  -   & 5            &   VRa 3B $\meffIncFirst$  & 30  & -               &   VRb 3B $\meffIncFirst$  & 20  & 10    \\
  SR 3B $\meffIncSecond$ &  -   & 10           &   VRa 3B $\meffIncSecond$ & 30  & 5               &   VRb 3B $\meffIncSecond$ & 30  & 15    \\
  \hline            
}
{Assgined uncertainty for $\ttbar$ and $\wjets$ for the kinematical extrapolation from CRs to corresponsding VRs and SRs [$\%$], based on the result in Sec \ref{sec::BGestimation::nonClosure_kineExtp}.}
{tab::Uncertainties::noClosure_kineExtp}


%%%% objRep
\noindent For the object replacement method, the observed non-closure error discussed throughout Sec. \ref{sec::BGestimation::objRep::mcClosure} - \ref{sec::BGestimation::objRep::NonClosure} are included as systematics as listed in Tab. \ref{tab::Uncertainties::sys_nonClosure}. 
%The uncertainties are commonly assigned to all the SRs and VRs for missing-lepton replacement, while extra uncertainty is quoted for the estimation of the tau replacement in the \textbf{3B} towers, to account for the 

\tab{c|c c}
{
  \hline
                               & BV/BT & 3B \\
  \hline
  \hline
  Tau replacement              & 5 & 20 \\
  Missing electron replacement & \multicolumn{2}{c}{ 15 } \\
  Missing muon replacement     & \multicolumn{2}{c}{ 30 } \\
  \hline
}
{Summary of non-closure errors in the object replacement method [$\%$]. }
{tab::Uncertainties::sys_nonClosure}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Control region statistics}
In both of the background estimation methods, reflecting the (semi-)data driven nature, the statistical error in CRs often becomes the primary uncertainty in the estimation.
This typically occurs in case of the high $\meffInc$ bins, for instance the yields in the CRs for the kinematical extrapolation end up in about 15 events, immediately resulting in $20\%-30\%$ of uncertainty. The tendency is more striking concerning to the object replacement method where the uncertainty is solely dominated by the seed event statistical error that amounts $20\%-60\%$ in SRs depending on the tightness of selection. Furthermore, one has to mind that the statistical error in the object replacement method is not independent between the regions given that the sub-events from a single seed event can fall into different regions. The correlated statistical error between two signal regions is then evaluated by identifying the fraction of common seed events between their estimation. 
Tab. \ref{sec::BGestimation::objRep::binCor} shows the correlation coefficient in the estimated yields between SR$_i$ and SR$_j$ defined as:
$$\rho := \frac{\sum_e \sqrt{w^{i}_e w^{j}_e}}{\sqrt{\sum_e w^{i}_e} \sqrt{\sum_e w^{j}_e}}$$
where $e$ runs over all seed events, and $w^i_e$ denotes the sum of weighted sub-events falling into SR$_i$ generated by the seed event $e$. Correlation is mainly found in adjacent $\meffInc$-bins, high $\meffInc$ BT/3B bins, and high $\meffInc$ hard lepton / soft lepton bins. This correlation is taken into accounted in the final fitting. Though large inter-bin correlation can potentially destroy the sensitivity in the shape fit, the impact on the final result to this analysis is limited, since the signal points rarely lay over multiple bins with equal abandunce. 

%Seed events overlap between the SRs
\fig[180]{Uncertainties/seed_overlap_objRep/seed_overlap_data.pdf}
{The correlation coefficient in the estimated yields between two signal regions, indicating the level of correlated statistical fluctuation.}
{sec::BGestimation::objRep::binCor}



%%%%%%
\subsubsection{MC statistics}
Limited MC statictics lead to non-negligible uncertainty in signal and background yields in regions with tight selection. The largest impact is found in SR 3B $\meffIncSecond$ amounting upto $15\%$, which is still minor compared with the other systematics sources. The statistical behavior is carefully taken into account in the fit, as detailed in the Sec. \ref{sec::Uncertainties::statistics}.








\clearpage

\section{Result} 

\subsection{Statical Analysis and Hypothetical Test} \label{sec::Result::statistics}
\paragraph{The Profle Likelihood and Treatment of Systematics}
Statitical tests are performed to examine the consistency of observed data with respect to prediction of SM or that with specific signal being overlayed. This is implemented via a likelihood function based on the probability desnsity distribution (PDF) in terms of number of observed events in each signal region bin. The full representation of the likelihood is given by Eq. \ref{eq::Result::LH}:
\begin{align}
 \mathcal{L} (\mu; \muw, \mutop, \bmtheta)  
 & =  \mathcal{L} ( \bm{n}^{\SR}, \bm{n}^{\WR}, \bm{n}^{\TR} | \mu, \muw, \mutop, \bmtheta) \nn \\
  & = \mathcal{P}_{\SR} \times \mathcal{P}_{\CR}  \times \prod_{k\in \mathrm{syst.}} \rho(\theta_k), \\
& \nn \\
% 
  \mathcal{P}_{\SR} & = \prod_{i\notin \textbf{3B}} \,\, \left[ \prod_{b \in \mathrm{BT}, \mathrm{BV}} \,\, \mathrm{Pois}\, (n^{\SR}_{i,b} | \mu s^{\SR}_{i,b}(\bmtheta) + \muw \,  w^{\SR}_{i,b}(\bmtheta) + \mutop \,  t^{\SR}_{i,b}(\bmtheta) + b^{\SR}_{i,b}(\bmtheta)) \right] \nn \\
& \times \prod_{i\in \textbf{3B}} \,\, \mathrm{Pois}\, (n^{\SR}_i | \mu s^{\SR}_i(\bmtheta) + \muw \,  w^{\SR}_i(\bmtheta) + \mutop \,  t^{\SR}_i(\bmtheta) + b^{\SR}_i(\bmtheta))  \\
& \nn\\
%
  \mathcal{P}_{\CR} =  \prod_i \,\,
  &        \mathrm{Pois}\, (n^{\TR}_i | \mu s^{\WR}_i(\bmtheta) + \muw \,  w^{\WR}_i(\bmtheta) + \mutop \,  t^{\WR}_i(\bmtheta) + b^{\WR}_i(\bmtheta))  \nn \\
  & \times \mathrm{Pois}\, (n^{\WR}_i | \mu s^{\TR}_i(\bmtheta) + \muw \,  w^{\TR}_i(\bmtheta) + \mutop \,  t^{\TR}_i(\bmtheta) + b^{\TR}_i(\bmtheta))  
\label{eq::Result::LH}
\end{align}
where 
$\bm{n}^{\SR}$, $\bm{n}^{\WR}$ and $\bm{n}^{\TR}$ are respectively the numbers of observed events in SRs, corresponding CRs such as WRs and TRs, with the vetor indices running over ;
$s_{r}$ is the expected signal yield in region $r$ in the signal model to be tested;
$w_{r}$ and $t_{r}$ are respectively the expectated yields of $\wjets$ and $\ttbar$ in region $r$ before the normalization, with the components derived by the object replacement method being exluded;
$b_{r}$ are the expectated yields of the other backgrounds in region $r$;
$\bmtheta$ is the vector of nuisance parameters for each systematic uncertainty; 
$\muw$ and $\mutop$ are the normalization factors for $\wjets$ and $\ttbar$ which are allowed to vary between $i$; 
and $\mu$ is the signal strength, a parameter denoting relative normalization with respect to the signal model to be tested i.e. $\mu=0$ corresponds to a background-only hypothesis and $\mu=1$ to a hypothesis with the nominal signal level expected by the signal model. Index $i$ runs along signal region bins joining the combined fit that are orothgonal to each other s.t. :
%
\begin{align}
i \in \,\, \{ \,\,\, & \textbf{2J},\textbf{6J},\textbf{3B}  \,\,\, \}  \nn \\
\mbox{ or } & \{ \,\, \textbf{2J},\textbf{High-x},\textbf{3B} \,\,\}  \nn \\
\mbox{ or } & \{ \,\, \textbf{Low-x},\textbf{6J},\textbf{3B} \,\,\}  \nn \\
\mbox{ or } & \{ \,\, \textbf{Low-x},\textbf{High-x},\textbf{3B} \,\,\}
\end{align}
where
\begin{align}
\textbf{2J} & = \,\, \{ \,\,\, \mbox{2J-$\meffIncFirst$, 2J-$\meffIncSecond$, 2J-$\meffIncThird$}  \,\,\, \}  \nn \\
\textbf{6J} & = \,\, \{ \,\,\, \mbox{6J-$\meffIncFirst$, 6J-$\meffIncSecond$, 6J-$\meffIncThird$}  \,\,\, \}  \nn \\
\textbf{Low-x} & = \,\, \{ \,\,\, \mbox{Low-x}  \,\,\, \}  \nn \\
\textbf{High-x} & = \,\, \{ \,\,\,\mbox{High-x}   \,\,\, \}  \nn \\
\textbf{3B} & = \,\, \{ \,\,\, \mbox{3B-$\meffIncFirst$, 3B-$\meffIncSecond$}  \,\,\, \}  \nn \\
\end{align}

The normalization factors for $\wjets$ and $\ttbar$ backgrounds are simultaneouly determined by the fit,
in order to correlate the behavior of systematics.
Therefore the CRs terms are also placed in the common likelihood with an identical representation as SRs. \\

The statistical behavior of the PDF is fully characterized by a set of independent poisson PDF, namely:
\begin{align}
\mathrm{Pois}\,(n|\nu) & := \frac{\nu^n}{n!} e^{-\nu} \nn
\end{align}
with $\nu$ and $n$ being the expected yield and observed number repectively. \\

The effect of a systematics (indexed by $k$) are then incorporated by shifting the poisson means $\nu$, via a corresponding nuisance parameter $\theta_k$ so as:
\begin{align}
        \nu(\theta_k) := f(\theta_k), 
\end{align}
with $f(\theta_k)$ being a continuous function satisfying:
\begin{align}
        f(\theta_k=0) & =  \nu(0)  \nn \\
        f(\theta_k=\pm1) & =  \nu(\pm1\sigma).
\end{align}
$\nu(0)$ is the nominal expectation yields, while $\nu(\pm1\sigma)$ is given by that with the systematic variation applied by $\pm1\sigma$ which are evaluated beforehand. $f(\theta_k)$ in the other $\theta_k$ is then interporated or extrapolated using the three points by a polynomial or an exponential function, providing a continuous functional form of $\mathcal{L}$ in terms of $\bmtheta$.

What is here intend to do is to perform a global fit on data, simultaneously determining $\mu, \muw, \mutop$ and $\bmtheta$ by minimizing the likehood $\mathcal{L}$ (Eq. \ref{eq::Result::LH}). While the $\mu, \muw$ and $\mutop$ are allowed to flow based on our total ignorance, the shifts of the nuisance parameters $\bmtheta$ need to be restricted reflecting the level of our confidence. This is implemented by the last terms in the likelihood $\rho(\theta_k)$ known as the ``penalty terms'' serving as the prior constraints for the likehood. The form of the penalty terms depends on the statistical nature of each systematics:
\begin{itemize}
%jav. need to change
\item A Gaussian PDF is commonly assumed for most systematic uncertainties:
\begin{align}
%\rho (\theta) = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp} \left( - \frac{(\theta-\hat{\theta})^2}{2\sigma^2} \right)
\rho (\theta) = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp} \left( - \frac{\theta^2}{2} \right)
\end{align}

%\begin{align}
%\rho (\theta) = \frac{1}{\sqrt{2\pi}\log{\sigma}} \mathrm{exp} \left( - \frac{(\log{(\theta/\hat{\theta})})^2}{2(\log{\sigma})^2} \right) \frac{1}{\theta}
%\end{align}

\item The Gamma PDF is used to describe uncertainties following according to Poisson distribution, typically associated with the number of data events in control regions, or selected MC events:
\begin{align}
& \rho (a) = \frac{\nu^a}{a!} e^{-\nu} 
\end{align}
where $a$ is related with $\theta$ using the symmetrized uncertainty by 
\begin{align}
& \theta = \frac{a-\nu}{\sigma} \nn \\
\end{align}
\end{itemize}

A multi-dimensional minimization over the parameter spaces of all the normalization factors, nuisance parameters and signal strength 
\footnote{Remind that we have $8-16$ normalization factors and $\sim 150$ nuisance parameters in case of combined fit over all SR towers.}
is performed by the Minuit2 algorithm \cite{Minuite2} interfaced by a number wrapper packages; \texttt{HistFitter} \cite{HistFitter}, \texttt{HistFactory} \cite{HistFactory} and \texttt{RooFit} \cite{RooFit}.
Signal strength and the background normalization factors are allowed to range $0-5$, while nuisance parameters are to moved by $-5\sigma - 5\sigma$ during the fit. Typically, the deminsion of systematics resulting in tiny enough impact on the yields in the SRs/CRs region bins (evaluated by the Kolmogorov-Smirov test) are excluded from the fit so as to reduce the redundant steps of scan (``pruning'').


%%%%%%%%%%%%%%%%%%%%
\paragraph{Hypothetical Testing}
A hypothetical test against a hypothesis $H$ is done by examining the compatibility with observation, via p-value $p^\mu$.
P-value is commonly defined as the probability to find even rarer outcome than observation under $H$. 
For the simpleest one bin counting experiment, expecting an data excess as signal, it can be derived as:
\begin{align}
p_\mu := \sum_{n=n_{\mathrm{obs}}}^{\infty} L(n|\mu) 
\end{align}
using the number of observed events $n_{\mathrm{obs.}}$ as the test static.
One would claim a discovery against the null hypothesis $H_0$ if the $p_0$ is significantly low that the observation can be hardly ascribed to statistical fluctuation out of $H_0$. In the filed of high energy physics experiment, this is usually set to one corresponding to $5\sigma$ gaussian standard deviation ($\sim 10^{-7}$). \\

On the other hand, one can claim the exclusion of a signal hypothesis $H_1$ when $p_1$ is reasonably low. $p_1<0.05$ is conventionally used as the threshold, equivalent to an exclusion with $95\%$ confidence level. There are circumstates where observation does not agree with either $H_0$ and $H_1$ due to statistical fluctuation or more seriously poor understanding to backgrounds, and result in strong exclusion power typically when data undershoots the expectation. In LHC, in order to prevent such potentially unreasonably strong exclusion, a modified measure $cls$ is used:
\begin{align}
\cls := \frac{p_1}{1-p_0},
\end{align}
and $\cls<0.05$ is accepted as the equivalence of an exclusion at $95\%$ confidence level. \\

In presence of multiple test statics ($\bm{n}^{\SR}$) together with bunches of nuisance parameters, it is not obvious how to define the ``rareness'' on the muti-dimension of space. In such cases, likelihood is often chosen as the test static projecting n-dimension observables into 1 dimension, as well as providing a well-defined meausre of ``reareness'' by definition. In LHC analysis, a normalized likelihood test static $\lambda_\mu$ is widely used:
\begin{align}
\lambda_\mu = 
        \begin{cases}
        \dfrac{ \mathcal{L} (\mu, \hat{\hat{\bmtheta}}(\mu)) }{ \mathcal{L} (\hat{\mu}, \hat{\bmtheta})  }         \mbox{\phantom{MMMMMMMMMM}} (\hat{\mu}>0)        \\
        \dfrac{ \mathcal{L} (\mu, \hat{\hat{\bmtheta}}(\mu)) }{ \mathcal{L} (0, \hat{\hat{\bmtheta}}(0))  }  \mbox{\phantom{MMMMMMMMMM}} (\hat{\mu}<0)
        \end{cases}
\end{align}
where $\hat{\hat{\theta}}(\mu)$ denotes the best-fit nuisance parameters with fixed $\mu$,
while $\hat{\mu}$ and $\hat{\theta}$ the best-fit parameters with $\mu$ is allowed to float. 
$\mathcal{L} (\mu, \hat{\hat{\theta}}(\mu))$ presents the conditional likelihood normalized by the $\mu$-agonistic denominator $\mathcal{L} (\hat{\mu}, \hat{\theta})$, forcing the range of $\lambda_\mu$ to $0<\lambda_\mu<1$. \\
%(Eq. \ref{sec::Result::LH})

The p-value is finally defined as:
\begin{align}
p_\mu := \int_{q_{\mu,\mathrm{obs.}}}^{\infty}  f(q^{\mu}|\mu) dq_\mu
\end{align}
where $q_{\mu}$ is:
\begin{align}
q_{\mu} = 
        \begin{cases}
        -2 \log\lambda(\mu)     \,\,\,\,  (\hat{\mu} < \mu) \\
        \mbox{\phantom{k}} 0     \mbox{\phantom{MMMMM}} (\hat{\mu} > \mu)
        \end{cases}
\end{align}
and $f(q_\mu)$ is the PDF that $q_\mu$ obeys, defined by the vaiation of $q_\mu$ (1-to-1 to the variation on the best-fit likelihood value) when suffering from both the statistical fluctuation as well as systematics.
In principle this requires a bunch of toy experiments; scanning from $\mu=0$ upto $\mu=5\sim 10$ with a finite step, on each of which a number of the likelihood fits are performed with different fluctuating data statistics and systematic variation applied. This is an incredibly crazy course of computation.
\footnote{Each likehood fit takes typically 8-12 minutes. }

Fortunately, there are a couple of powerful approximation formula; Wald's approximation:
\begin{align}
q_\mu = -2 \log\lambda(\mu) = \frac{\mu-\hat{\mu}}{\sigma^2} + O(1/\sqrt{N})
\end{align}
and the Asimov's asymptotic formula using the result above:
\begin{align}
f(q_\mu,\mu) & = \frac{1}{\sqrt{q_\mu}} \frac{1}{\sqrt{2\pi}} \left[ \exp \left( -\frac{1}{2}(\sqrt{q_\mu}+\sqrt{R}) \right) + \exp \left( -\frac{1}{2}(\sqrt{q_\mu}-\sqrt{R}) \right) \right], \nn \\
R & := \frac{(\mu-\hat{\mu})^2}{\sigma^2},
\label{eq::asimov}
\end{align}
where $\sigma$ is the fitting error on $\hat{\mu}$ and $N$ symbolizes the magnitude of number of events in signal regions, with which the PDF $f(q_\mu)$ can be determined by only one fit.

One disclaimer is however about the validity of the approximation where $O(1/\sqrt{N})$ terms are ignored.
This may not be the case given that the signal regions typically contains events less than 5.
In the thesis, the result for background-only hypothesis (shown in Sec. \ref{sed::Result::bgOnly}) is derived using the rigid toy experiments, however the Asimov's formula (Eq.\ref{eq::asimov}) is nevertheless used for limit setting due to the unrealistic computing time required for the toy experiments. 
%Though it is generally claimed that the p-value could change, this would not change the grand picture of the resultant limits, given that rapid decrease of cross-section .
\footnote{This is in fact how ATLAS/CMS provides the result. We have to admit the imperfection but this is the best thing we could afford to do.}


\clearpage
\subsection{Unblinded Signal Regions with Background-only Hypothesis} \label{sed::Result::bgOnly}
The background expectation in signal regions for null singal hypothesis are determined tower-by-tower, 
by performing a simoultaneous fit on the normalization factors ($\mu_W$, $\mu_{\mathrm{Top}}$) as well as the nuisance parameters associated to systematics uncertainties, 
using all the relevent bins of control regions and signal regions.
The post-fit uncertainties are summarized in Fig. \ref{fig::Result::systSummary}.  \\

For the low $\meffInc$-bins, typically the estimation precision is at $20\%$ level where theory systematics is the main souce.
The signal region bins with tightest selection end up in $40\% \sim 60\%$ of total uncertainty, dominated by the control region statistics in the object replacement method. \\

The unblinded yields of observed data together with the expected backgrounds in the signal regions are shown in Tab. \ref{tab::Result::yieldsSRs_2J_6J} - \ref{tab::Result::yieldsSRs_3B}. Observed data are found to be consistent in general, with no signal regions exhibiting the deviation more than 2$\sigma$.
%A couple of minor excesses are found in some bins in the SR2JBV tower and SRHigh-xBT, however 
The pulls between data and expectation is shown in Fig. \ref{fig::Result::SRPulls}.

%%%%%%%%%%%%%%% SR Yields %%%%%%%%%%%%%%%%%%%%
\begin{table}
  \begin{center}
    \caption{
        Observed yields and backgrounds expection in the signal region bins in tower \textbf{2J} and \textbf{6J}.
        Background component estimated by the object replacement are denoted as ``Di-leptonic'', 
        while the others are derived from the kinematical extrapolation method. Displayed errors are only systematics uncertainty.
    \label{tab::Result::yieldsSRs_2J_6J}}
    \input{tex/Result/yieldTables/yieldTable_2J.tex}
    \input{tex/Result/yieldTables/yieldTable_6J.tex}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \caption{
        Observed yields and backgrounds expection in the signal region bins in tower \textbf{Low-x} and \textbf{High-x}.
        Background component estimated by the object replacement are denoted as ``Di-leptonic'', 
        while the others are derived from the kinematical extrapolation method. Displayed errors are only systematics uncertainty.
    \label{tab::Result::yieldsSRs_Lowx_Highx}}
    \input{tex/Result/yieldTables/yieldTable_Lowx.tex}
    \input{tex/Result/yieldTables/yieldTable_Highx.tex}
  \end{center}
\end{table}

\begin{table}
  \begin{center}
    \caption{
        Observed yields and backgrounds expection in the signal region bins in tower \textbf{3B}.
        Background component estimated by the object replacement are denoted as ``Di-leptonic'', 
        while the others are derived from the kinematical extrapolation method. Displayed errors are only systematics uncertainty.
    \label{tab::Result::yieldsSRs_3B}}
    \input{tex/Result/yieldTables/yieldTable_3B.tex}
  \end{center}
\end{table}


%%%%%%% Summary of post fit uncertainties %%%%%%%%%%%%%%%%
\fig[170]{Result/systSummary/syst_summary_SRs.pdf}
{
    Post-fit systematic uncertainty with respective to the expected yield in the signal regions. 
    Total systematics uncertainty is shown by the filled orange histogram, and the breakdowns are by dashed lines.
    While the systematics in b-tagged bins are purely dominated by control region statistics, 
    it is comparable to the other sources in the b-veto bins. The overall uncertainty ranges between $20\%\sim\%50$.
}       
{fig::Result::systSummary}
%-------------------------------                                                                                                                                                                                                                           
    
%%%%%%%%%%%%%%%%% SRpulls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\fig[170]{BGestimation/PullVRsSRs/histpull_SRs.pdf}
{
    (Top) Observed yields and the background expectation in signal regions. The white component is the backgounds estimated by the object replacement method, while the colored ones are by the kinamtical extrapolation method. The dashed band represents the combined statistical and systematic uncertainty on the total estimated backgrounds.
    (Bottom) Pull between the observed data and the expectation. No significant deviation from expectation exceeding $2\sigma$.
}
{fig::Result::SRPulls}
%-------------------------------                                                                                                   

                                                                                                                         
\clearpage
\subsection{Constraints on the Benchmark Models}
The observed results are then interpretated into constraints on the benchamark models listed in Tab. \ref{tab::Introduction::modelsBV} - \ref{tab::Introduction::models3B}. The limits on the reference models (colored in Tab. \ref{tab::Introduction::modelsBV} - \ref{tab::Introduction::models3B}) are mainly presented in this section, while the full numerical results on the others models are shown in the Appendix. \\


\subsubsection{Exclusion Limits}
Fig. \ref{fig::Result::exclLimit::GG_onestepCC} presents the exclusion limit on QQC1QQC1, the reference model for BV benchmarks (Tab. \ref{tab::Introduction::modelsBV}). Hypothetical tests are done with each signal point using the combination of signal regions that gives the best expected sensitivity. The excluded region is defined by areas with $CL_s<0.05$, corresponding to $95\%$ confidence level. The associated expected limit is represented by a yellow band, showing the range of obtained limit if observed data is consistent to the expectation within $\pm1\sigma$. Observed limits are typically worse than the expected ones in the mass region where sensitivity is primarily driven by SR \textbf{2J} and SR \textbf{High-x}, namely the diagonal region in the \xhalf grid, and the high-$x$ region in the \varx grid respectively, reflecting the observed excess there which weakens the exclusion power. For the \xhalf and the \varx grid, limits are compared with the up-to-date published result provided by ATLAS \cite{strong1L_ICHEP2016_CONF}, shown as grey shades. The exclusion limits are pushed forward by about $200\gev\sim300\gev$ in gluino mass. Obviously, this owes a lot to the increased data statistics by factor of 2.5 though, the benefit by the improved analysis can also be acknowledged, given that the cross-section of gluino pair production falls more rapidly with respect the gluino mass increase, which is roughly $1/3$ by every $100\gev$.Upto $2\tev$ of gluino mass is excluted at the most optimistic massless LSP scenario, while it also reaches about $1.9\tev$ of gluino mass for the more realistic case with $0<\mLSP<1\tev$. 
% 
The constraints on the scenario where EW-gauginos are compressed, 
represented by the \DMth, \DMtw grids,
are explicitly investigated for the first time, which will provide useful input for a number of dark matter oriented SUSY phenomelogical models, such as the well-tempered neutrino scenario.
Though the limit is weaker than the typical signatures in \xhalf and \varx, sensitivity is addressed without loopholes. \\

Likewise, Fig. \ref{fig::Result::exclLimit::GG_QQC1BTC1} exhibits the limits for model QQC1BTC1, the reference model for BT benchmarks (Tab. \ref{tab::Introduction::modelsBV}). As the sensitivity is mainly driven by the b-tagged bins for this model, it is relatively more affected by the $\sim 1.9\sigma$ excess observed in the SR High-x BT, which drastically weakens the limit in the \varx grid. Nevertheless, the overall exclusion reach is comparable to QQC1QQC1, amounting to $1.9\sim2\tev$ in gluino mass for $\mLSP <1\tev$. As the the asymmetric decays of gluino have never been interpretated before, this is the first explicit constrains on such class of models. \\

The exclusion limit for the model TT11TTN1, the reference model for the 3B benchmarks (Tab. \ref{tab::Introduction::modelsBV}) is shown in Fig. \ref{fig::Result::exclLimit::GG_ttn1}. The observed and expected limits agree as the global feature. The most prominent update from the past result \cite{strong3B_ICHEP2016_CONF} by ATLAS (shaded area in the plot) is around the diagonal region with the mass spliting $\dmg$ below $400\gev$, where the explicit limit is set for the first time. On the other hand, there is seemingly no improvedment in the direction toward high gluino mass despite the increased data, which is because the compared limit is provided by the combination of both 0-lepton and 1-lepton channel in the analysis. The sensitivities driven by 1-lepton channel are compareable between the analyses. \\
%%%%%%%%
\input{tex/Result/fig_contLimit.tex}
%%%%%%%%%

The exclusion limits for all the 45 models and grids are calculated similarly. Observed limits are compared in Fig. \ref{fig::Result::combLimit::BV1}-\ref{fig::Result::combLimit::3B2}. Models in the same BV/BT/3B type (defined by the different tables in Tab. \ref{tab::Introduction::modelsBV} - \ref{tab::Introduction::models3B}) are overlaid in the same plot. Though the acceptance after the 1-lepton pre-selection are all similar between them, the final seinsitivity does vary depending on the branching into 1-lepton final state of the model, which has relatively a wide variety. This ends up in $300\gev\sim400\gev$ of differece in gluino mass at the worse case. Aside such several models with small 1-lepton branches, the variation is typically $100\gev\sim200\gev$, which indicates inclusive acceptance by the analysis.


%%%%%%%%
\input{tex/Result/fig_combLimit.tex}
%%%%%%%

\clearpage
\subsubsection{Cross-section Upper limit}
In a hypothetical test, the $\mathrm{CL_s}$ values are calculated for multiple points in $\mu_{\mathrm{sig.}}$, ranging from $0\sim10$. $\mathrm{CL_s}$ is then modeled as funtion of $\mu_{\mathrm{sig.}}$, therefore the upper limit on $\mu_{\mathrm{sig.}}$ can be defined by:
$$
\mu_{\mathrm{sig.},95} := \mu_{\mathrm{sig.}}(\mathrm{CL_s}=0.05),
$$
for each signal points in the model.
This can be straightforwardly interpreted into cross-section upper limit ($\sigma_{95}$), which can be model-independenly ultimately by computing $\sigma_{95}$ as the function of masses of gluino and EW-gauginos including the LSP, for all the decay model of gluino. Fig. \ref{fig::Result::xsecUL::QQC1QQC1}-Fig. \ref{fig::Result::xsecUL::TTN1TTN1} present the results for the reference models QQC1QQC1, QQC1BTC1 and TTN1TTN1.
%, and the full result are in the Appendix \ref{sec::Result::xsecUL::nonBenchMark}.


\input{tex/Result/fig_xsecUL.tex}





%\subsection{Discussion}
\section{Conclusion}
%The completion of the Standard Model (SM) is one of the greatest feat that mankind ever experienced; it suceeded in explaining three out of four forces in the universe; offering a mechanism of particles obtaining the masses (BEH mechanism).
%toghether with the enormous efforts and successes of experimental examinations. Nevertheless there are still quite a room for unrevealed mysteries of universe including unaccounted phenomane such as dark matter or matter anti-matter asymmetry, as well as gliches inside the theory for instance the higgs mass divergence. New physics beyond the SM is hench highly motivated. Supper-symmetry (SUSY) is known as the one of the most favored framwork for the new physics, since it can addresses to a wide range of the problems, extending the SM based on a simple princile of boson-fermion symmetry. \\
%
%The completion of the Standard Model (SM), brought by the discovery of higgs boson, is one of the greatest feat that mankind ever experienced. 
%It suceeded in explaining three out of four forces in the universe upon a fairy straightfoward principle of gauge symmetry,
%and in offering a mechanism of particles obtaining the masses (BEH mechanism), 
%toghether with the enormous efforts and successes of experimental examinations. Nevertheless there are still quite a room for unrevealed mysteries of universe including unaccounted phenomane such as dark matter or matter anti-matter asymmetry, as well as gliches inside the theory for instance the higgs mass divergence. New physics beyond the SM is hench highly motivated. Supper-symmetry (SUSY) is known as the one of the most favored framwork for the new physics, since it can addresses to a wide range of the problems, extending the SM based on a simple princile of boson-fermion symmetry. \\
%As SUSY generally predicts another set of paricle contents with repect to the SM,
%Experimental search
%
This thesis presented the search for gluinos using proton-proton collisions in the Large Hadron Collider (LHC) at the center-of-mass energy of $\sqrt{s}=13\tev$ collected in the ATLAS detector. Focusing on the final state with one leptons, all relevant 45 decay chains for pair produced gluinos are explored, together with various scenarios of the mass spectra, aiming to provide the most general result achiavble in principle. \\

The highlight of the analysis is designing a dedicated data-driven background estimation method, reinforce the confidence on the estimation by reducingthe reliance on simulation which typically less performing in an extreme phase space. \\

Analysis is performed with dataset with 36.1 fb$^{\-1}$ of integrated luminosity. 
In the unblinded signal regions, no significant excess is found.
Constraints are set on each of the 45 models of gluino decay chain.
Exclusion upto $1.7\tev - 2.0 \tev$ in gluino mass, and upto $\sim 1\tev$ in the lightest neutralino mass is widely confirmed 
with typical mass spectra of gluino and EW gauginos, while upto $1.5\tev - 1.9 \tev$ in gluino mass is excluded in case of compressed EW gaugino masses ($\Delta M \sim 20-30\gev$) which is motivated by dark matter relic observations.

